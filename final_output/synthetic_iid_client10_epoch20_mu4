Arguments:
	       batch_size : 10
	clients_per_round : 10
	          dataset : synthetic_iid
	     drop_percent : 0.0
	       eval_every : 1
	    learning_rate : 0.01
	            model : mclr
	     model_params : (10,)
	               mu : 4.0
	       num_epochs : 20
	        num_iters : 1
	       num_rounds : 200
	        optimizer : fedprox
	             seed : 0
Using Federated prox to Train

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/3.64k flops)
  dense/kernel/Initializer/stateless_random_uniform (600/1.20k flops)
    dense/kernel/Initializer/stateless_random_uniform/mul (600/600 flops)
    dense/kernel/Initializer/stateless_random_uniform/sub (1/1 flops)
  PGD/update_dense/kernel/add (600/600 flops)
  PGD/update_dense/kernel/mul (600/600 flops)
  PGD/update_dense/kernel/mul_1 (600/600 flops)
  PGD/update_dense/kernel/sub (600/600 flops)
  PGD/update_dense/bias/add (10/10 flops)
  PGD/update_dense/bias/mul (10/10 flops)
  PGD/update_dense/bias/mul_1 (10/10 flops)
  PGD/update_dense/bias/sub (10/10 flops)
  gradients/sparse_softmax_cross_entropy_loss/value_grad/Neg (1/1 flops)
  gradients/sparse_softmax_cross_entropy_loss/value_grad/mul (1/1 flops)
  sparse_softmax_cross_entropy_loss/num_present/Equal (1/1 flops)

======================End of Report==========================
30 Clients in Total
Training with 10 workers ---
At round 0 accuracy: 0.04627766599597585
At round 0 training accuracy: 0.06398706398706398
At round 0 training loss: 2.3676777861910843
gradient difference: 0.04150136419749851
At round 1 accuracy: 0.07042253521126761
At round 1 training accuracy: 0.08339108339108339
At round 1 training loss: 2.317853041624852
gradient difference: 0.04107144320292424
At round 2 accuracy: 0.09054325955734406
At round 2 training accuracy: 0.10903210903210903
At round 2 training loss: 2.2730208348666143
gradient difference: 0.04065295492180212
At round 3 accuracy: 0.13480885311871227
At round 3 training accuracy: 0.14137214137214138
At round 3 training loss: 2.2303354405288363
gradient difference: 0.04024227634376905
At round 4 accuracy: 0.17303822937625754
At round 4 training accuracy: 0.18895818895818897
At round 4 training loss: 2.187752852678905
gradient difference: 0.039807817769028746
At round 5 accuracy: 0.22132796780684105
At round 5 training accuracy: 0.23284823284823286
At round 5 training loss: 2.1477361633585406
gradient difference: 0.03939262559505339
At round 6 accuracy: 0.26156941649899396
At round 6 training accuracy: 0.27927927927927926
At round 6 training loss: 2.108401787641776
gradient difference: 0.03896253033643489
At round 7 accuracy: 0.3058350100603622
At round 7 training accuracy: 0.32062832062832064
At round 7 training loss: 2.0708948785478287
gradient difference: 0.03853684596657094
At round 8 accuracy: 0.358148893360161
At round 8 training accuracy: 0.355971355971356
At round 8 training loss: 2.0359965436423892
gradient difference: 0.0381176532333246
At round 9 accuracy: 0.3983903420523139
At round 9 training accuracy: 0.38854238854238854
At round 9 training loss: 2.002035427792955
gradient difference: 0.03770308528957138
At round 10 accuracy: 0.44064386317907445
At round 10 training accuracy: 0.4060984060984061
At round 10 training loss: 1.970081043948365
gradient difference: 0.03729771884162721
At round 11 accuracy: 0.45271629778672035
At round 11 training accuracy: 0.4261954261954262
At round 11 training loss: 1.9402445377117217
gradient difference: 0.03689578082373625
At round 12 accuracy: 0.4647887323943662
At round 12 training accuracy: 0.4416724416724417
At round 12 training loss: 1.9126936168091269
gradient difference: 0.036536957933009345
At round 13 accuracy: 0.47484909456740443
At round 13 training accuracy: 0.4518364518364518
At round 13 training loss: 1.8849545804808228
gradient difference: 0.03615274020100123
At round 14 accuracy: 0.47484909456740443
At round 14 training accuracy: 0.46153846153846156
At round 14 training loss: 1.8593501408013662
gradient difference: 0.03579603622083749
At round 15 accuracy: 0.4788732394366197
At round 15 training accuracy: 0.47031647031647034
At round 15 training loss: 1.8344514224016522
gradient difference: 0.035448801440923765
At round 16 accuracy: 0.48893360160965793
At round 16 training accuracy: 0.47793947793947794
At round 16 training loss: 1.8112707729839679
gradient difference: 0.03511234324839345
At round 17 accuracy: 0.5050301810865191
At round 17 training accuracy: 0.48533148533148535
At round 17 training loss: 1.7885081660364759
gradient difference: 0.03478527633997308
At round 18 accuracy: 0.5090543259557344
At round 18 training accuracy: 0.4934164934164934
At round 18 training loss: 1.7682433798616604
gradient difference: 0.034485445578846426
At round 19 accuracy: 0.5130784708249497
At round 19 training accuracy: 0.4991914991914992
At round 19 training loss: 1.7481134575601263
gradient difference: 0.03418311390034823
At round 20 accuracy: 0.5211267605633803
At round 20 training accuracy: 0.504966504966505
At round 20 training loss: 1.7284430992413293
gradient difference: 0.03388711099579524
At round 21 accuracy: 0.5231388329979879
At round 21 training accuracy: 0.5112035112035112
At round 21 training loss: 1.709827578544176
gradient difference: 0.03359820027308868
At round 22 accuracy: 0.5251509054325956
At round 22 training accuracy: 0.5144375144375144
At round 22 training loss: 1.6924873359706172
gradient difference: 0.03332296022535456
At round 23 accuracy: 0.5311871227364185
At round 23 training accuracy: 0.5181335181335182
At round 23 training loss: 1.6766833169930084
gradient difference: 0.03307513560704114
At round 24 accuracy: 0.5311871227364185
At round 24 training accuracy: 0.5222915222915223
At round 24 training loss: 1.6612172838886496
gradient difference: 0.032828694412577054
At round 25 accuracy: 0.5331991951710262
At round 25 training accuracy: 0.5264495264495265
At round 25 training loss: 1.6463617071815715
gradient difference: 0.032607808250884676
At round 26 accuracy: 0.5392354124748491
At round 26 training accuracy: 0.528990528990529
At round 26 training loss: 1.6327764162375102
gradient difference: 0.03239763480059807
At round 27 accuracy: 0.5412474849094567
At round 27 training accuracy: 0.5308385308385308
At round 27 training loss: 1.6196180160449323
gradient difference: 0.03218495710799192
At round 28 accuracy: 0.5432595573440644
At round 28 training accuracy: 0.5331485331485332
At round 28 training loss: 1.6071197843683815
gradient difference: 0.03198927981179053
At round 29 accuracy: 0.545271629778672
At round 29 training accuracy: 0.5359205359205359
At round 29 training loss: 1.594425315007681
gradient difference: 0.03179368379748403
At round 30 accuracy: 0.5492957746478874
At round 30 training accuracy: 0.5375375375375375
At round 30 training loss: 1.5823238285068664
gradient difference: 0.03160737043155574
At round 31 accuracy: 0.5513078470824949
At round 31 training accuracy: 0.5400785400785401
At round 31 training loss: 1.5707926562928967
gradient difference: 0.031434039541839744
At round 32 accuracy: 0.5533199195171026
At round 32 training accuracy: 0.5416955416955417
At round 32 training loss: 1.5596826148655487
gradient difference: 0.031264983983408844
At round 33 accuracy: 0.5573440643863179
At round 33 training accuracy: 0.544005544005544
At round 33 training loss: 1.5495924872265976
gradient difference: 0.031108555440759526
At round 34 accuracy: 0.5573440643863179
At round 34 training accuracy: 0.5446985446985447
At round 34 training loss: 1.5394266213740673
gradient difference: 0.0309537406170581
At round 35 accuracy: 0.5553319919517102
At round 35 training accuracy: 0.5463155463155464
At round 35 training loss: 1.5299164778807886
gradient difference: 0.030809198470268428
At round 36 accuracy: 0.5533199195171026
At round 36 training accuracy: 0.5477015477015477
At round 36 training loss: 1.5206646657920815
gradient difference: 0.030658983245171387
At round 37 accuracy: 0.5553319919517102
At round 37 training accuracy: 0.5483945483945484
At round 37 training loss: 1.5115207622056195
gradient difference: 0.03050648852169384
At round 38 accuracy: 0.5633802816901409
At round 38 training accuracy: 0.5502425502425502
At round 38 training loss: 1.5031902950612468
gradient difference: 0.030370836783549068
At round 39 accuracy: 0.5593561368209256
At round 39 training accuracy: 0.55001155001155
At round 39 training loss: 1.4950121883487943
gradient difference: 0.030244878616956825
At round 40 accuracy: 0.5613682092555332
At round 40 training accuracy: 0.5527835527835527
At round 40 training loss: 1.4873176294596868
gradient difference: 0.030121696260598307
At round 41 accuracy: 0.5613682092555332
At round 41 training accuracy: 0.5555555555555556
At round 41 training loss: 1.479979194623269
gradient difference: 0.029993890363892498
At round 42 accuracy: 0.5633802816901409
At round 42 training accuracy: 0.5564795564795565
At round 42 training loss: 1.473163412470387
gradient difference: 0.029872055124024504
At round 43 accuracy: 0.5613682092555332
At round 43 training accuracy: 0.5580965580965581
At round 43 training loss: 1.4658205777124433
gradient difference: 0.029762158096990524
At round 44 accuracy: 0.5613682092555332
At round 44 training accuracy: 0.5592515592515592
At round 44 training loss: 1.4591235694006262
gradient difference: 0.029659106040374226
At round 45 accuracy: 0.5633802816901409
At round 45 training accuracy: 0.561099561099561
At round 45 training loss: 1.4524189189995484
gradient difference: 0.02955620913153515
At round 46 accuracy: 0.5633802816901409
At round 46 training accuracy: 0.5634095634095634
At round 46 training loss: 1.4462318204517148
gradient difference: 0.02945888271897158
At round 47 accuracy: 0.5674044265593562
At round 47 training accuracy: 0.565026565026565
At round 47 training loss: 1.4400658506696122
gradient difference: 0.02936166525343853
At round 48 accuracy: 0.5694164989939637
At round 48 training accuracy: 0.5661815661815662
At round 48 training loss: 1.4342020725262141
gradient difference: 0.02926378145659165
At round 49 accuracy: 0.5734406438631791
At round 49 training accuracy: 0.5673365673365673
At round 49 training loss: 1.428505119563636
gradient difference: 0.029171088892899863
At round 50 accuracy: 0.5734406438631791
At round 50 training accuracy: 0.5705705705705706
At round 50 training loss: 1.4229424043295427
gradient difference: 0.02906868449758348
At round 51 accuracy: 0.5754527162977867
At round 51 training accuracy: 0.5714945714945715
At round 51 training loss: 1.41735463096683
gradient difference: 0.028982907641389742
At round 52 accuracy: 0.579476861167002
At round 52 training accuracy: 0.5724185724185724
At round 52 training loss: 1.4119242246609742
gradient difference: 0.028896931195036425
At round 53 accuracy: 0.579476861167002
At round 53 training accuracy: 0.5738045738045738
At round 53 training loss: 1.4068646866335113
gradient difference: 0.02881848467418071
At round 54 accuracy: 0.5814889336016097
At round 54 training accuracy: 0.5735735735735735
At round 54 training loss: 1.4017238840271458
gradient difference: 0.028735066604685184
At round 55 accuracy: 0.579476861167002
At round 55 training accuracy: 0.5735735735735735
At round 55 training loss: 1.3967549713303955
gradient difference: 0.02865553498274313
At round 56 accuracy: 0.5814889336016097
At round 56 training accuracy: 0.5754215754215755
At round 56 training loss: 1.3919530834328855
gradient difference: 0.028582358192531195
At round 57 accuracy: 0.5855130784708249
At round 57 training accuracy: 0.5761145761145761
At round 57 training loss: 1.387248124528851
gradient difference: 0.02850671191431892
At round 58 accuracy: 0.5855130784708249
At round 58 training accuracy: 0.5772695772695773
At round 58 training loss: 1.3827074461320334
gradient difference: 0.028432244525362707
At round 59 accuracy: 0.5855130784708249
At round 59 training accuracy: 0.5772695772695773
At round 59 training loss: 1.3782698611624937
gradient difference: 0.028355380411083556
At round 60 accuracy: 0.5814889336016097
At round 60 training accuracy: 0.5784245784245784
At round 60 training loss: 1.3738537386015013
gradient difference: 0.028288409470353017
At round 61 accuracy: 0.5835010060362174
At round 61 training accuracy: 0.5798105798105798
At round 61 training loss: 1.3697879545736544
gradient difference: 0.028216286818304
At round 62 accuracy: 0.5855130784708249
At round 62 training accuracy: 0.5809655809655809
At round 62 training loss: 1.3656856185653574
gradient difference: 0.028149429177407384
At round 63 accuracy: 0.5895372233400402
At round 63 training accuracy: 0.5818895818895818
At round 63 training loss: 1.3617285234213812
gradient difference: 0.02807533968423243
At round 64 accuracy: 0.5875251509054326
At round 64 training accuracy: 0.5816585816585816
At round 64 training loss: 1.3578242270328014
gradient difference: 0.02800714991026048
At round 65 accuracy: 0.5895372233400402
At round 65 training accuracy: 0.5828135828135829
At round 65 training loss: 1.3541000742756981
gradient difference: 0.02794839626262983
At round 66 accuracy: 0.5895372233400402
At round 66 training accuracy: 0.5835065835065835
At round 66 training loss: 1.3503036039424674
gradient difference: 0.027880280957655594
At round 67 accuracy: 0.5915492957746479
At round 67 training accuracy: 0.583968583968584
At round 67 training loss: 1.3466318052397173
gradient difference: 0.02782295337360806
At round 68 accuracy: 0.5935613682092555
At round 68 training accuracy: 0.5855855855855856
At round 68 training loss: 1.3432401306058055
gradient difference: 0.027765910382749774
At round 69 accuracy: 0.5955734406438632
At round 69 training accuracy: 0.5867405867405867
At round 69 training loss: 1.3396862005906558
gradient difference: 0.027703541956241803
At round 70 accuracy: 0.5955734406438632
At round 70 training accuracy: 0.5876645876645876
At round 70 training loss: 1.336373953930526
gradient difference: 0.027653308719182017
At round 71 accuracy: 0.5955734406438632
At round 71 training accuracy: 0.5883575883575883
At round 71 training loss: 1.3329298130274645
gradient difference: 0.02759426637514947
At round 72 accuracy: 0.5955734406438632
At round 72 training accuracy: 0.5897435897435898
At round 72 training loss: 1.329614360714157
gradient difference: 0.027533830105564885
At round 73 accuracy: 0.5935613682092555
At round 73 training accuracy: 0.58997458997459
At round 73 training loss: 1.326339403072277
gradient difference: 0.027477271880421775
At round 74 accuracy: 0.5935613682092555
At round 74 training accuracy: 0.5902055902055902
At round 74 training loss: 1.323218868934797
gradient difference: 0.027425483907173812
At round 75 accuracy: 0.5935613682092555
At round 75 training accuracy: 0.5902055902055902
At round 75 training loss: 1.320170188341165
gradient difference: 0.02737066200363817
At round 76 accuracy: 0.5955734406438632
At round 76 training accuracy: 0.5922845922845923
At round 76 training loss: 1.3171760639381012
gradient difference: 0.02731782996084242
At round 77 accuracy: 0.5935613682092555
At round 77 training accuracy: 0.5932085932085932
At round 77 training loss: 1.3141925062585798
gradient difference: 0.027266141012792233
At round 78 accuracy: 0.5935613682092555
At round 78 training accuracy: 0.5936705936705937
At round 78 training loss: 1.3111700964018058
gradient difference: 0.027225326107843194
At round 79 accuracy: 0.5935613682092555
At round 79 training accuracy: 0.5948255948255948
At round 79 training loss: 1.3083212367930404
gradient difference: 0.02717778650907831
At round 80 accuracy: 0.5975855130784709
At round 80 training accuracy: 0.5948255948255948
At round 80 training loss: 1.3055427171639302
gradient difference: 0.027124539748577127
At round 81 accuracy: 0.5995975855130785
At round 81 training accuracy: 0.5957495957495957
At round 81 training loss: 1.3027020982243112
gradient difference: 0.027078665758414118
At round 82 accuracy: 0.5975855130784709
At round 82 training accuracy: 0.5962115962115963
At round 82 training loss: 1.30000350700567
gradient difference: 0.027029843728734647
At round 83 accuracy: 0.5995975855130785
At round 83 training accuracy: 0.5969045969045969
At round 83 training loss: 1.29729804614863
gradient difference: 0.026985207213822738
At round 84 accuracy: 0.5995975855130785
At round 84 training accuracy: 0.5973665973665974
At round 84 training loss: 1.2946072911852693
gradient difference: 0.026939096802022538
At round 85 accuracy: 0.6036217303822937
At round 85 training accuracy: 0.5975975975975976
At round 85 training loss: 1.2919513044689952
gradient difference: 0.026890217971804613
At round 86 accuracy: 0.607645875251509
At round 86 training accuracy: 0.5969045969045969
At round 86 training loss: 1.2892974254632827
gradient difference: 0.0268488410882108
At round 87 accuracy: 0.6036217303822937
At round 87 training accuracy: 0.5966735966735967
At round 87 training loss: 1.286805760367524
gradient difference: 0.026804865191951906
At round 88 accuracy: 0.6056338028169014
At round 88 training accuracy: 0.5966735966735967
At round 88 training loss: 1.2842130841105164
gradient difference: 0.02676014547317164
At round 89 accuracy: 0.6036217303822937
At round 89 training accuracy: 0.5980595980595981
At round 89 training loss: 1.2816386539794524
gradient difference: 0.02672169383281807
At round 90 accuracy: 0.6036217303822937
At round 90 training accuracy: 0.598983598983599
At round 90 training loss: 1.2792311478333045
gradient difference: 0.02668446769136208
At round 91 accuracy: 0.6016096579476862
At round 91 training accuracy: 0.5992145992145992
At round 91 training loss: 1.276925290228451
gradient difference: 0.02663664775572495
At round 92 accuracy: 0.6036217303822937
At round 92 training accuracy: 0.5999075999075999
At round 92 training loss: 1.2745360490162727
gradient difference: 0.026594043449889672
At round 93 accuracy: 0.6036217303822937
At round 93 training accuracy: 0.6008316008316008
At round 93 training loss: 1.272106183486713
gradient difference: 0.02655669826800234
At round 94 accuracy: 0.607645875251509
At round 94 training accuracy: 0.6012936012936013
At round 94 training loss: 1.269828530926439
gradient difference: 0.026518042239394947
At round 95 accuracy: 0.6056338028169014
At round 95 training accuracy: 0.6017556017556017
At round 95 training loss: 1.2675466455689588
gradient difference: 0.026474545158494744
At round 96 accuracy: 0.6056338028169014
At round 96 training accuracy: 0.6017556017556017
At round 96 training loss: 1.26528458348541
gradient difference: 0.026435465743904173
At round 97 accuracy: 0.6056338028169014
At round 97 training accuracy: 0.6026796026796026
At round 97 training loss: 1.2630039995992726
gradient difference: 0.026401237654397856
At round 98 accuracy: 0.6056338028169014
At round 98 training accuracy: 0.6036036036036037
At round 98 training loss: 1.260881475610308
gradient difference: 0.02635827284179585
At round 99 accuracy: 0.6056338028169014
At round 99 training accuracy: 0.6036036036036037
At round 99 training loss: 1.2586998468625075
gradient difference: 0.02632408348670515
At round 100 accuracy: 0.607645875251509
At round 100 training accuracy: 0.604989604989605
At round 100 training loss: 1.2565565355712423
gradient difference: 0.026283500512888535
At round 101 accuracy: 0.6096579476861167
At round 101 training accuracy: 0.6054516054516055
At round 101 training loss: 1.2544009566554903
gradient difference: 0.026247950532825415
At round 102 accuracy: 0.6096579476861167
At round 102 training accuracy: 0.6054516054516055
At round 102 training loss: 1.2523674058649825
gradient difference: 0.02621189457344487
At round 103 accuracy: 0.6096579476861167
At round 103 training accuracy: 0.6063756063756064
At round 103 training loss: 1.2502843730855577
gradient difference: 0.026178488721524455
At round 104 accuracy: 0.6096579476861167
At round 104 training accuracy: 0.6054516054516055
At round 104 training loss: 1.2482565579168734
gradient difference: 0.02614761540197393
At round 105 accuracy: 0.6096579476861167
At round 105 training accuracy: 0.6054516054516055
At round 105 training loss: 1.2461532851955197
gradient difference: 0.026118636830903114
At round 106 accuracy: 0.6096579476861167
At round 106 training accuracy: 0.6059136059136059
At round 106 training loss: 1.2441180812814103
gradient difference: 0.026090437673818412
At round 107 accuracy: 0.6116700201207244
At round 107 training accuracy: 0.6061446061446062
At round 107 training loss: 1.242053990304594
gradient difference: 0.026059638550234566
At round 108 accuracy: 0.613682092555332
At round 108 training accuracy: 0.6066066066066066
At round 108 training loss: 1.2400641575727598
gradient difference: 0.026019958889989623
At round 109 accuracy: 0.6116700201207244
At round 109 training accuracy: 0.6072996072996073
At round 109 training loss: 1.2380459391236387
gradient difference: 0.02599717722530532
At round 110 accuracy: 0.613682092555332
At round 110 training accuracy: 0.6068376068376068
At round 110 training loss: 1.2361084889748764
gradient difference: 0.02596408131182776
At round 111 accuracy: 0.613682092555332
At round 111 training accuracy: 0.6077616077616078
At round 111 training loss: 1.234154884549205
gradient difference: 0.025932090294235172
At round 112 accuracy: 0.6156941649899397
At round 112 training accuracy: 0.6084546084546084
At round 112 training loss: 1.232195719253882
gradient difference: 0.025893875849085087
At round 113 accuracy: 0.613682092555332
At round 113 training accuracy: 0.607992607992608
At round 113 training loss: 1.2303088327069898
gradient difference: 0.02586704031464856
At round 114 accuracy: 0.613682092555332
At round 114 training accuracy: 0.6089166089166089
At round 114 training loss: 1.22837790156325
gradient difference: 0.025839056591629085
At round 115 accuracy: 0.6197183098591549
At round 115 training accuracy: 0.6098406098406098
At round 115 training loss: 1.2265593295946604
gradient difference: 0.025792343388160277
At round 116 accuracy: 0.6177062374245473
At round 116 training accuracy: 0.6116886116886117
At round 116 training loss: 1.2247415804427886
gradient difference: 0.025763594664791428
At round 117 accuracy: 0.6217303822937625
At round 117 training accuracy: 0.6128436128436129
At round 117 training loss: 1.222982719000175
gradient difference: 0.02573268342605046
At round 118 accuracy: 0.6237424547283702
At round 118 training accuracy: 0.6128436128436129
At round 118 training loss: 1.2212526504149144
gradient difference: 0.025706127443694646
At round 119 accuracy: 0.6217303822937625
At round 119 training accuracy: 0.6130746130746131
At round 119 training loss: 1.2195679164698339
gradient difference: 0.025676739427514916
At round 120 accuracy: 0.6237424547283702
At round 120 training accuracy: 0.613998613998614
At round 120 training loss: 1.217824991373356
gradient difference: 0.02564271329081709
At round 121 accuracy: 0.6237424547283702
At round 121 training accuracy: 0.6142296142296142
At round 121 training loss: 1.2160053157619142
gradient difference: 0.025617663211343393
At round 122 accuracy: 0.6237424547283702
At round 122 training accuracy: 0.6146916146916147
At round 122 training loss: 1.214344748285302
gradient difference: 0.02559224574445982
At round 123 accuracy: 0.6237424547283702
At round 123 training accuracy: 0.6153846153846154
At round 123 training loss: 1.2125720738208055
gradient difference: 0.02556150499917942
At round 124 accuracy: 0.6237424547283702
At round 124 training accuracy: 0.6156156156156156
At round 124 training loss: 1.2108212586059446
gradient difference: 0.02553162145520075
At round 125 accuracy: 0.6237424547283702
At round 125 training accuracy: 0.6172326172326172
At round 125 training loss: 1.209126470077393
gradient difference: 0.025505082353533392
At round 126 accuracy: 0.6257545271629779
At round 126 training accuracy: 0.6165396165396165
At round 126 training loss: 1.2074644778170798
gradient difference: 0.02548194356309317
At round 127 accuracy: 0.6257545271629779
At round 127 training accuracy: 0.617001617001617
At round 127 training loss: 1.2057810696863802
gradient difference: 0.025455649723544492
At round 128 accuracy: 0.6277665995975855
At round 128 training accuracy: 0.617925617925618
At round 128 training loss: 1.2041559597999236
gradient difference: 0.025432316753219164
At round 129 accuracy: 0.6277665995975855
At round 129 training accuracy: 0.6183876183876184
At round 129 training loss: 1.2025456522705649
gradient difference: 0.02540751649372343
At round 130 accuracy: 0.6277665995975855
At round 130 training accuracy: 0.617925617925618
At round 130 training loss: 1.2009430139272421
gradient difference: 0.025372915402663655
At round 131 accuracy: 0.6277665995975855
At round 131 training accuracy: 0.6167706167706167
At round 131 training loss: 1.1993861174467777
gradient difference: 0.02534031451557336
At round 132 accuracy: 0.6277665995975855
At round 132 training accuracy: 0.617925617925618
At round 132 training loss: 1.1978228751438322
gradient difference: 0.025313266123387572
At round 133 accuracy: 0.6277665995975855
At round 133 training accuracy: 0.6181566181566182
At round 133 training loss: 1.1962593859958714
gradient difference: 0.025287672986619103
At round 134 accuracy: 0.6277665995975855
At round 134 training accuracy: 0.6186186186186187
At round 134 training loss: 1.1947135847086827
gradient difference: 0.025268346066016453
At round 135 accuracy: 0.6277665995975855
At round 135 training accuracy: 0.6188496188496189
At round 135 training loss: 1.1931276501505554
gradient difference: 0.025244747053319517
At round 136 accuracy: 0.6277665995975855
At round 136 training accuracy: 0.6195426195426196
At round 136 training loss: 1.1916090943406321
gradient difference: 0.025220387772404127
At round 137 accuracy: 0.6277665995975855
At round 137 training accuracy: 0.6197736197736198
At round 137 training loss: 1.1900547981868028
gradient difference: 0.025189809585660212
At round 138 accuracy: 0.6277665995975855
At round 138 training accuracy: 0.62000462000462
At round 138 training loss: 1.1885988690280893
gradient difference: 0.025172003531316316
At round 139 accuracy: 0.6277665995975855
At round 139 training accuracy: 0.62000462000462
At round 139 training loss: 1.187023562767071
gradient difference: 0.02513703905731052
At round 140 accuracy: 0.6277665995975855
At round 140 training accuracy: 0.6195426195426196
At round 140 training loss: 1.1854558037841367
gradient difference: 0.02511025672567145
At round 141 accuracy: 0.6277665995975855
At round 141 training accuracy: 0.6204666204666205
At round 141 training loss: 1.1839420951577582
gradient difference: 0.025080971921982855
At round 142 accuracy: 0.6297786720321932
At round 142 training accuracy: 0.6209286209286209
At round 142 training loss: 1.182521736051833
gradient difference: 0.025064567814683098
At round 143 accuracy: 0.6317907444668008
At round 143 training accuracy: 0.6204666204666205
At round 143 training loss: 1.1811055803607369
gradient difference: 0.025042463233572763
At round 144 accuracy: 0.6317907444668008
At round 144 training accuracy: 0.6213906213906214
At round 144 training loss: 1.1796840139337488
gradient difference: 0.02501980153936804
At round 145 accuracy: 0.635814889336016
At round 145 training accuracy: 0.6206976206976207
At round 145 training loss: 1.1782498185368602
gradient difference: 0.024997647869119864
At round 146 accuracy: 0.6338028169014085
At round 146 training accuracy: 0.6211596211596212
At round 146 training loss: 1.176807391971219
gradient difference: 0.024955011427090643
At round 147 accuracy: 0.635814889336016
At round 147 training accuracy: 0.6225456225456225
At round 147 training loss: 1.175395658369234
gradient difference: 0.02493902255635738
At round 148 accuracy: 0.635814889336016
At round 148 training accuracy: 0.623007623007623
At round 148 training loss: 1.1740155785649389
gradient difference: 0.024904361943485186
At round 149 accuracy: 0.6378269617706237
At round 149 training accuracy: 0.6234696234696234
At round 149 training loss: 1.17263122787925
gradient difference: 0.02488093348593913
At round 150 accuracy: 0.6398390342052314
At round 150 training accuracy: 0.6241626241626241
At round 150 training loss: 1.1712944244066212
gradient difference: 0.024855824855688363
At round 151 accuracy: 0.6398390342052314
At round 151 training accuracy: 0.6239316239316239
At round 151 training loss: 1.169991443685363
gradient difference: 0.024831924983536834
At round 152 accuracy: 0.6378269617706237
At round 152 training accuracy: 0.6246246246246246
At round 152 training loss: 1.168587773219078
gradient difference: 0.02480407525285617
At round 153 accuracy: 0.6378269617706237
At round 153 training accuracy: 0.6246246246246246
At round 153 training loss: 1.1671895901251952
gradient difference: 0.024785873581018303
At round 154 accuracy: 0.6398390342052314
At round 154 training accuracy: 0.6248556248556248
At round 154 training loss: 1.165803325702203
gradient difference: 0.024757174755490795
At round 155 accuracy: 0.6378269617706237
At round 155 training accuracy: 0.6262416262416263
At round 155 training loss: 1.1644467249233856
gradient difference: 0.02472468117198207
At round 156 accuracy: 0.641851106639839
At round 156 training accuracy: 0.6271656271656272
At round 156 training loss: 1.1630781979835125
gradient difference: 0.02468722345423081
At round 157 accuracy: 0.641851106639839
At round 157 training accuracy: 0.6273966273966274
At round 157 training loss: 1.1617271116738728
gradient difference: 0.024667310897599524
At round 158 accuracy: 0.6398390342052314
At round 158 training accuracy: 0.6280896280896281
At round 158 training loss: 1.1604729527330035
gradient difference: 0.024645889434720406
At round 159 accuracy: 0.6398390342052314
At round 159 training accuracy: 0.6280896280896281
At round 159 training loss: 1.1591656247967521
gradient difference: 0.024618192035084165
At round 160 accuracy: 0.6398390342052314
At round 160 training accuracy: 0.6276276276276276
At round 160 training loss: 1.1578270616584243
gradient difference: 0.02459531937616616
At round 161 accuracy: 0.6398390342052314
At round 161 training accuracy: 0.6276276276276276
At round 161 training loss: 1.156569906678864
gradient difference: 0.024574763362778904
At round 162 accuracy: 0.6438631790744467
At round 162 training accuracy: 0.6278586278586279
At round 162 training loss: 1.1553684082557765
gradient difference: 0.024549933431510174
At round 163 accuracy: 0.6438631790744467
At round 163 training accuracy: 0.6287826287826288
At round 163 training loss: 1.154085316760459
gradient difference: 0.024530132398023128
At round 164 accuracy: 0.6438631790744467
At round 164 training accuracy: 0.6292446292446292
At round 164 training loss: 1.152823544314123
gradient difference: 0.02450984673970959
At round 165 accuracy: 0.6458752515090543
At round 165 training accuracy: 0.6297066297066297
At round 165 training loss: 1.1515245099463125
gradient difference: 0.024487615117568824
At round 166 accuracy: 0.6458752515090543
At round 166 training accuracy: 0.6294756294756295
At round 166 training loss: 1.1502203657434895
gradient difference: 0.024458775945340353
At round 167 accuracy: 0.6458752515090543
At round 167 training accuracy: 0.6303996303996304
At round 167 training loss: 1.148938033123109
gradient difference: 0.02444039982749282
At round 168 accuracy: 0.647887323943662
At round 168 training accuracy: 0.6297066297066297
At round 168 training loss: 1.1477414054830952
gradient difference: 0.024416384592694875
At round 169 accuracy: 0.647887323943662
At round 169 training accuracy: 0.6313236313236313
At round 169 training loss: 1.1465198879953509
gradient difference: 0.024401421122895804
At round 170 accuracy: 0.647887323943662
At round 170 training accuracy: 0.6310926310926311
At round 170 training loss: 1.1453275062603332
gradient difference: 0.024385551818507004
At round 171 accuracy: 0.647887323943662
At round 171 training accuracy: 0.6310926310926311
At round 171 training loss: 1.1441571510094202
gradient difference: 0.024363356693801953
At round 172 accuracy: 0.6498993963782697
At round 172 training accuracy: 0.6315546315546315
At round 172 training loss: 1.1429915687839052
gradient difference: 0.024342457860057894
At round 173 accuracy: 0.6498993963782697
At round 173 training accuracy: 0.6322476322476323
At round 173 training loss: 1.1417812608796858
gradient difference: 0.02432643920040576
At round 174 accuracy: 0.647887323943662
At round 174 training accuracy: 0.6327096327096328
At round 174 training loss: 1.1405849980547953
gradient difference: 0.024309490106243843
At round 175 accuracy: 0.6519114688128773
At round 175 training accuracy: 0.6331716331716332
At round 175 training loss: 1.1394010086372395
gradient difference: 0.02428046535414419
At round 176 accuracy: 0.6519114688128773
At round 176 training accuracy: 0.6338646338646339
At round 176 training loss: 1.138243428882591
gradient difference: 0.024265250391097132
At round 177 accuracy: 0.6519114688128773
At round 177 training accuracy: 0.6338646338646339
At round 177 training loss: 1.1370803358153345
gradient difference: 0.02424677649219092
At round 178 accuracy: 0.6498993963782697
At round 178 training accuracy: 0.6338646338646339
At round 178 training loss: 1.1359278406714644
gradient difference: 0.024227054860475605
At round 179 accuracy: 0.6498993963782697
At round 179 training accuracy: 0.6340956340956341
At round 179 training loss: 1.1347501670055782
gradient difference: 0.024200309544678913
At round 180 accuracy: 0.647887323943662
At round 180 training accuracy: 0.6357126357126357
At round 180 training loss: 1.1336060299777522
gradient difference: 0.024178974861763305
At round 181 accuracy: 0.6498993963782697
At round 181 training accuracy: 0.6345576345576346
At round 181 training loss: 1.1324903029551763
gradient difference: 0.02416891462788844
At round 182 accuracy: 0.6498993963782697
At round 182 training accuracy: 0.6345576345576346
At round 182 training loss: 1.1313274291955617
gradient difference: 0.024152017481308425
At round 183 accuracy: 0.6498993963782697
At round 183 training accuracy: 0.6352506352506353
At round 183 training loss: 1.1302149334083833
gradient difference: 0.024131395061666217
At round 184 accuracy: 0.6519114688128773
At round 184 training accuracy: 0.6354816354816355
At round 184 training loss: 1.1291347924928847
gradient difference: 0.02411564717309949
At round 185 accuracy: 0.6519114688128773
At round 185 training accuracy: 0.6352506352506353
At round 185 training loss: 1.1279843228880542
gradient difference: 0.02409632959295528
At round 186 accuracy: 0.6498993963782697
At round 186 training accuracy: 0.6366366366366366
At round 186 training loss: 1.126927785880022
gradient difference: 0.024068843224008523
At round 187 accuracy: 0.647887323943662
At round 187 training accuracy: 0.6370986370986371
At round 187 training loss: 1.1258403214826735
gradient difference: 0.024050910703150144
At round 188 accuracy: 0.647887323943662
At round 188 training accuracy: 0.6368676368676369
At round 188 training loss: 1.1247300372770295
gradient difference: 0.024028243498805016
At round 189 accuracy: 0.6498993963782697
At round 189 training accuracy: 0.6370986370986371
At round 189 training loss: 1.1236164263088397
gradient difference: 0.02400812376082463
At round 190 accuracy: 0.6498993963782697
At round 190 training accuracy: 0.638022638022638
At round 190 training loss: 1.1225266815140607
gradient difference: 0.023980109502297273
At round 191 accuracy: 0.6519114688128773
At round 191 training accuracy: 0.6382536382536382
At round 191 training loss: 1.1214328661722794
gradient difference: 0.023961176061152476
At round 192 accuracy: 0.6519114688128773
At round 192 training accuracy: 0.638022638022638
At round 192 training loss: 1.1203387534544027
gradient difference: 0.023940762940155347
At round 193 accuracy: 0.6519114688128773
At round 193 training accuracy: 0.6377916377916378
At round 193 training loss: 1.1192243239928625
gradient difference: 0.023924511491924847
At round 194 accuracy: 0.6519114688128773
At round 194 training accuracy: 0.6377916377916378
At round 194 training loss: 1.1181440460789311
gradient difference: 0.023909104625030445
At round 195 accuracy: 0.6519114688128773
At round 195 training accuracy: 0.6384846384846384
At round 195 training loss: 1.117080327626821
gradient difference: 0.023881110655218276
At round 196 accuracy: 0.6498993963782697
At round 196 training accuracy: 0.6387156387156387
At round 196 training loss: 1.1160796664278292
gradient difference: 0.02385877666486112
At round 197 accuracy: 0.6498993963782697
At round 197 training accuracy: 0.6391776391776391
At round 197 training loss: 1.114978223481446
gradient difference: 0.023830367680649216
At round 198 accuracy: 0.6498993963782697
At round 198 training accuracy: 0.6391776391776391
At round 198 training loss: 1.1139381114977782
gradient difference: 0.023810743990900878
At round 199 accuracy: 0.6498993963782697
At round 199 training accuracy: 0.6403326403326404
At round 199 training loss: 1.1129105377759028
gradient difference: 0.023784760517950258
At round 200 accuracy: 0.6519114688128773
At round 200 training accuracy: 0.6398706398706399
