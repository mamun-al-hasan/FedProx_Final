Arguments:
	       batch_size : 10
	clients_per_round : 10
	          dataset : synthetic_0.5_0.5
	     drop_percent : 0.0
	       eval_every : 1
	    learning_rate : 0.01
	            model : mclr
	     model_params : (10,)
	               mu : 4.0
	       num_epochs : 20
	        num_iters : 1
	       num_rounds : 200
	        optimizer : fedprox
	             seed : 0
Using Federated prox to Train

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/3.64k flops)
  dense/kernel/Initializer/stateless_random_uniform (600/1.20k flops)
    dense/kernel/Initializer/stateless_random_uniform/mul (600/600 flops)
    dense/kernel/Initializer/stateless_random_uniform/sub (1/1 flops)
  PGD/update_dense/kernel/add (600/600 flops)
  PGD/update_dense/kernel/mul (600/600 flops)
  PGD/update_dense/kernel/mul_1 (600/600 flops)
  PGD/update_dense/kernel/sub (600/600 flops)
  PGD/update_dense/bias/add (10/10 flops)
  PGD/update_dense/bias/mul (10/10 flops)
  PGD/update_dense/bias/mul_1 (10/10 flops)
  PGD/update_dense/bias/sub (10/10 flops)
  gradients/sparse_softmax_cross_entropy_loss/value_grad/Neg (1/1 flops)
  gradients/sparse_softmax_cross_entropy_loss/value_grad/mul (1/1 flops)
  sparse_softmax_cross_entropy_loss/num_present/Equal (1/1 flops)

======================End of Report==========================
30 Clients in Total
Training with 10 workers ---
At round 0 accuracy: 0.04693733865289838
At round 0 training accuracy: 0.04931162644610794
At round 0 training loss: 1.7335920322068055
gradient difference: 143.68560131796275
At round 1 accuracy: 0.04177423140107956
At round 1 training accuracy: 0.04253258650473748
At round 1 training loss: 1.8365873311215943
gradient difference: 148.648680065456
At round 2 accuracy: 0.05679417977000704
At round 2 training accuracy: 0.05713762236297964
At round 2 training loss: 1.915929800927848
gradient difference: 151.71051604190313
At round 3 accuracy: 0.9202065242900728
At round 3 training accuracy: 0.9232319530963723
At round 3 training loss: 0.3358692498552464
gradient difference: 60.22866973396572
At round 4 accuracy: 0.924665571462098
At round 4 training accuracy: 0.9275506464953148
At round 4 training loss: 0.32334282087938376
gradient difference: 58.65083196838087
At round 5 accuracy: 0.9253696315418916
At round 5 training accuracy: 0.9278385593885777
At round 5 training loss: 0.32970402245378827
gradient difference: 56.95319678392953
At round 6 accuracy: 0.9249002581553626
At round 6 training accuracy: 0.9273936030989897
At round 6 training loss: 0.2428015793194894
gradient difference: 55.743154780034196
At round 7 accuracy: 0.9281858718610655
At round 7 training accuracy: 0.931555253101607
At round 7 training loss: 0.2347915289481947
gradient difference: 53.91856226936444
At round 8 accuracy: 0.9359305327387937
At round 8 training accuracy: 0.9384913364393027
At round 8 training loss: 0.22974818148994666
gradient difference: 52.53508582063682
At round 9 accuracy: 0.9373386528983807
At round 9 training accuracy: 0.9393027273203162
At round 9 training loss: 0.22291023467662965
gradient difference: 51.42065941824078
At round 10 accuracy: 0.9371039662051162
At round 10 training accuracy: 0.9385436842380778
At round 10 training loss: 0.20467413894257633
gradient difference: 51.696896678081515
At round 11 accuracy: 0.9394508331377611
At round 11 training accuracy: 0.9415275087682563
At round 11 training loss: 0.1975200027161098
gradient difference: 50.44773254920823
At round 12 accuracy: 0.9394508331377611
At round 12 training accuracy: 0.9422603779511072
At round 12 training loss: 0.19243435133754622
gradient difference: 49.66966910856842
At round 13 accuracy: 0.9389814597512322
At round 13 training accuracy: 0.9419986389572318
At round 13 training loss: 0.1857154941970642
gradient difference: 49.944825480728994
At round 14 accuracy: 0.9425017601501995
At round 14 training accuracy: 0.9454012458776109
At round 14 training loss: 0.1830843325105848
gradient difference: 47.94575012006636
At round 15 accuracy: 0.9425017601501995
At round 15 training accuracy: 0.945165680783123
At round 15 training loss: 0.18105269387328896
gradient difference: 47.28155174800024
At round 16 accuracy: 0.9439098803097864
At round 16 training accuracy: 0.9459770716641365
At round 16 training loss: 0.17979909810619563
gradient difference: 47.16143557686717
At round 17 accuracy: 0.9455526871626379
At round 17 training accuracy: 0.9468931581427001
At round 17 training loss: 0.172577577630567
gradient difference: 45.665861715319394
At round 18 accuracy: 0.9446139403895799
At round 18 training accuracy: 0.9465005496518871
At round 18 training loss: 0.16752004889251737
gradient difference: 45.86214900642734
At round 19 accuracy: 0.9453180004693734
At round 19 training accuracy: 0.9467884625451499
At round 19 training loss: 0.16201186676573107
gradient difference: 44.67159205459324
At round 20 accuracy: 0.9448486270828444
At round 20 training accuracy: 0.9466052452494372
At round 20 training loss: 0.15963092983689073
gradient difference: 44.82263580692817
At round 21 accuracy: 0.9448486270828444
At round 21 training accuracy: 0.9462649845573994
At round 21 training loss: 0.15809115408022947
gradient difference: 44.92422273530906
At round 22 accuracy: 0.9448486270828444
At round 22 training accuracy: 0.9462911584567869
At round 22 training loss: 0.15731534856491367
gradient difference: 45.0312669827478
At round 23 accuracy: 0.9448486270828444
At round 23 training accuracy: 0.9461079411610742
At round 23 training loss: 0.15674726517304224
gradient difference: 45.09210499315534
At round 24 accuracy: 0.9481342407885472
At round 24 training accuracy: 0.949353504685128
At round 24 training loss: 0.14750582636330833
gradient difference: 43.566432786258545
At round 25 accuracy: 0.9532973480403661
At round 25 training accuracy: 0.9536198502852955
At round 25 training loss: 0.14300913351965247
gradient difference: 42.59254886752285
At round 26 accuracy: 0.9544707815066886
At round 26 training accuracy: 0.9558184578338481
At round 26 training loss: 0.14007342809043186
gradient difference: 41.99876435679801
At round 27 accuracy: 0.9554095282797466
At round 27 training accuracy: 0.9566298487148616
At round 27 training loss: 0.13819637150991076
gradient difference: 41.671040913446824
At round 28 accuracy: 0.9558789016662755
At round 28 training accuracy: 0.956211066324661
At round 28 training loss: 0.13808999458774074
gradient difference: 41.12672786458891
At round 29 accuracy: 0.9558789016662755
At round 29 training accuracy: 0.9561325446264984
At round 29 training loss: 0.13714554383925653
gradient difference: 41.17259056864694
At round 30 accuracy: 0.955644214973011
At round 30 training accuracy: 0.9560016751295608
At round 30 training loss: 0.13649477299980187
gradient difference: 41.224173901145484
At round 31 accuracy: 0.9568176484393335
At round 31 training accuracy: 0.9574150656964875
At round 31 training loss: 0.13093670276392685
gradient difference: 39.93571177253336
At round 32 accuracy: 0.956582961746069
At round 32 training accuracy: 0.9585667172695388
At round 32 training loss: 0.12796649793520298
gradient difference: 39.33767250027953
At round 33 accuracy: 0.9596338887585074
At round 33 training accuracy: 0.9613149767052296
At round 33 training loss: 0.12428570792824108
gradient difference: 37.605709662950616
At round 34 accuracy: 0.9619807556911523
At round 34 training accuracy: 0.9624142804795058
At round 34 training loss: 0.12240582381625771
gradient difference: 37.00936024113922
At round 35 accuracy: 0.9633888758507393
At round 35 training accuracy: 0.9641417578390828
At round 35 training loss: 0.1200502541761381
gradient difference: 35.76939218198784
At round 36 accuracy: 0.9633888758507393
At round 36 training accuracy: 0.9644820185311208
At round 36 training loss: 0.11750861214253872
gradient difference: 35.26711760038946
At round 37 accuracy: 0.9633888758507393
At round 37 training accuracy: 0.9644034968329582
At round 37 training loss: 0.11642755956040379
gradient difference: 35.28765298581555
At round 38 accuracy: 0.9643276226237972
At round 38 training accuracy: 0.965136366015809
At round 38 training loss: 0.11456467586634865
gradient difference: 35.019273538357034
At round 39 accuracy: 0.9650316827035907
At round 39 training accuracy: 0.966366539287023
At round 39 training loss: 0.11313059718909668
gradient difference: 34.573368286223655
At round 40 accuracy: 0.9652663693968552
At round 40 training accuracy: 0.9664712348845731
At round 40 training loss: 0.1123433697390807
gradient difference: 34.60083742258588
At round 41 accuracy: 0.9655010560901197
At round 41 training accuracy: 0.9669685389729362
At round 41 training loss: 0.11119493727373303
gradient difference: 34.380216786804716
At round 42 accuracy: 0.9664398028631777
At round 42 training accuracy: 0.9670470606710988
At round 42 training loss: 0.1099894624502785
gradient difference: 34.33261442289345
At round 43 accuracy: 0.9659704294766487
At round 43 training accuracy: 0.9670470606710988
At round 43 training loss: 0.10933643236493255
gradient difference: 34.339415063965724
At round 44 accuracy: 0.9662051161699131
At round 44 training accuracy: 0.9670208867717113
At round 44 training loss: 0.10832422242143945
gradient difference: 33.95163075060466
At round 45 accuracy: 0.9662051161699131
At round 45 training accuracy: 0.9669947128723237
At round 45 training loss: 0.10774217526470475
gradient difference: 33.969777310398726
At round 46 accuracy: 0.9657357427833841
At round 46 training accuracy: 0.9660786263937602
At round 46 training loss: 0.1075790310076455
gradient difference: 33.95965195392933
At round 47 accuracy: 0.9650316827035907
At round 47 training accuracy: 0.9658168873998848
At round 47 training loss: 0.10555042646726248
gradient difference: 33.53838617771785
At round 48 accuracy: 0.9662051161699131
At round 48 training accuracy: 0.9667067999790608
At round 48 training loss: 0.10413245832422154
gradient difference: 33.19856537357113
At round 49 accuracy: 0.9657357427833841
At round 49 training accuracy: 0.9664450609851856
At round 49 training loss: 0.10357826889145891
gradient difference: 33.225472373706225
At round 50 accuracy: 0.9664398028631777
At round 50 training accuracy: 0.9663927131864105
At round 50 training loss: 0.10303404526314065
gradient difference: 33.179231723401266
At round 51 accuracy: 0.9676132363295001
At round 51 training accuracy: 0.9684604512380255
At round 51 training loss: 0.10015742489583963
gradient difference: 31.599443596322406
At round 52 accuracy: 0.9678479230227646
At round 52 training accuracy: 0.9685651468355756
At round 52 training loss: 0.09971319489669642
gradient difference: 31.59880499882919
At round 53 accuracy: 0.9678479230227646
At round 53 training accuracy: 0.9690101031251636
At round 53 training loss: 0.09815841472316156
gradient difference: 31.199776266521337
At round 54 accuracy: 0.9678479230227646
At round 54 training accuracy: 0.9691147987227137
At round 54 training loss: 0.09767432355000225
gradient difference: 31.207422272613236
At round 55 accuracy: 0.9692560431823516
At round 55 training accuracy: 0.9698476679055645
At round 55 training loss: 0.09656727373342007
gradient difference: 30.863642972793127
At round 56 accuracy: 0.9687866697958226
At round 56 training accuracy: 0.9704234936920902
At round 56 training loss: 0.0949372984666629
gradient difference: 30.163711765050312
At round 57 accuracy: 0.969960103262145
At round 57 training accuracy: 0.9719939276553421
At round 57 training loss: 0.0939520505287275
gradient difference: 29.599600181580424
At round 58 accuracy: 0.9706641633419385
At round 58 training accuracy: 0.9717321886614668
At round 58 training loss: 0.09376692448951886
gradient difference: 29.510505680649747
At round 59 accuracy: 0.9711335367284675
At round 59 training accuracy: 0.9731717531277809
At round 59 training loss: 0.09210066204880468
gradient difference: 28.809086911335974
At round 60 accuracy: 0.971368223421732
At round 60 training accuracy: 0.9730408836308433
At round 60 training loss: 0.09163882321468124
gradient difference: 28.73671206317367
At round 61 accuracy: 0.971837596808261
At round 61 training accuracy: 0.9736428833167565
At round 61 training loss: 0.09091627762822056
gradient difference: 28.484969361879998
At round 62 accuracy: 0.9727763435813189
At round 62 training accuracy: 0.9744804480971575
At round 62 training loss: 0.08998306177499767
gradient difference: 27.96867090626028
At round 63 accuracy: 0.9730110302745835
At round 63 training accuracy: 0.974506621996545
At round 63 training loss: 0.08946531755633842
gradient difference: 27.967464738570264
At round 64 accuracy: 0.9732457169678479
At round 64 training accuracy: 0.9747421870910328
At round 64 training loss: 0.08881914125337521
gradient difference: 27.743694337869602
At round 65 accuracy: 0.9741844637409058
At round 65 training accuracy: 0.9756320996702089
At round 65 training loss: 0.08785921505849865
gradient difference: 27.112788471054444
At round 66 accuracy: 0.9737150903543769
At round 66 training accuracy: 0.9760508820604094
At round 66 training loss: 0.08672764783365153
gradient difference: 26.650797388944223
At round 67 accuracy: 0.9734804036611124
At round 67 training accuracy: 0.9760770559597969
At round 67 training loss: 0.0855339141692507
gradient difference: 26.37776635102193
At round 68 accuracy: 0.9730110302745835
At round 68 training accuracy: 0.976234099356122
At round 68 training loss: 0.08445459047475934
gradient difference: 25.9112717318503
At round 69 accuracy: 0.9732457169678479
At round 69 training accuracy: 0.9762864471548971
At round 69 training loss: 0.08394722473843563
gradient difference: 25.766856253874113
At round 70 accuracy: 0.9741844637409058
At round 70 training accuracy: 0.9765481861487725
At round 70 training loss: 0.08348009032003018
gradient difference: 25.572453362635336
At round 71 accuracy: 0.9739497770476414
At round 71 training accuracy: 0.9764434905512224
At round 71 training loss: 0.08323817092879288
gradient difference: 25.43902723922867
At round 72 accuracy: 0.9744191504341704
At round 72 training accuracy: 0.9766528817463226
At round 72 training loss: 0.08278203217070587
gradient difference: 25.361145659517693
At round 73 accuracy: 0.9748885238206993
At round 73 training accuracy: 0.9770978380359105
At round 73 training loss: 0.08215168090054051
gradient difference: 25.145480960951193
At round 74 accuracy: 0.9748885238206993
At round 74 training accuracy: 0.9770978380359105
At round 74 training loss: 0.08163433940810588
gradient difference: 25.14005933395943
At round 75 accuracy: 0.9746538371274349
At round 75 training accuracy: 0.9770454902371355
At round 75 training loss: 0.08096090705579916
gradient difference: 24.747561388217242
At round 76 accuracy: 0.9748885238206993
At round 76 training accuracy: 0.9773857509291735
At round 76 training loss: 0.08064241758798069
gradient difference: 24.60183432105787
At round 77 accuracy: 0.9753578972072283
At round 77 training accuracy: 0.9774380987279485
At round 77 training loss: 0.08030432916652903
gradient difference: 24.37683509008061
At round 78 accuracy: 0.9753578972072283
At round 78 training accuracy: 0.9775951421242737
At round 78 training loss: 0.08002251720942066
gradient difference: 24.29130687223528
At round 79 accuracy: 0.9753578972072283
At round 79 training accuracy: 0.9774904465267236
At round 79 training loss: 0.0799275173541707
gradient difference: 24.07210251609039
At round 80 accuracy: 0.9755925839004929
At round 80 training accuracy: 0.9776213160236612
At round 80 training loss: 0.07956517041467638
gradient difference: 23.926319880681014
At round 81 accuracy: 0.9751232105139639
At round 81 training accuracy: 0.9775427943254986
At round 81 training loss: 0.07936569427225423
gradient difference: 23.96076869529112
At round 82 accuracy: 0.9751232105139639
At round 82 training accuracy: 0.9774380987279485
At round 82 training loss: 0.07905117132887848
gradient difference: 23.79888842989717
At round 83 accuracy: 0.9758272705937573
At round 83 training accuracy: 0.9777521855205988
At round 83 training loss: 0.07792992812756822
gradient difference: 23.261042759063127
At round 84 accuracy: 0.9760619572870218
At round 84 training accuracy: 0.9780662723132493
At round 84 training loss: 0.07731655907327172
gradient difference: 22.872036908555568
At round 85 accuracy: 0.9760619572870218
At round 85 training accuracy: 0.9781447940114119
At round 85 training loss: 0.07658510675482401
gradient difference: 22.31951910605824
At round 86 accuracy: 0.9767660173668153
At round 86 training accuracy: 0.9785374025022248
At round 86 training loss: 0.07609214127031184
gradient difference: 22.04916334144029
At round 87 accuracy: 0.9779394508331377
At round 87 training accuracy: 0.9787729675967125
At round 87 training loss: 0.07590194790041634
gradient difference: 21.70935865784923
At round 88 accuracy: 0.9777047641398733
At round 88 training accuracy: 0.978746793697325
At round 88 training loss: 0.07568275008858322
gradient difference: 21.721337035828626
At round 89 accuracy: 0.9777047641398733
At round 89 training accuracy: 0.9789300109930378
At round 89 training loss: 0.07541789589226476
gradient difference: 21.539515092860157
At round 90 accuracy: 0.9777047641398733
At round 90 training accuracy: 0.9789038370936503
At round 90 training loss: 0.07478875179281146
gradient difference: 21.543886718804636
At round 91 accuracy: 0.9777047641398733
At round 91 training accuracy: 0.9789300109930378
At round 91 training loss: 0.07427956690417101
gradient difference: 21.547992398054046
At round 92 accuracy: 0.9779394508331377
At round 92 training accuracy: 0.9789561848924253
At round 92 training loss: 0.07346024554838959
gradient difference: 21.024982545096115
At round 93 accuracy: 0.9777047641398733
At round 93 training accuracy: 0.9790870543893629
At round 93 training loss: 0.07295260968698196
gradient difference: 20.973625672240942
At round 94 accuracy: 0.9786435109129312
At round 94 training accuracy: 0.9797152279746637
At round 94 training loss: 0.07185394244701655
gradient difference: 19.870979454436398
At round 95 accuracy: 0.9788781976061958
At round 95 training accuracy: 0.9798984452703764
At round 95 training loss: 0.07166392980779163
gradient difference: 19.83231293914199
At round 96 accuracy: 0.9793475709927247
At round 96 training accuracy: 0.9798722713709889
At round 96 training loss: 0.07147227261195557
gradient difference: 19.737484878575557
At round 97 accuracy: 0.9793475709927247
At round 97 training accuracy: 0.9798722713709889
At round 97 training loss: 0.07045543423174154
gradient difference: 19.22526207493909
At round 98 accuracy: 0.9791128842994602
At round 98 training accuracy: 0.9800554886667016
At round 98 training loss: 0.06998804244407573
gradient difference: 19.13616531821599
At round 99 accuracy: 0.9791128842994602
At round 99 training accuracy: 0.9800816625660891
At round 99 training loss: 0.06969181481158522
gradient difference: 18.916825723185557
At round 100 accuracy: 0.9791128842994602
At round 100 training accuracy: 0.9801078364654766
At round 100 training loss: 0.06925413224771337
gradient difference: 18.926705020038632
At round 101 accuracy: 0.9793475709927247
At round 101 training accuracy: 0.9803434015599644
At round 101 training loss: 0.06918915565685958
gradient difference: 18.892028313346795
At round 102 accuracy: 0.9791128842994602
At round 102 training accuracy: 0.9806313144532273
At round 102 training loss: 0.06880500979559882
gradient difference: 18.68396907425255
At round 103 accuracy: 0.9793475709927247
At round 103 training accuracy: 0.9809454012458776
At round 103 training loss: 0.06863005328107831
gradient difference: 18.535607703964214
At round 104 accuracy: 0.9795822576859892
At round 104 training accuracy: 0.9814165314348532
At round 104 training loss: 0.0678950420174987
gradient difference: 18.03319185516817
At round 105 accuracy: 0.9800516310725181
At round 105 training accuracy: 0.9815997487305659
At round 105 training loss: 0.06748293536996366
gradient difference: 17.608566786786188
At round 106 accuracy: 0.9800516310725181
At round 106 training accuracy: 0.9815997487305659
At round 106 training loss: 0.06708643552769446
gradient difference: 17.614838926539168
At round 107 accuracy: 0.9800516310725181
At round 107 training accuracy: 0.9815735748311784
At round 107 training loss: 0.0667561996966558
gradient difference: 17.623923386152914
At round 108 accuracy: 0.9802863177657827
At round 108 training accuracy: 0.9820185311207664
At round 108 training loss: 0.06642378932639777
gradient difference: 17.322379718563436
At round 109 accuracy: 0.9802863177657827
At round 109 training accuracy: 0.9822279223158666
At round 109 training loss: 0.0662942375977262
gradient difference: 17.159090507883587
At round 110 accuracy: 0.9802863177657827
At round 110 training accuracy: 0.9821755745170916
At round 110 training loss: 0.0659835232357256
gradient difference: 17.163181727135
At round 111 accuracy: 0.9807556911523116
At round 111 training accuracy: 0.9821755745170916
At round 111 training loss: 0.0656867098655795
gradient difference: 17.06855032729556
At round 112 accuracy: 0.9807556911523116
At round 112 training accuracy: 0.9821494006177041
At round 112 training loss: 0.0654376343417762
gradient difference: 17.072964911686825
At round 113 accuracy: 0.9805210044590472
At round 113 training accuracy: 0.9821755745170916
At round 113 training loss: 0.06523068796285608
gradient difference: 17.068147068109273
At round 114 accuracy: 0.9805210044590472
At round 114 training accuracy: 0.9821232267183165
At round 114 training loss: 0.06512270127480879
gradient difference: 17.029054317301537
At round 115 accuracy: 0.9805210044590472
At round 115 training accuracy: 0.9821232267183165
At round 115 training loss: 0.0649289281299766
gradient difference: 17.027427802904487
At round 116 accuracy: 0.9805210044590472
At round 116 training accuracy: 0.9823064440140292
At round 116 training loss: 0.06474587947209866
gradient difference: 16.90023346106119
At round 117 accuracy: 0.9805210044590472
At round 117 training accuracy: 0.9826728786054546
At round 117 training loss: 0.06393657298901227
gradient difference: 16.44856439129178
At round 118 accuracy: 0.9802863177657827
At round 118 training accuracy: 0.9826467047060671
At round 118 training loss: 0.06339431588856785
gradient difference: 16.110875444410613
At round 119 accuracy: 0.9802863177657827
At round 119 training accuracy: 0.9826205308066795
At round 119 training loss: 0.0632573787180688
gradient difference: 16.110086763651914
At round 120 accuracy: 0.9802863177657827
At round 120 training accuracy: 0.9823064440140292
At round 120 training loss: 0.06300829927966851
gradient difference: 16.03205429190718
At round 121 accuracy: 0.9802863177657827
At round 121 training accuracy: 0.9824373135109669
At round 121 training loss: 0.06276685844351222
gradient difference: 16.007084310761464
At round 122 accuracy: 0.9802863177657827
At round 122 training accuracy: 0.9824373135109669
At round 122 training loss: 0.06263057480379042
gradient difference: 16.00766616009234
At round 123 accuracy: 0.9802863177657827
At round 123 training accuracy: 0.9824111396115793
At round 123 training loss: 0.06252339212407686
gradient difference: 16.009054078930838
At round 124 accuracy: 0.9802863177657827
At round 124 training accuracy: 0.9824373135109669
At round 124 training loss: 0.06242841324334184
gradient difference: 16.014522906676582
At round 125 accuracy: 0.9802863177657827
At round 125 training accuracy: 0.9825158352091294
At round 125 training loss: 0.06235504358759608
gradient difference: 16.076696073153162
At round 126 accuracy: 0.9805210044590472
At round 126 training accuracy: 0.9826990525048421
At round 126 training loss: 0.06214256850890053
gradient difference: 15.977110285809859
At round 127 accuracy: 0.9805210044590472
At round 127 training accuracy: 0.9826990525048421
At round 127 training loss: 0.06205390027948664
gradient difference: 15.981942750526283
At round 128 accuracy: 0.9805210044590472
At round 128 training accuracy: 0.9826728786054546
At round 128 training loss: 0.06181222754463086
gradient difference: 15.907451416214668
At round 129 accuracy: 0.9805210044590472
At round 129 training accuracy: 0.9827252264042297
At round 129 training loss: 0.061558938569230885
gradient difference: 15.78744987972274
At round 130 accuracy: 0.9807556911523116
At round 130 training accuracy: 0.9828560959011674
At round 130 training loss: 0.061195894692077826
gradient difference: 15.407990658416333
At round 131 accuracy: 0.9807556911523116
At round 131 training accuracy: 0.9828560959011674
At round 131 training loss: 0.061119055818009325
gradient difference: 15.411077354929942
At round 132 accuracy: 0.9805210044590472
At round 132 training accuracy: 0.9829084436999425
At round 132 training loss: 0.06103764932338112
gradient difference: 15.427966553326147
At round 133 accuracy: 0.9805210044590472
At round 133 training accuracy: 0.9826990525048421
At round 133 training loss: 0.060915265183352524
gradient difference: 15.481402421505884
At round 134 accuracy: 0.9805210044590472
At round 134 training accuracy: 0.9828560959011674
At round 134 training loss: 0.06066834618270076
gradient difference: 15.385170552502665
At round 135 accuracy: 0.9805210044590472
At round 135 training accuracy: 0.9826467047060671
At round 135 training loss: 0.060380596938714975
gradient difference: 15.364371131313112
At round 136 accuracy: 0.9805210044590472
At round 136 training accuracy: 0.9826467047060671
At round 136 training loss: 0.060288638668705215
gradient difference: 15.359884104072671
At round 137 accuracy: 0.9805210044590472
At round 137 training accuracy: 0.9826990525048421
At round 137 training loss: 0.06015584686709022
gradient difference: 15.295372553136103
At round 138 accuracy: 0.9805210044590472
At round 138 training accuracy: 0.9827252264042297
At round 138 training loss: 0.060075693794713905
gradient difference: 15.295184864821241
At round 139 accuracy: 0.9805210044590472
At round 139 training accuracy: 0.9826728786054546
At round 139 training loss: 0.06000983426897915
gradient difference: 15.297505903975098
At round 140 accuracy: 0.9807556911523116
At round 140 training accuracy: 0.9827775742030047
At round 140 training loss: 0.059898222989216425
gradient difference: 15.223059907049235
At round 141 accuracy: 0.9805210044590472
At round 141 training accuracy: 0.9828560959011674
At round 141 training loss: 0.059549435356640944
gradient difference: 15.026421676753504
At round 142 accuracy: 0.9805210044590472
At round 142 training accuracy: 0.9828560959011674
At round 142 training loss: 0.05947576370925136
gradient difference: 15.024652302265556
At round 143 accuracy: 0.9805210044590472
At round 143 training accuracy: 0.9828037481023923
At round 143 training loss: 0.05940210725690417
gradient difference: 15.019201549235087
At round 144 accuracy: 0.9805210044590472
At round 144 training accuracy: 0.9828037481023923
At round 144 training loss: 0.059331503910264254
gradient difference: 15.01477490264062
At round 145 accuracy: 0.9805210044590472
At round 145 training accuracy: 0.9827514003036172
At round 145 training loss: 0.05927509662806824
gradient difference: 15.014597187922039
At round 146 accuracy: 0.9807556911523116
At round 146 training accuracy: 0.982986965398105
At round 146 training loss: 0.059103121642083874
gradient difference: 14.931420414336946
At round 147 accuracy: 0.9805210044590472
At round 147 training accuracy: 0.982986965398105
At round 147 training loss: 0.058945438717825635
gradient difference: 14.769517396407482
At round 148 accuracy: 0.9809903778455762
At round 148 training accuracy: 0.9830654870962676
At round 148 training loss: 0.058726704304143534
gradient difference: 14.679080254939095
At round 149 accuracy: 0.9809903778455762
At round 149 training accuracy: 0.9830393131968801
At round 149 training loss: 0.05834674828181035
gradient difference: 14.459596319630085
At round 150 accuracy: 0.9807556911523116
At round 150 training accuracy: 0.9829607914987175
At round 150 training loss: 0.05810710538404968
gradient difference: 14.367699092330888
At round 151 accuracy: 0.9809903778455762
At round 151 training accuracy: 0.9828822698005549
At round 151 training loss: 0.057791456558393975
gradient difference: 14.238229003637988
At round 152 accuracy: 0.9809903778455762
At round 152 training accuracy: 0.9828822698005549
At round 152 training loss: 0.057733188849850535
gradient difference: 14.234833788657662
At round 153 accuracy: 0.9809903778455762
At round 153 training accuracy: 0.9830131392974926
At round 153 training loss: 0.05766902657268557
gradient difference: 14.263622855941005
At round 154 accuracy: 0.9809903778455762
At round 154 training accuracy: 0.982986965398105
At round 154 training loss: 0.057625625763648
gradient difference: 14.268996975028552
At round 155 accuracy: 0.9809903778455762
At round 155 training accuracy: 0.9829607914987175
At round 155 training loss: 0.057578877389025326
gradient difference: 14.270910129304756
At round 156 accuracy: 0.9812250645388406
At round 156 training accuracy: 0.982986965398105
At round 156 training loss: 0.057560739742519046
gradient difference: 14.354686420973781
At round 157 accuracy: 0.9812250645388406
At round 157 training accuracy: 0.9830131392974926
At round 157 training loss: 0.05726621479554061
gradient difference: 14.16700046356831
At round 158 accuracy: 0.9812250645388406
At round 158 training accuracy: 0.9830393131968801
At round 158 training loss: 0.05721219154203617
gradient difference: 14.162188642060109
At round 159 accuracy: 0.9812250645388406
At round 159 training accuracy: 0.9830393131968801
At round 159 training loss: 0.05716895302575042
gradient difference: 14.166792530946056
At round 160 accuracy: 0.9812250645388406
At round 160 training accuracy: 0.9831440087944302
At round 160 training loss: 0.05694891682030753
gradient difference: 14.058027021877688
At round 161 accuracy: 0.9812250645388406
At round 161 training accuracy: 0.9830654870962676
At round 161 training loss: 0.05681461123717193
gradient difference: 13.998916623567242
At round 162 accuracy: 0.9814597512321052
At round 162 training accuracy: 0.9832487043919803
At round 162 training loss: 0.05660885455857875
gradient difference: 13.830030111955297
At round 163 accuracy: 0.9814597512321052
At round 163 training accuracy: 0.9832748782913678
At round 163 training loss: 0.056566294610518326
gradient difference: 13.83218140863693
At round 164 accuracy: 0.9812250645388406
At round 164 training accuracy: 0.9836413128827933
At round 164 training loss: 0.05623254712680621
gradient difference: 13.500537878020195
At round 165 accuracy: 0.9812250645388406
At round 165 training accuracy: 0.9836151389834058
At round 165 training loss: 0.056166990832203584
gradient difference: 13.491941730591169
At round 166 accuracy: 0.9812250645388406
At round 166 training accuracy: 0.9836151389834058
At round 166 training loss: 0.056129148856702886
gradient difference: 13.499155882486495
At round 167 accuracy: 0.9812250645388406
At round 167 training accuracy: 0.9836151389834058
At round 167 training loss: 0.05609047053118317
gradient difference: 13.495768985799854
At round 168 accuracy: 0.9809903778455762
At round 168 training accuracy: 0.9836413128827933
At round 168 training loss: 0.05564210255033972
gradient difference: 13.1541436247427
At round 169 accuracy: 0.9809903778455762
At round 169 training accuracy: 0.9836413128827933
At round 169 training loss: 0.0555049198432718
gradient difference: 13.120411863162738
At round 170 accuracy: 0.9809903778455762
At round 170 training accuracy: 0.9836413128827933
At round 170 training loss: 0.05547179461851537
gradient difference: 13.120873039032254
At round 171 accuracy: 0.9809903778455762
At round 171 training accuracy: 0.9836674867821809
At round 171 training loss: 0.05543070420160553
gradient difference: 13.11920748620076
At round 172 accuracy: 0.9809903778455762
At round 172 training accuracy: 0.9836151389834058
At round 172 training loss: 0.05539752215422154
gradient difference: 13.120923396947443
At round 173 accuracy: 0.9812250645388406
At round 173 training accuracy: 0.9836674867821809
At round 173 training loss: 0.055260651986922656
gradient difference: 13.096346366973844
At round 174 accuracy: 0.9809903778455762
At round 174 training accuracy: 0.9836674867821809
At round 174 training loss: 0.055227294096398066
gradient difference: 13.094985171075997
At round 175 accuracy: 0.9814597512321052
At round 175 training accuracy: 0.9837460084803434
At round 175 training loss: 0.055037825792322526
gradient difference: 12.952293425997873
At round 176 accuracy: 0.9816944379253696
At round 176 training accuracy: 0.9836936606815684
At round 176 training loss: 0.05481913723173064
gradient difference: 12.79621739712782
At round 177 accuracy: 0.9816944379253696
At round 177 training accuracy: 0.9837198345809559
At round 177 training loss: 0.05449160082763841
gradient difference: 12.601850583715876
At round 178 accuracy: 0.9819291246186341
At round 178 training accuracy: 0.9835366172852431
At round 178 training loss: 0.0543986961521672
gradient difference: 12.575644497431758
At round 179 accuracy: 0.9814597512321052
At round 179 training accuracy: 0.9836674867821809
At round 179 training loss: 0.05413672200209633
gradient difference: 12.37518470231887
At round 180 accuracy: 0.9816944379253696
At round 180 training accuracy: 0.9836674867821809
At round 180 training loss: 0.053921714879699684
gradient difference: 12.172555361681207
At round 181 accuracy: 0.9812250645388406
At round 181 training accuracy: 0.9836413128827933
At round 181 training loss: 0.05379192558530414
gradient difference: 12.160981857332112
At round 182 accuracy: 0.9812250645388406
At round 182 training accuracy: 0.9835889650840182
At round 182 training loss: 0.05365416840235987
gradient difference: 12.104616577847953
At round 183 accuracy: 0.9812250645388406
At round 183 training accuracy: 0.9835889650840182
At round 183 training loss: 0.053419256633747886
gradient difference: 12.009208041310615
At round 184 accuracy: 0.9812250645388406
At round 184 training accuracy: 0.9835889650840182
At round 184 training loss: 0.05332276552339999
gradient difference: 11.992952982857947
At round 185 accuracy: 0.9816944379253696
At round 185 training accuracy: 0.9836936606815684
At round 185 training loss: 0.05315422419371263
gradient difference: 11.83542623243634
At round 186 accuracy: 0.9814597512321052
At round 186 training accuracy: 0.9836936606815684
At round 186 training loss: 0.05311679202376544
gradient difference: 11.840101654612912
At round 187 accuracy: 0.9814597512321052
At round 187 training accuracy: 0.9836936606815684
At round 187 training loss: 0.05307967400783749
gradient difference: 11.848814415597095
At round 188 accuracy: 0.9809903778455762
At round 188 training accuracy: 0.9837198345809559
At round 188 training loss: 0.05292179143332555
gradient difference: 11.728903888654068
At round 189 accuracy: 0.9823984980051631
At round 189 training accuracy: 0.9838507040778935
At round 189 training loss: 0.05279448013863246
gradient difference: 11.736941090576845
At round 190 accuracy: 0.9826331846984276
At round 190 training accuracy: 0.9838768779772811
At round 190 training loss: 0.05265819462203582
gradient difference: 11.711766594166283
At round 191 accuracy: 0.9826331846984276
At round 191 training accuracy: 0.9839553996754437
At round 191 training loss: 0.052486925415485275
gradient difference: 11.529254662183575
At round 192 accuracy: 0.9826331846984276
At round 192 training accuracy: 0.9839553996754437
At round 192 training loss: 0.05245094659177172
gradient difference: 11.5386852213291
At round 193 accuracy: 0.9821638113118986
At round 193 training accuracy: 0.9839292257760561
At round 193 training loss: 0.05233052839977079
gradient difference: 11.50480331277938
At round 194 accuracy: 0.9821638113118986
At round 194 training accuracy: 0.9839292257760561
At round 194 training loss: 0.0522982059410443
gradient difference: 11.505458745305889
At round 195 accuracy: 0.9823984980051631
At round 195 training accuracy: 0.9840077474742187
At round 195 training loss: 0.05219358733104348
gradient difference: 11.515909626417699
At round 196 accuracy: 0.9823984980051631
At round 196 training accuracy: 0.9840077474742187
At round 196 training loss: 0.05216125243389777
gradient difference: 11.507639696532689
At round 197 accuracy: 0.9823984980051631
At round 197 training accuracy: 0.9840077474742187
At round 197 training loss: 0.05207998562972126
gradient difference: 11.562393317237394
At round 198 accuracy: 0.9826331846984276
At round 198 training accuracy: 0.9840339213736062
At round 198 training loss: 0.052035495672596396
gradient difference: 11.576233081234449
At round 199 accuracy: 0.9823984980051631
At round 199 training accuracy: 0.9841124430717688
At round 199 training loss: 0.05186971932756283
gradient difference: 11.477868641845038
At round 200 accuracy: 0.9826331846984276
At round 200 training accuracy: 0.9840862691723813
