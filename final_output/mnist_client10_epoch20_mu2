Arguments:
	       batch_size : 10
	clients_per_round : 10
	          dataset : mnist
	     drop_percent : 0.0
	       eval_every : 1
	    learning_rate : 0.01
	            model : mclr
	     model_params : (10,)
	               mu : 2.0
	       num_epochs : 20
	        num_iters : 1
	       num_rounds : 200
	        optimizer : fedprox
	             seed : 0
Using Federated prox to Train

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/47.08k flops)
  dense/kernel/Initializer/stateless_random_uniform (7.84k/15.68k flops)
    dense/kernel/Initializer/stateless_random_uniform/mul (7.84k/7.84k flops)
    dense/kernel/Initializer/stateless_random_uniform/sub (1/1 flops)
  PGD/update_dense/kernel/add (7.84k/7.84k flops)
  PGD/update_dense/kernel/mul (7.84k/7.84k flops)
  PGD/update_dense/kernel/mul_1 (7.84k/7.84k flops)
  PGD/update_dense/kernel/sub (7.84k/7.84k flops)
  PGD/update_dense/bias/add (10/10 flops)
  PGD/update_dense/bias/mul (10/10 flops)
  PGD/update_dense/bias/mul_1 (10/10 flops)
  PGD/update_dense/bias/sub (10/10 flops)
  gradients/sparse_softmax_cross_entropy_loss/value_grad/Neg (1/1 flops)
  gradients/sparse_softmax_cross_entropy_loss/value_grad/mul (1/1 flops)
  sparse_softmax_cross_entropy_loss/num_present/Equal (1/1 flops)

======================End of Report==========================
1000 Clients in Total
Training with 10 workers ---
At round 0 accuracy: 0.12562677869630032
At round 0 training accuracy: 0.12760230883974316
At round 0 training loss: 2.785880700883501
gradient difference: 89.47495366335541
At round 1 accuracy: 0.3099335953381217
At round 1 training accuracy: 0.30888838446073025
At round 1 training loss: 2.2081191781010716
gradient difference: 78.18730183995687
At round 2 accuracy: 0.43081718390025747
At round 2 training accuracy: 0.43425319411116153
At round 2 training loss: 1.9127991235342592
gradient difference: 69.33653293606157
At round 3 accuracy: 0.5225640330668112
At round 3 training accuracy: 0.5254718204812244
At round 3 training loss: 1.6459167526520384
gradient difference: 62.580519489639386
At round 4 accuracy: 0.5596964358314135
At round 4 training accuracy: 0.5614177313703872
At round 4 training loss: 1.529572880658432
gradient difference: 58.334068067552224
At round 5 accuracy: 0.6125491258978182
At round 5 training accuracy: 0.608372786821454
At round 5 training loss: 1.3812415752709937
gradient difference: 53.57045712858634
At round 6 accuracy: 0.6208158287030763
At round 6 training accuracy: 0.6203547571178416
At round 6 training loss: 1.2817166401435138
gradient difference: 51.32642837353019
At round 7 accuracy: 0.643989700501423
At round 7 training accuracy: 0.6463454179907906
At round 7 training loss: 1.2221071745061645
gradient difference: 49.61717877289014
At round 8 accuracy: 0.6888467271988075
At round 8 training accuracy: 0.6850152409365069
At round 8 training loss: 1.0992890529832733
gradient difference: 45.958803711201924
At round 9 accuracy: 0.7376338257216425
At round 9 training accuracy: 0.7360237369479214
At round 9 training loss: 1.0088883628394054
gradient difference: 42.56207394784923
At round 10 accuracy: 0.7567421059764196
At round 10 training accuracy: 0.758058239833971
At round 10 training loss: 0.9251276346183146
gradient difference: 39.159055420546956
At round 11 accuracy: 0.7281474454533134
At round 11 training accuracy: 0.7264738309877424
At round 11 training loss: 0.9426970719506892
gradient difference: 39.823484966110776
At round 12 accuracy: 0.7229976961647919
At round 12 training accuracy: 0.7175238342304948
At round 12 training loss: 0.9257511448533569
gradient difference: 39.67967260483332
At round 13 accuracy: 0.7266567285540046
At round 13 training accuracy: 0.7229230170568779
At round 13 training loss: 0.8988452057973081
gradient difference: 38.74917834267304
At round 14 accuracy: 0.7194741834936983
At round 14 training accuracy: 0.7185128737272196
At round 14 training loss: 0.9182367135650844
gradient difference: 39.11910402664532
At round 15 accuracy: 0.7509147580973031
At round 15 training accuracy: 0.7507296193008626
At round 15 training loss: 0.8365935533364209
gradient difference: 35.56153171625623
At round 16 accuracy: 0.7475267651443285
At round 16 training accuracy: 0.748378623775861
At round 16 training loss: 0.8279438200396764
gradient difference: 35.41210947880534
At round 17 accuracy: 0.7812711749559561
At round 17 training accuracy: 0.7826058758674362
At round 17 training loss: 0.7553801594137889
gradient difference: 32.52848276104141
At round 18 accuracy: 0.7980756200027104
At round 18 training accuracy: 0.8012354886827939
At round 18 training loss: 0.7061867738988585
gradient difference: 31.411848027321508
At round 19 accuracy: 0.8001084157744952
At round 19 training accuracy: 0.8013651987807251
At round 19 training loss: 0.6775850954415013
gradient difference: 30.225150350422854
At round 20 accuracy: 0.811356552378371
At round 20 training accuracy: 0.8109961735521111
At round 20 training loss: 0.6561282465315457
gradient difference: 29.279665315497592
At round 21 accuracy: 0.8246374847540318
At round 21 training accuracy: 0.8213567676243596
At round 21 training loss: 0.6283517697378699
gradient difference: 28.486153608572767
At round 22 accuracy: 0.8232822875728418
At round 22 training accuracy: 0.8202055905052208
At round 22 training loss: 0.6358050100126759
gradient difference: 28.768332393039994
At round 23 accuracy: 0.8230112481366039
At round 23 training accuracy: 0.8208541409948764
At round 23 training loss: 0.6356553013008072
gradient difference: 28.561069428076085
At round 24 accuracy: 0.8239598861634367
At round 24 training accuracy: 0.819881315260393
At round 24 training loss: 0.6280275205976169
gradient difference: 28.926481278884573
At round 25 accuracy: 0.8402222523377152
At round 25 training accuracy: 0.8370516894740255
At round 25 training loss: 0.591527822989241
gradient difference: 26.548005849223653
At round 26 accuracy: 0.8048516059086597
At round 26 training accuracy: 0.8061482586419353
At round 26 training loss: 0.6401141374744922
gradient difference: 29.376887939849375
At round 27 accuracy: 0.8156931833581786
At round 27 training accuracy: 0.8149685453012517
At round 27 training loss: 0.624001068556979
gradient difference: 28.973020482474535
At round 28 accuracy: 0.8170483805393685
At round 28 training accuracy: 0.8163142875672871
At round 28 training loss: 0.6265395411629501
gradient difference: 28.817416704157736
At round 29 accuracy: 0.8152866242038217
At round 29 training accuracy: 0.8122608470069395
At round 29 training loss: 0.6431024912381751
gradient difference: 29.526051136356973
At round 30 accuracy: 0.8231467678547228
At round 30 training accuracy: 0.8205622932745315
At round 30 training loss: 0.625372505848497
gradient difference: 29.12421681670361
At round 31 accuracy: 0.832226588968695
At round 31 training accuracy: 0.8298689928010896
At round 31 training loss: 0.6024201016879644
gradient difference: 27.699989680455936
At round 32 accuracy: 0.8307358720693861
At round 32 training accuracy: 0.8314255139762631
At round 32 training loss: 0.592353042215592
gradient difference: 27.165359694519402
At round 33 accuracy: 0.8479468762704974
At round 33 training accuracy: 0.8438290420909268
At round 33 training loss: 0.5651726957192648
gradient difference: 25.898732227598934
At round 34 accuracy: 0.8372408185390975
At round 34 training accuracy: 0.8361923600752319
At round 34 training loss: 0.5663970042019116
gradient difference: 25.881969932826998
At round 35 accuracy: 0.8480823959886163
At round 35 training accuracy: 0.8483688955185161
At round 35 training loss: 0.5371447895621594
gradient difference: 24.59529055514141
At round 36 accuracy: 0.8468627185255455
At round 36 training accuracy: 0.8428562163564434
At round 36 training loss: 0.5580995555713891
gradient difference: 25.636670616457504
At round 37 accuracy: 0.822062610109771
At round 37 training accuracy: 0.8200758804072897
At round 37 training loss: 0.58513680954855
gradient difference: 26.789904239525832
At round 38 accuracy: 0.8551294213308036
At round 38 training accuracy: 0.8541572086386925
At round 38 training loss: 0.528674584176971
gradient difference: 23.23143737334275
At round 39 accuracy: 0.8724759452500339
At round 39 training accuracy: 0.8717977819573254
At round 39 training loss: 0.4811470135247305
gradient difference: 21.632198328146302
At round 40 accuracy: 0.8663775579346795
At round 40 training accuracy: 0.8648907192424931
At round 40 training loss: 0.4965536888195689
gradient difference: 22.28286883574581
At round 41 accuracy: 0.8582463748475403
At round 41 training accuracy: 0.8585349244438679
At round 41 training loss: 0.5018236353175949
gradient difference: 22.445370515337718
At round 42 accuracy: 0.8685458734245832
At round 42 training accuracy: 0.8654257733964589
At round 42 training loss: 0.48665631419000965
gradient difference: 22.018062598735813
At round 43 accuracy: 0.871256267786963
At round 43 training accuracy: 0.8671606459562877
At round 43 training loss: 0.4835803118051843
gradient difference: 21.778211455031595
At round 44 accuracy: 0.8699010706057732
At round 44 training accuracy: 0.8657014073545626
At round 44 training loss: 0.48463899549875994
gradient difference: 21.98143989389109
At round 45 accuracy: 0.8685458734245832
At round 45 training accuracy: 0.8657986899280109
At round 45 training loss: 0.48338917786706914
gradient difference: 22.084775584001754
At round 46 accuracy: 0.8659709987803226
At round 46 training accuracy: 0.866009468837149
At round 46 training loss: 0.4811246605152441
gradient difference: 21.978609281108415
At round 47 accuracy: 0.8480823959886163
At round 47 training accuracy: 0.8507523185680005
At round 47 training loss: 0.5122478016068633
gradient difference: 23.186027649504865
At round 48 accuracy: 0.8453720016262366
At round 48 training accuracy: 0.8481094753226539
At round 48 training loss: 0.517961414929399
gradient difference: 23.65326304971456
At round 49 accuracy: 0.8549939016126846
At round 49 training accuracy: 0.8584052143459369
At round 49 training loss: 0.5018111529401099
gradient difference: 22.960978298013025
At round 50 accuracy: 0.8616343678005149
At round 50 training accuracy: 0.8636098320254232
At round 50 training loss: 0.48094192182788836
gradient difference: 22.28535773805243
At round 51 accuracy: 0.8583818945656593
At round 51 training accuracy: 0.8610804851157663
At round 51 training loss: 0.48339645781694596
gradient difference: 22.49298626695472
At round 52 accuracy: 0.8628540452635859
At round 52 training accuracy: 0.8634314806407679
At round 52 training loss: 0.4795101130432507
gradient difference: 22.480324549115767
At round 53 accuracy: 0.8685458734245832
At round 53 training accuracy: 0.8696089240547377
At round 53 training loss: 0.46700197867278426
gradient difference: 21.403494198817345
At round 54 accuracy: 0.8625830058273479
At round 54 training accuracy: 0.8655230559699073
At round 54 training loss: 0.4745896025896928
gradient difference: 21.75355651191957
At round 55 accuracy: 0.8797940100284591
At round 55 training accuracy: 0.8813639016797458
At round 55 training loss: 0.4386774221074837
gradient difference: 19.66731290038824
At round 56 accuracy: 0.8750508198942947
At round 56 training accuracy: 0.8782022180426746
At round 56 training loss: 0.44108276289343934
gradient difference: 19.491728771086148
At round 57 accuracy: 0.8780322536929123
At round 57 training accuracy: 0.8800343731759518
At round 57 training loss: 0.4348143013758952
gradient difference: 19.32708045058625
At round 58 accuracy: 0.8669196368071554
At round 58 training accuracy: 0.8716356443349115
At round 58 training loss: 0.45133915297899424
gradient difference: 19.861422836712755
At round 59 accuracy: 0.8659709987803226
At round 59 training accuracy: 0.8698034892016343
At round 59 training loss: 0.45457077931466683
gradient difference: 20.115458828400477
At round 60 accuracy: 0.8213850115191761
At round 60 training accuracy: 0.8262695375835009
At round 60 training loss: 0.5331092598279732
gradient difference: 23.822712836136905
At round 61 accuracy: 0.8331752269955278
At round 61 training accuracy: 0.836322070173163
At round 61 training loss: 0.5070321005104361
gradient difference: 23.068710470080507
At round 62 accuracy: 0.8623119663911099
At round 62 training accuracy: 0.8644367338997341
At round 62 training loss: 0.45359924502461296
gradient difference: 20.532139985229595
At round 63 accuracy: 0.8671906762433934
At round 63 training accuracy: 0.8697224203904274
At round 63 training loss: 0.4383994790919478
gradient difference: 19.91746068843453
At round 64 accuracy: 0.8694945114514162
At round 64 training accuracy: 0.8715707892859459
At round 64 training loss: 0.433923726143561
gradient difference: 19.800886892809917
At round 65 accuracy: 0.8724759452500339
At round 65 training accuracy: 0.8733867306569817
At round 65 training loss: 0.43254311770656517
gradient difference: 19.2769606767846
At round 66 accuracy: 0.881826805800244
At round 66 training accuracy: 0.8823691549387119
At round 66 training loss: 0.41547151561781165
gradient difference: 18.340519569336042
At round 67 accuracy: 0.8785743325653883
At round 67 training accuracy: 0.8807477787145729
At round 67 training loss: 0.4163336460852788
gradient difference: 18.365083451215003
At round 68 accuracy: 0.8757284184848896
At round 68 training accuracy: 0.878007652895778
At round 68 training loss: 0.4245742448085001
gradient difference: 19.261957974364204
At round 69 accuracy: 0.8787098522835073
At round 69 training accuracy: 0.8808612750502627
At round 69 training loss: 0.42434329514156943
gradient difference: 18.930766730447136
At round 70 accuracy: 0.8812847269277679
At round 70 training accuracy: 0.8849309293728517
At round 70 training loss: 0.4143306122006399
gradient difference: 18.514644399658415
At round 71 accuracy: 0.8846727198807427
At round 71 training accuracy: 0.8867144432194046
At round 71 training loss: 0.40703989337648727
gradient difference: 18.305937856316774
At round 72 accuracy: 0.8883317522699553
At round 72 training accuracy: 0.8886600946883715
At round 72 training loss: 0.40926304818867015
gradient difference: 18.38689114462779
At round 73 accuracy: 0.8861634367800515
At round 73 training accuracy: 0.8855956936247487
At round 73 training loss: 0.4187879132155402
gradient difference: 18.750177930106737
At round 74 accuracy: 0.8827754438270768
At round 74 training accuracy: 0.880958557623711
At round 74 training loss: 0.43004526943400234
gradient difference: 19.282328067188278
At round 75 accuracy: 0.890229028323621
At round 75 training accuracy: 0.8881898955833711
At round 75 training loss: 0.4126338026268833
gradient difference: 18.266788472412205
At round 76 accuracy: 0.8926683832497628
At round 76 training accuracy: 0.8906381736818211
At round 76 training loss: 0.3993627320371297
gradient difference: 17.70274104211082
At round 77 accuracy: 0.8923973438135249
At round 77 training accuracy: 0.8912380828847526
At round 77 training loss: 0.3955073703490055
gradient difference: 17.449734158735193
At round 78 accuracy: 0.8876541536793603
At round 78 training accuracy: 0.8880439717231986
At round 78 training loss: 0.39773965333327793
gradient difference: 17.536535279017585
At round 79 accuracy: 0.8899579888873831
At round 79 training accuracy: 0.8904111810104417
At round 79 training loss: 0.3906646461530286
gradient difference: 17.370761598924453
At round 80 accuracy: 0.8872475945250033
At round 80 training accuracy: 0.8890492249821649
At round 80 training loss: 0.39406443198770624
gradient difference: 17.48580946589742
At round 81 accuracy: 0.8900935086055021
At round 81 training accuracy: 0.8910435177378558
At round 81 training loss: 0.3918261923332547
gradient difference: 17.133020527248732
At round 82 accuracy: 0.8909066269142161
At round 82 training accuracy: 0.8910435177378558
At round 82 training loss: 0.39008825820451454
gradient difference: 17.053161454630747
At round 83 accuracy: 0.891448705786692
At round 83 training accuracy: 0.8921946948569947
At round 83 training loss: 0.3860422713734363
gradient difference: 16.96474993053663
At round 84 accuracy: 0.8892803902967882
At round 84 training accuracy: 0.8934755820740644
At round 84 training loss: 0.38194503749404435
gradient difference: 16.829926423792312
At round 85 accuracy: 0.8860279170619325
At round 85 training accuracy: 0.8899571956676827
At round 85 training loss: 0.3901459968016634
gradient difference: 16.92783699649226
At round 86 accuracy: 0.8872475945250033
At round 86 training accuracy: 0.8900220507166483
At round 86 training loss: 0.39087736021218866
gradient difference: 17.29506975352187
At round 87 accuracy: 0.8868410353706464
At round 87 training accuracy: 0.888806018548544
At round 87 training loss: 0.3947204490607431
gradient difference: 17.438974354262083
At round 88 accuracy: 0.8865699959344084
At round 88 training accuracy: 0.8884493157792334
At round 88 training loss: 0.395722428900643
gradient difference: 17.48809847462282
At round 89 accuracy: 0.8857568776256945
At round 89 training accuracy: 0.8891951488423374
At round 89 training loss: 0.3967732989690544
gradient difference: 17.446532638885667
At round 90 accuracy: 0.8877896733974794
At round 90 training accuracy: 0.8909462351644075
At round 90 training loss: 0.39537537816025975
gradient difference: 17.455623650282718
At round 91 accuracy: 0.8862989564981705
At round 91 training accuracy: 0.8897788442830274
At round 91 training loss: 0.39503614943938653
gradient difference: 17.443440718971278
At round 92 accuracy: 0.8861634367800515
At round 92 training accuracy: 0.890135547052338
At round 92 training loss: 0.3943815963553344
gradient difference: 17.34399936608502
At round 93 accuracy: 0.8743732213036997
At round 93 training accuracy: 0.8773266748816395
At round 93 training loss: 0.41412939069499927
gradient difference: 18.22902525337328
At round 94 accuracy: 0.8823688846727199
At round 94 training accuracy: 0.8863577404500941
At round 94 training loss: 0.39962340775572575
gradient difference: 17.495551932041405
At round 95 accuracy: 0.8791164114378642
At round 95 training accuracy: 0.8834392632466438
At round 95 training loss: 0.40525362088222194
gradient difference: 17.666144083741507
At round 96 accuracy: 0.8743732213036997
At round 96 training accuracy: 0.8790939749659511
At round 96 training loss: 0.4147703776143168
gradient difference: 18.215189433013833
At round 97 accuracy: 0.870714188914487
At round 97 training accuracy: 0.8736785783773267
At round 97 training loss: 0.4265838829675215
gradient difference: 18.466208530492107
At round 98 accuracy: 0.8623119663911099
At round 98 training accuracy: 0.8638043971723198
At round 98 training loss: 0.4419322527300684
gradient difference: 19.265643579428005
At round 99 accuracy: 0.8604146903374441
At round 99 training accuracy: 0.8630099228224918
At round 99 training loss: 0.4477925504866377
gradient difference: 19.345547910527667
At round 100 accuracy: 0.8749153001761756
At round 100 training accuracy: 0.8804397172319866
At round 100 training loss: 0.4061148973730043
gradient difference: 17.975781006567665
At round 101 accuracy: 0.8762704973573655
At round 101 training accuracy: 0.8836824696802646
At round 101 training loss: 0.403135277565654
gradient difference: 17.91650438194514
At round 102 accuracy: 0.8736956227131047
At round 102 training accuracy: 0.8805207860431935
At round 102 training loss: 0.409355664621717
gradient difference: 18.172597171399886
At round 103 accuracy: 0.8776256945385553
At round 103 training accuracy: 0.8851254945197483
At round 103 training loss: 0.40872495051136826
gradient difference: 17.290782594522177
At round 104 accuracy: 0.8770836156660794
At round 104 training accuracy: 0.8827258577080226
At round 104 training loss: 0.40803604993202836
gradient difference: 17.449913765610674
At round 105 accuracy: 0.8787098522835073
At round 105 training accuracy: 0.8854173422400934
At round 105 training loss: 0.4062938296796438
gradient difference: 17.61446549754351
At round 106 accuracy: 0.8820978452364819
At round 106 training accuracy: 0.8886925222128542
At round 106 training loss: 0.3993410965302968
gradient difference: 17.425533895069393
At round 107 accuracy: 0.8853503184713376
At round 107 training accuracy: 0.8915785718918218
At round 107 training loss: 0.3882677810360779
gradient difference: 16.787005798946105
At round 108 accuracy: 0.8834530424176718
At round 108 training accuracy: 0.8895032103249237
At round 108 training loss: 0.39393571381442594
gradient difference: 17.03068422685845
At round 109 accuracy: 0.8845372001626237
At round 109 training accuracy: 0.8902004021013036
At round 109 training loss: 0.39136207498340875
gradient difference: 17.08322161584353
At round 110 accuracy: 0.8876541536793603
At round 110 training accuracy: 0.8913191516959595
At round 110 training loss: 0.37421479337740665
gradient difference: 16.58240012583618
At round 111 accuracy: 0.8929394226860008
At round 111 training accuracy: 0.8970588235294118
At round 111 training loss: 0.3632379944063397
gradient difference: 16.245774605821712
At round 112 accuracy: 0.8955142973302616
At round 112 training accuracy: 0.9005934236980349
At round 112 training loss: 0.35653109191590415
gradient difference: 15.77994980178474
At round 113 accuracy: 0.8925328635316438
At round 113 training accuracy: 0.9007069200337247
At round 113 training loss: 0.35816757368000324
gradient difference: 15.49900827409772
At round 114 accuracy: 0.8888738311424312
At round 114 training accuracy: 0.8979343666904469
At round 114 training loss: 0.3697819968044835
gradient difference: 16.03227097184424
At round 115 accuracy: 0.8940235804309528
At round 115 training accuracy: 0.9006744925092418
At round 115 training loss: 0.36226578738552007
gradient difference: 15.875861031315145
At round 116 accuracy: 0.89171974522293
At round 116 training accuracy: 0.8979505804526883
At round 116 training loss: 0.3684035172443229
gradient difference: 15.987032961055041
At round 117 accuracy: 0.8890093508605502
At round 117 training accuracy: 0.8959725014592386
At round 117 training loss: 0.3723393396131162
gradient difference: 16.207396115571242
At round 118 accuracy: 0.891313186068573
At round 118 training accuracy: 0.8977884428302744
At round 118 training loss: 0.3664770862588935
gradient difference: 16.14429649360021
At round 119 accuracy: 0.8937525409947147
At round 119 training accuracy: 0.900269148453207
At round 119 training loss: 0.36327393170157685
gradient difference: 16.02396563183747
At round 120 accuracy: 0.8867055156525274
At round 120 training accuracy: 0.8930215967313055
At round 120 training loss: 0.3805722078013767
gradient difference: 16.621722556365086
At round 121 accuracy: 0.8858923973438135
At round 121 training accuracy: 0.8920811985213049
At round 121 training loss: 0.3874742627629544
gradient difference: 16.803229125179133
At round 122 accuracy: 0.8854858381894566
At round 122 training accuracy: 0.8913353654582009
At round 122 training loss: 0.37231716991061126
gradient difference: 16.403577769099666
At round 123 accuracy: 0.8860279170619325
At round 123 training accuracy: 0.8906868149685453
At round 123 training loss: 0.374405243922686
gradient difference: 16.42373924137291
At round 124 accuracy: 0.8894159100149072
At round 124 training accuracy: 0.891627213178546
At round 124 training loss: 0.3731522719650221
gradient difference: 16.514567051192433
At round 125 accuracy: 0.8922618240954059
At round 125 training accuracy: 0.8944321940463065
At round 125 training loss: 0.36874840996787533
gradient difference: 16.506315270329857
At round 126 accuracy: 0.8942946198671907
At round 126 training accuracy: 0.8970101822426876
At round 126 training loss: 0.35992435804127365
gradient difference: 16.276579334699242
At round 127 accuracy: 0.8933459818403577
At round 127 training accuracy: 0.8979667942149296
At round 127 training loss: 0.36159995726749256
gradient difference: 16.313956357987706
At round 128 accuracy: 0.8825044043908389
At round 128 training accuracy: 0.8843796614566444
At round 128 training loss: 0.377413721506899
gradient difference: 17.251001380436808
At round 129 accuracy: 0.8850792790350996
At round 129 training accuracy: 0.8880763992476814
At round 129 training loss: 0.37713920875846335
gradient difference: 17.15053495846
At round 130 accuracy: 0.8942946198671907
At round 130 training accuracy: 0.895615798689928
At round 130 training loss: 0.36373277637337786
gradient difference: 16.555359595714172
At round 131 accuracy: 0.8957853367664995
At round 131 training accuracy: 0.8984694208444127
At round 131 training loss: 0.3587362121179269
gradient difference: 16.192284842900857
At round 132 accuracy: 0.8989022902832362
At round 132 training accuracy: 0.9002205071664829
At round 132 training loss: 0.3542344845880077
gradient difference: 15.60335855915974
At round 133 accuracy: 0.8978181325382844
At round 133 training accuracy: 0.8983883520332058
At round 133 training loss: 0.3596868714630194
gradient difference: 15.860419853713218
At round 134 accuracy: 0.8976826128201654
At round 134 training accuracy: 0.8989720474738958
At round 134 training loss: 0.3550733886336604
gradient difference: 15.6221853267542
At round 135 accuracy: 0.8963274156389754
At round 135 training accuracy: 0.895615798689928
At round 135 training loss: 0.3641923119116383
gradient difference: 16.17586905688197
At round 136 accuracy: 0.8936170212765957
At round 136 training accuracy: 0.895242882158376
At round 136 training loss: 0.3609986278578848
gradient difference: 16.26899690796522
At round 137 accuracy: 0.891448705786692
At round 137 training accuracy: 0.891756923276477
At round 137 training loss: 0.372036940170268
gradient difference: 16.786246413433666
At round 138 accuracy: 0.8937525409947147
At round 138 training accuracy: 0.8967994033335496
At round 138 training loss: 0.36113229678212055
gradient difference: 16.18270726996076
At round 139 accuracy: 0.90107060577314
At round 139 training accuracy: 0.9030417017964849
At round 139 training loss: 0.3504656466968431
gradient difference: 15.715336628600774
At round 140 accuracy: 0.8983602114107603
At round 140 training accuracy: 0.9015176081457942
At round 140 training loss: 0.35200965919292876
gradient difference: 15.824830349998747
At round 141 accuracy: 0.8972760536658084
At round 141 training accuracy: 0.9005285686490694
At round 141 training loss: 0.35098086876545964
gradient difference: 15.840634258187942
At round 142 accuracy: 0.8959208564846185
At round 142 training accuracy: 0.8967183345223426
At round 142 training loss: 0.36764706820494786
gradient difference: 16.72671032893978
At round 143 accuracy: 0.8934815015584767
At round 143 training accuracy: 0.897772229068033
At round 143 training loss: 0.3677982514416634
gradient difference: 16.830771360956714
At round 144 accuracy: 0.8975470931020464
At round 144 training accuracy: 0.8995071016278617
At round 144 training loss: 0.36027930452005397
gradient difference: 16.59043265808101
At round 145 accuracy: 0.8967339747933324
At round 145 training accuracy: 0.8995395291523445
At round 145 training loss: 0.36066273150587036
gradient difference: 16.52876160797728
At round 146 accuracy: 0.8972760536658084
At round 146 training accuracy: 0.902506647642519
At round 146 training loss: 0.3541684965317961
gradient difference: 16.208021700724945
At round 147 accuracy: 0.8955142973302616
At round 147 training accuracy: 0.898728841040275
At round 147 training loss: 0.36800981139364575
gradient difference: 16.67239367346996
At round 148 accuracy: 0.8959208564846185
At round 148 training accuracy: 0.89902068876062
At round 148 training loss: 0.3644429601938991
gradient difference: 16.448075559318433
At round 149 accuracy: 0.891177666350454
At round 149 training accuracy: 0.8956806537388936
At round 149 training loss: 0.3692059644278645
gradient difference: 16.113196579132975
At round 150 accuracy: 0.8896869494511451
At round 150 training accuracy: 0.8926648939619949
At round 150 training loss: 0.3718675135318218
gradient difference: 16.33015923883762
At round 151 accuracy: 0.8919907846591679
At round 151 training accuracy: 0.8920001297100979
At round 151 training loss: 0.3796209525828097
gradient difference: 17.031684488292413
At round 152 accuracy: 0.8891448705786692
At round 152 training accuracy: 0.8938322848433751
At round 152 training loss: 0.37682317775823715
gradient difference: 17.03054576208867
At round 153 accuracy: 0.8884672719880743
At round 153 training accuracy: 0.890800311304235
At round 153 training loss: 0.37748953536037716
gradient difference: 17.00005242434105
At round 154 accuracy: 0.8896869494511451
At round 154 training accuracy: 0.8914164342694079
At round 154 training loss: 0.373821566578746
gradient difference: 16.685648947203454
At round 155 accuracy: 0.8880607128337173
At round 155 training accuracy: 0.888887087359751
At round 155 training loss: 0.3851788015496319
gradient difference: 16.69514245268402
At round 156 accuracy: 0.8898224691692641
At round 156 training accuracy: 0.8912218691225112
At round 156 training loss: 0.3771461107856998
gradient difference: 16.574303375909086
At round 157 accuracy: 0.8860279170619325
At round 157 training accuracy: 0.8866171606459563
At round 157 training loss: 0.3858249471369072
gradient difference: 16.727074661616236
At round 158 accuracy: 0.8881962325518363
At round 158 training accuracy: 0.8896653479473376
At round 158 training loss: 0.3746970692723837
gradient difference: 16.514602050799713
At round 159 accuracy: 0.890771107196097
At round 159 training accuracy: 0.892875672871133
At round 159 training loss: 0.365058117082307
gradient difference: 16.26096299336045
At round 160 accuracy: 0.8964629353570944
At round 160 training accuracy: 0.8988423373759648
At round 160 training loss: 0.3527784792435955
gradient difference: 15.70534372338328
At round 161 accuracy: 0.8972760536658084
At round 161 training accuracy: 0.9003988585511382
At round 161 training loss: 0.3465846020313878
gradient difference: 15.21798071296536
At round 162 accuracy: 0.8932104621222388
At round 162 training accuracy: 0.9003988585511382
At round 162 training loss: 0.34908909822923917
gradient difference: 12.63445234578558
At round 163 accuracy: 0.8955142973302616
At round 163 training accuracy: 0.9026201439782087
At round 163 training loss: 0.34368974082339226
gradient difference: 12.280755643174976
At round 164 accuracy: 0.8921263043772869
At round 164 training accuracy: 0.8990693300473442
At round 164 training loss: 0.35141216045211343
gradient difference: 12.497587579266344
At round 165 accuracy: 0.8919907846591679
At round 165 training accuracy: 0.8988423373759648
At round 165 training loss: 0.3486667678681058
gradient difference: 12.315016158595908
At round 166 accuracy: 0.8892803902967882
At round 166 training accuracy: 0.896069784032687
At round 166 training loss: 0.3563123234734072
gradient difference: 12.515498628458921
At round 167 accuracy: 0.8858923973438135
At round 167 training accuracy: 0.8936377196964783
At round 167 training loss: 0.35998079918319026
gradient difference: 12.554728737382
At round 168 accuracy: 0.8881962325518363
At round 168 training accuracy: 0.8970426097671704
At round 168 training loss: 0.35278141687756615
gradient difference: 12.378334283566884
At round 169 accuracy: 0.8959208564846185
At round 169 training accuracy: 0.901760814579415
At round 169 training loss: 0.34425930984422026
gradient difference: 12.113595042742373
At round 170 accuracy: 0.8942946198671907
At round 170 training accuracy: 0.901971593488553
At round 170 training loss: 0.34288848176308867
gradient difference: 12.03439551923822
At round 171 accuracy: 0.9006640466187831
At round 171 training accuracy: 0.906576301965108
At round 171 training loss: 0.33306667239910953
gradient difference: 11.704900732013993
At round 172 accuracy: 0.901206125491259
At round 172 training accuracy: 0.9074680588883844
At round 172 training loss: 0.3320630320275183
gradient difference: 11.755309866362799
At round 173 accuracy: 0.900257487464426
At round 173 training accuracy: 0.9061547441468318
At round 173 training loss: 0.33314305259121035
gradient difference: 11.834395770436934
At round 174 accuracy: 0.900257487464426
At round 174 training accuracy: 0.9057494000907971
At round 174 training loss: 0.3352300532314883
gradient difference: 11.932809571134557
At round 175 accuracy: 0.8896869494511451
At round 175 training accuracy: 0.8943673389973409
At round 175 training loss: 0.3550039979277564
gradient difference: 12.647333057632567
At round 176 accuracy: 0.8949722184577856
At round 176 training accuracy: 0.9010636228030352
At round 176 training loss: 0.3394448879075891
gradient difference: 12.080562441439833
At round 177 accuracy: 0.8978181325382844
At round 177 training accuracy: 0.9028471366495882
At round 177 training loss: 0.33833927776691314
gradient difference: 11.906238623696009
At round 178 accuracy: 0.8968694945114514
At round 178 training accuracy: 0.9019878072507945
At round 178 training loss: 0.3391679906305265
gradient difference: 11.866150387837507
At round 179 accuracy: 0.8976826128201654
At round 179 training accuracy: 0.9016635320059667
At round 179 training loss: 0.33823724478374234
gradient difference: 11.855858876636015
At round 180 accuracy: 0.881555766364006
At round 180 training accuracy: 0.8848660743238861
At round 180 training loss: 0.37100873049007327
gradient difference: 13.307099167465445
At round 181 accuracy: 0.8844016804445046
At round 181 training accuracy: 0.887103573513198
At round 181 training loss: 0.3651317267889055
gradient difference: 13.02447194725301
At round 182 accuracy: 0.8850792790350996
At round 182 training accuracy: 0.8864550230235424
At round 182 training loss: 0.3636316403746182
gradient difference: 12.850731761626399
At round 183 accuracy: 0.8896869494511451
At round 183 training accuracy: 0.89050846358389
At round 183 training loss: 0.35567004387811446
gradient difference: 12.52763258754521
At round 184 accuracy: 0.8899579888873831
At round 184 training accuracy: 0.8893086451780271
At round 184 training loss: 0.3564235409270235
gradient difference: 12.588898245454388
At round 185 accuracy: 0.891313186068573
At round 185 training accuracy: 0.8928594591088916
At round 185 training loss: 0.3503327477764493
gradient difference: 12.41241445613516
At round 186 accuracy: 0.8856213579075756
At round 186 training accuracy: 0.8876710551916467
At round 186 training loss: 0.3589444637492894
gradient difference: 12.640322179718234
At round 187 accuracy: 0.8890093508605502
At round 187 training accuracy: 0.891546144367339
At round 187 training loss: 0.3505140202800605
gradient difference: 12.42836613657862
At round 188 accuracy: 0.8959208564846185
At round 188 training accuracy: 0.8982748556975161
At round 188 training loss: 0.33993373846337926
gradient difference: 12.04924513785889
At round 189 accuracy: 0.8825044043908389
At round 189 training accuracy: 0.8863577404500941
At round 189 training loss: 0.35828402017311034
gradient difference: 12.97793748364938
At round 190 accuracy: 0.8957853367664995
At round 190 training accuracy: 0.897772229068033
At round 190 training loss: 0.33738465772752535
gradient difference: 11.986067619881224
At round 191 accuracy: 0.9020192437999729
At round 191 training accuracy: 0.9037388935728646
At round 191 training loss: 0.33009725821062724
gradient difference: 11.706234603863862
At round 192 accuracy: 0.8979536522564033
At round 192 training accuracy: 0.9030741293209676
At round 192 training loss: 0.33178670862367676
gradient difference: 11.666407393638957
At round 193 accuracy: 0.9006640466187831
At round 193 training accuracy: 0.9053116285102796
At round 193 training loss: 0.329212778690632
gradient difference: 11.498230996329825
At round 194 accuracy: 0.9005285269006641
At round 194 training accuracy: 0.9054089110837279
At round 194 training loss: 0.3297363667090136
gradient difference: 11.493984706810352
At round 195 accuracy: 0.8974115733839274
At round 195 training accuracy: 0.9027012127894156
At round 195 training loss: 0.33640408949295064
gradient difference: 11.712155836261926
At round 196 accuracy: 0.8979536522564033
At round 196 training accuracy: 0.9027012127894156
At round 196 training loss: 0.3359312211516748
gradient difference: 11.72969038132928
At round 197 accuracy: 0.8972760536658084
At round 197 training accuracy: 0.9018905246773461
At round 197 training loss: 0.3344848375963229
gradient difference: 11.659498886674868
At round 198 accuracy: 0.8972760536658084
At round 198 training accuracy: 0.9010474090407938
At round 198 training loss: 0.337935771041135
gradient difference: 11.78204080753941
At round 199 accuracy: 0.8961918959208565
At round 199 training accuracy: 0.9008690576561386
At round 199 training loss: 0.3384814557325059
gradient difference: 11.807079428728747
At round 200 accuracy: 0.8990378100013552
At round 200 training accuracy: 0.9042253064401063
