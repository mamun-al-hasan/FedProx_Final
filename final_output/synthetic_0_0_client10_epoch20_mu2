Arguments:
	       batch_size : 10
	clients_per_round : 10
	          dataset : synthetic_0_0
	     drop_percent : 0.0
	       eval_every : 1
	    learning_rate : 0.01
	            model : mclr
	     model_params : (10,)
	               mu : 2.0
	       num_epochs : 20
	        num_iters : 1
	       num_rounds : 200
	        optimizer : fedprox
	             seed : 0
Using Federated prox to Train

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/3.64k flops)
  dense/kernel/Initializer/stateless_random_uniform (600/1.20k flops)
    dense/kernel/Initializer/stateless_random_uniform/mul (600/600 flops)
    dense/kernel/Initializer/stateless_random_uniform/sub (1/1 flops)
  PGD/update_dense/kernel/add (600/600 flops)
  PGD/update_dense/kernel/mul (600/600 flops)
  PGD/update_dense/kernel/mul_1 (600/600 flops)
  PGD/update_dense/kernel/sub (600/600 flops)
  PGD/update_dense/bias/add (10/10 flops)
  PGD/update_dense/bias/mul (10/10 flops)
  PGD/update_dense/bias/mul_1 (10/10 flops)
  PGD/update_dense/bias/sub (10/10 flops)
  gradients/sparse_softmax_cross_entropy_loss/value_grad/Neg (1/1 flops)
  gradients/sparse_softmax_cross_entropy_loss/value_grad/mul (1/1 flops)
  sparse_softmax_cross_entropy_loss/num_present/Equal (1/1 flops)

======================End of Report==========================
30 Clients in Total
Training with 10 workers ---
At round 0 accuracy: 0.09186746987951808
At round 0 training accuracy: 0.0941358024691358
At round 0 training loss: 3.3660748895201498
gradient difference: 49.98099832688448
At round 1 accuracy: 0.14457831325301204
At round 1 training accuracy: 0.14591906721536352
At round 1 training loss: 2.787229345117495
gradient difference: 48.17561673416044
At round 2 accuracy: 0.26506024096385544
At round 2 training accuracy: 0.269718792866941
At round 2 training loss: 2.455720552466861
gradient difference: 45.53501875408711
At round 3 accuracy: 0.27710843373493976
At round 3 training accuracy: 0.28412208504801095
At round 3 training loss: 2.109820317287913
gradient difference: 43.77720293484182
At round 4 accuracy: 0.26506024096385544
At round 4 training accuracy: 0.27503429355281206
At round 4 training loss: 2.032834707423329
gradient difference: 43.60760482906935
At round 5 accuracy: 0.35993975903614456
At round 5 training accuracy: 0.37774348422496573
At round 5 training loss: 1.8996637298409995
gradient difference: 42.03090007504868
At round 6 accuracy: 0.411144578313253
At round 6 training accuracy: 0.4267832647462277
At round 6 training loss: 1.7994623863443588
gradient difference: 41.0996820885538
At round 7 accuracy: 0.4713855421686747
At round 7 training accuracy: 0.4699931412894376
At round 7 training loss: 1.7365248201642345
gradient difference: 40.39250760148776
At round 8 accuracy: 0.48042168674698793
At round 8 training accuracy: 0.4886831275720165
At round 8 training loss: 1.6715720946579886
gradient difference: 39.82156838593382
At round 9 accuracy: 0.5286144578313253
At round 9 training accuracy: 0.5272633744855967
At round 9 training loss: 1.6142500135102227
gradient difference: 39.581651008934806
At round 10 accuracy: 0.5466867469879518
At round 10 training accuracy: 0.5576131687242798
At round 10 training loss: 1.5614272744608186
gradient difference: 38.97582826048957
At round 11 accuracy: 0.5512048192771084
At round 11 training accuracy: 0.5723593964334706
At round 11 training loss: 1.5137675334846368
gradient difference: 37.64652402582126
At round 12 accuracy: 0.5587349397590361
At round 12 training accuracy: 0.5739026063100137
At round 12 training loss: 1.4534073295672163
gradient difference: 36.453459891267094
At round 13 accuracy: 0.5557228915662651
At round 13 training accuracy: 0.5691015089163237
At round 13 training loss: 1.4207055747652086
gradient difference: 36.34970634387978
At round 14 accuracy: 0.5813253012048193
At round 14 training accuracy: 0.586076817558299
At round 14 training loss: 1.3851593187020066
gradient difference: 35.55637849175943
At round 15 accuracy: 0.5677710843373494
At round 15 training accuracy: 0.5768175582990398
At round 15 training loss: 1.3718783903422425
gradient difference: 35.3524645526745
At round 16 accuracy: 0.5647590361445783
At round 16 training accuracy: 0.5713305898491083
At round 16 training loss: 1.3595692309434686
gradient difference: 35.14703773151305
At round 17 accuracy: 0.5632530120481928
At round 17 training accuracy: 0.5737311385459534
At round 17 training loss: 1.320208262289339
gradient difference: 34.095066688813475
At round 18 accuracy: 0.5828313253012049
At round 18 training accuracy: 0.5953360768175583
At round 18 training loss: 1.2814253761812493
gradient difference: 33.705645113612235
At round 19 accuracy: 0.5783132530120482
At round 19 training accuracy: 0.5968792866941015
At round 19 training loss: 1.2416709119298166
gradient difference: 33.212616661648106
At round 20 accuracy: 0.5828313253012049
At round 20 training accuracy: 0.5999657064471879
At round 20 training loss: 1.2225699179380396
gradient difference: 33.15037240086126
At round 21 accuracy: 0.5813253012048193
At round 21 training accuracy: 0.5982510288065843
At round 21 training loss: 1.1942565655782922
gradient difference: 32.95413189545093
At round 22 accuracy: 0.5933734939759037
At round 22 training accuracy: 0.6080246913580247
At round 22 training loss: 1.1761711201028815
gradient difference: 31.332828501156502
At round 23 accuracy: 0.6009036144578314
At round 23 training accuracy: 0.6109396433470508
At round 23 training loss: 1.1625453982469005
gradient difference: 31.030138498420794
At round 24 accuracy: 0.6009036144578314
At round 24 training accuracy: 0.6133401920438958
At round 24 training loss: 1.1367513245298682
gradient difference: 30.60428950096769
At round 25 accuracy: 0.6069277108433735
At round 25 training accuracy: 0.6208847736625515
At round 25 training loss: 1.1008104651065131
gradient difference: 29.80594223480073
At round 26 accuracy: 0.608433734939759
At round 26 training accuracy: 0.6213991769547325
At round 26 training loss: 1.085703997087086
gradient difference: 29.62979544835879
At round 27 accuracy: 0.6129518072289156
At round 27 training accuracy: 0.6237997256515775
At round 27 training loss: 1.0736297680478957
gradient difference: 29.376849789138035
At round 28 accuracy: 0.6325301204819277
At round 28 training accuracy: 0.6486625514403292
At round 28 training loss: 1.0503440116466993
gradient difference: 28.322066085938403
At round 29 accuracy: 0.641566265060241
At round 29 training accuracy: 0.6536351165980796
At round 29 training loss: 1.0346344887382455
gradient difference: 28.285867676343187
At round 30 accuracy: 0.6460843373493976
At round 30 training accuracy: 0.6584362139917695
At round 30 training loss: 1.0264026372794404
gradient difference: 28.24712300914235
At round 31 accuracy: 0.6475903614457831
At round 31 training accuracy: 0.6623799725651578
At round 31 training loss: 1.017465903568088
gradient difference: 28.171220298558413
At round 32 accuracy: 0.6475903614457831
At round 32 training accuracy: 0.6659807956104252
At round 32 training loss: 1.00228195706132
gradient difference: 27.88288386665563
At round 33 accuracy: 0.6430722891566265
At round 33 training accuracy: 0.668724279835391
At round 33 training loss: 0.9950692479048993
gradient difference: 27.958919618230897
At round 34 accuracy: 0.6490963855421686
At round 34 training accuracy: 0.6666666666666666
At round 34 training loss: 0.9836244507921433
gradient difference: 27.79223048360804
At round 35 accuracy: 0.6566265060240963
At round 35 training accuracy: 0.6754115226337448
At round 35 training loss: 0.9566729134587451
gradient difference: 27.020225192257616
At round 36 accuracy: 0.6566265060240963
At round 36 training accuracy: 0.6778120713305898
At round 36 training loss: 0.9445926402887107
gradient difference: 26.683979097693395
At round 37 accuracy: 0.6551204819277109
At round 37 training accuracy: 0.6791838134430727
At round 37 training loss: 0.9320606494479556
gradient difference: 26.20370119065965
At round 38 accuracy: 0.6551204819277109
At round 38 training accuracy: 0.6796982167352538
At round 38 training loss: 0.9240815649799164
gradient difference: 26.007873077205193
At round 39 accuracy: 0.6536144578313253
At round 39 training accuracy: 0.6803840877914952
At round 39 training loss: 0.9190495767022531
gradient difference: 26.0374962988054
At round 40 accuracy: 0.6566265060240963
At round 40 training accuracy: 0.6826131687242798
At round 40 training loss: 0.9160648014987928
gradient difference: 26.092440936044397
At round 41 accuracy: 0.6671686746987951
At round 41 training accuracy: 0.6868998628257887
At round 41 training loss: 0.8952562537482481
gradient difference: 25.279246775110913
At round 42 accuracy: 0.6686746987951807
At round 42 training accuracy: 0.6881001371742113
At round 42 training loss: 0.8894103841590395
gradient difference: 25.03266761756794
At round 43 accuracy: 0.6641566265060241
At round 43 training accuracy: 0.6879286694101509
At round 43 training loss: 0.883767836537488
gradient difference: 24.921902764154797
At round 44 accuracy: 0.6837349397590361
At round 44 training accuracy: 0.6958161865569273
At round 44 training loss: 0.8714446619603993
gradient difference: 24.461099213991496
At round 45 accuracy: 0.6882530120481928
At round 45 training accuracy: 0.6983882030178327
At round 45 training loss: 0.8640534040615198
gradient difference: 24.335975470456066
At round 46 accuracy: 0.6867469879518072
At round 46 training accuracy: 0.7013031550068587
At round 46 training loss: 0.8564921582484329
gradient difference: 23.97648322496463
At round 47 accuracy: 0.6882530120481928
At round 47 training accuracy: 0.7004458161865569
At round 47 training loss: 0.8518047408460778
gradient difference: 23.852706041068725
At round 48 accuracy: 0.6837349397590361
At round 48 training accuracy: 0.6995884773662552
At round 48 training loss: 0.8485339610931298
gradient difference: 23.819336229313762
At round 49 accuracy: 0.6822289156626506
At round 49 training accuracy: 0.6978737997256516
At round 49 training loss: 0.847455351200499
gradient difference: 23.91605605653178
At round 50 accuracy: 0.6867469879518072
At round 50 training accuracy: 0.7016460905349794
At round 50 training loss: 0.8409534884528582
gradient difference: 23.47549145895919
At round 51 accuracy: 0.6897590361445783
At round 51 training accuracy: 0.7061042524005487
At round 51 training loss: 0.8357944148640558
gradient difference: 23.370556172741367
At round 52 accuracy: 0.6867469879518072
At round 52 training accuracy: 0.7059327846364883
At round 52 training loss: 0.8334405655852571
gradient difference: 23.38068916837978
At round 53 accuracy: 0.6867469879518072
At round 53 training accuracy: 0.7090192043895748
At round 53 training loss: 0.8216479837914241
gradient difference: 22.869398138093675
At round 54 accuracy: 0.6822289156626506
At round 54 training accuracy: 0.7090192043895748
At round 54 training loss: 0.8170473468828492
gradient difference: 22.75896073238457
At round 55 accuracy: 0.6852409638554217
At round 55 training accuracy: 0.7100480109739369
At round 55 training loss: 0.8135445149355429
gradient difference: 22.62727342333259
At round 56 accuracy: 0.6852409638554217
At round 56 training accuracy: 0.7091906721536351
At round 56 training loss: 0.8112750438186347
gradient difference: 22.629692748057593
At round 57 accuracy: 0.6912650602409639
At round 57 training accuracy: 0.7121056241426612
At round 57 training loss: 0.8075431301785487
gradient difference: 22.7079247945858
At round 58 accuracy: 0.6867469879518072
At round 58 training accuracy: 0.7148491083676269
At round 58 training loss: 0.8050503547712828
gradient difference: 22.8506674129541
At round 59 accuracy: 0.6867469879518072
At round 59 training accuracy: 0.7138203017832647
At round 59 training loss: 0.8031516928121637
gradient difference: 22.828155746613152
At round 60 accuracy: 0.6882530120481928
At round 60 training accuracy: 0.7165637860082305
At round 60 training loss: 0.8010878177019237
gradient difference: 22.84305711955781
At round 61 accuracy: 0.6897590361445783
At round 61 training accuracy: 0.71639231824417
At round 61 training loss: 0.7961479719649663
gradient difference: 22.627822179550552
At round 62 accuracy: 0.6912650602409639
At round 62 training accuracy: 0.7181069958847737
At round 62 training loss: 0.7881356717821272
gradient difference: 22.227902604489227
At round 63 accuracy: 0.697289156626506
At round 63 training accuracy: 0.7211934156378601
At round 63 training loss: 0.7674702468745932
gradient difference: 20.91985190562445
At round 64 accuracy: 0.7003012048192772
At round 64 training accuracy: 0.7208504801097394
At round 64 training loss: 0.761648072409846
gradient difference: 20.682582456196695
At round 65 accuracy: 0.6987951807228916
At round 65 training accuracy: 0.7232510288065843
At round 65 training loss: 0.7600600390751886
gradient difference: 20.728945530335874
At round 66 accuracy: 0.7018072289156626
At round 66 training accuracy: 0.723079561042524
At round 66 training loss: 0.7552240352588354
gradient difference: 20.431695886527237
At round 67 accuracy: 0.6987951807228916
At round 67 training accuracy: 0.7211934156378601
At round 67 training loss: 0.7538575593529675
gradient difference: 20.56355507584094
At round 68 accuracy: 0.7003012048192772
At round 68 training accuracy: 0.7229080932784636
At round 68 training loss: 0.7488387299928972
gradient difference: 20.202501575749583
At round 69 accuracy: 0.7078313253012049
At round 69 training accuracy: 0.7210219478737997
At round 69 training loss: 0.7439166348435838
gradient difference: 19.81651608902524
At round 70 accuracy: 0.7063253012048193
At round 70 training accuracy: 0.7275377229080933
At round 70 training loss: 0.7400401753487783
gradient difference: 19.60618721040246
At round 71 accuracy: 0.7123493975903614
At round 71 training accuracy: 0.7295953360768176
At round 71 training loss: 0.7340995063947766
gradient difference: 19.125405006254237
At round 72 accuracy: 0.7108433734939759
At round 72 training accuracy: 0.7309670781893004
At round 72 training loss: 0.7295283730667524
gradient difference: 18.836074200698068
At round 73 accuracy: 0.7123493975903614
At round 73 training accuracy: 0.732681755829904
At round 73 training loss: 0.723350129342882
gradient difference: 18.363222694194583
At round 74 accuracy: 0.7183734939759037
At round 74 training accuracy: 0.73559670781893
At round 74 training loss: 0.721173821308424
gradient difference: 18.21642215455954
At round 75 accuracy: 0.7183734939759037
At round 75 training accuracy: 0.7330246913580247
At round 75 training loss: 0.7189037354435613
gradient difference: 18.31432997697735
At round 76 accuracy: 0.7168674698795181
At round 76 training accuracy: 0.7345679012345679
At round 76 training loss: 0.7103623228986966
gradient difference: 17.974154890298294
At round 77 accuracy: 0.7274096385542169
At round 77 training accuracy: 0.7421124828532236
At round 77 training loss: 0.7028427572361359
gradient difference: 17.446711279185806
At round 78 accuracy: 0.7274096385542169
At round 78 training accuracy: 0.7484567901234568
At round 78 training loss: 0.6970530072094896
gradient difference: 17.358331673515004
At round 79 accuracy: 0.7228915662650602
At round 79 training accuracy: 0.7477709190672154
At round 79 training loss: 0.6943926832508373
gradient difference: 17.320011389860397
At round 80 accuracy: 0.7274096385542169
At round 80 training accuracy: 0.7467421124828533
At round 80 training loss: 0.6903591585014685
gradient difference: 17.156022890296647
At round 81 accuracy: 0.733433734939759
At round 81 training accuracy: 0.75
At round 81 training loss: 0.686869202229285
gradient difference: 16.910595972929258
At round 82 accuracy: 0.7319277108433735
At round 82 training accuracy: 0.7489711934156379
At round 82 training loss: 0.6835227142680891
gradient difference: 16.638564725542537
At round 83 accuracy: 0.7304216867469879
At round 83 training accuracy: 0.7503429355281207
At round 83 training loss: 0.6814333526388868
gradient difference: 16.693153306717164
At round 84 accuracy: 0.733433734939759
At round 84 training accuracy: 0.7539437585733882
At round 84 training loss: 0.6753245717254998
gradient difference: 16.28985397256778
At round 85 accuracy: 0.7394578313253012
At round 85 training accuracy: 0.7642318244170097
At round 85 training loss: 0.6664237833664266
gradient difference: 15.298878890043797
At round 86 accuracy: 0.7349397590361446
At round 86 training accuracy: 0.7664609053497943
At round 86 training loss: 0.6614985981322468
gradient difference: 15.093817571214405
At round 87 accuracy: 0.7454819277108434
At round 87 training accuracy: 0.7731481481481481
At round 87 training loss: 0.6602021317682465
gradient difference: 14.894633042986737
At round 88 accuracy: 0.75
At round 88 training accuracy: 0.7791495198902606
At round 88 training loss: 0.6569751053898428
gradient difference: 14.666163188563512
At round 89 accuracy: 0.75
At round 89 training accuracy: 0.7784636488340192
At round 89 training loss: 0.6536946655500205
gradient difference: 14.464031203175773
At round 90 accuracy: 0.7560240963855421
At round 90 training accuracy: 0.7806927297668038
At round 90 training loss: 0.6497256514608006
gradient difference: 14.167701964713846
At round 91 accuracy: 0.7590361445783133
At round 91 training accuracy: 0.7801783264746228
At round 91 training loss: 0.6441629824678277
gradient difference: 13.804133862318198
At round 92 accuracy: 0.7590361445783133
At round 92 training accuracy: 0.7800068587105624
At round 92 training loss: 0.6401530546679153
gradient difference: 13.723640312667586
At round 93 accuracy: 0.7560240963855421
At round 93 training accuracy: 0.7796639231824417
At round 93 training loss: 0.637218506076879
gradient difference: 13.568411980985633
At round 94 accuracy: 0.7605421686746988
At round 94 training accuracy: 0.7803497942386831
At round 94 training loss: 0.6345596094371428
gradient difference: 13.230400238918095
At round 95 accuracy: 0.7605421686746988
At round 95 training accuracy: 0.7830932784636488
At round 95 training loss: 0.6291160667564245
gradient difference: 12.87140477153941
At round 96 accuracy: 0.7590361445783133
At round 96 training accuracy: 0.7868655692729767
At round 96 training loss: 0.6238647415047187
gradient difference: 12.453401494915912
At round 97 accuracy: 0.7605421686746988
At round 97 training accuracy: 0.7901234567901234
At round 97 training loss: 0.6216989151273684
gradient difference: 12.480736344530111
At round 98 accuracy: 0.7620481927710844
At round 98 training accuracy: 0.7932098765432098
At round 98 training loss: 0.6187185562284093
gradient difference: 12.19805339473748
At round 99 accuracy: 0.7635542168674698
At round 99 training accuracy: 0.7923525377229081
At round 99 training loss: 0.6156483754777223
gradient difference: 12.105315489427374
At round 100 accuracy: 0.7605421686746988
At round 100 training accuracy: 0.7928669410150891
At round 100 training loss: 0.6137321962170438
gradient difference: 11.937360697069336
At round 101 accuracy: 0.766566265060241
At round 101 training accuracy: 0.7932098765432098
At round 101 training loss: 0.6095075641756504
gradient difference: 11.629920145704165
At round 102 accuracy: 0.7695783132530121
At round 102 training accuracy: 0.7930384087791496
At round 102 training loss: 0.6078147151502966
gradient difference: 11.562574733281783
At round 103 accuracy: 0.7695783132530121
At round 103 training accuracy: 0.796639231824417
At round 103 training loss: 0.60382646156634
gradient difference: 11.25061603367252
At round 104 accuracy: 0.7710843373493976
At round 104 training accuracy: 0.7980109739368999
At round 104 training loss: 0.6027149309117388
gradient difference: 11.294325687356483
At round 105 accuracy: 0.7756024096385542
At round 105 training accuracy: 0.8012688614540466
At round 105 training loss: 0.6016216003372742
gradient difference: 11.285959584671748
At round 106 accuracy: 0.7740963855421686
At round 106 training accuracy: 0.7981824417009602
At round 106 training loss: 0.6006650185900445
gradient difference: 11.337237466186126
At round 107 accuracy: 0.7740963855421686
At round 107 training accuracy: 0.7974965706447188
At round 107 training loss: 0.5970947173154505
gradient difference: 11.213350691137885
At round 108 accuracy: 0.7710843373493976
At round 108 training accuracy: 0.7995541838134431
At round 108 training loss: 0.5942123901627228
gradient difference: 10.98662726665928
At round 109 accuracy: 0.7725903614457831
At round 109 training accuracy: 0.8016117969821673
At round 109 training loss: 0.5900349841990977
gradient difference: 10.729157956404132
At round 110 accuracy: 0.7756024096385542
At round 110 training accuracy: 0.8010973936899863
At round 110 training loss: 0.5860898613119153
gradient difference: 10.557102888659747
At round 111 accuracy: 0.7831325301204819
At round 111 training accuracy: 0.8036694101508917
At round 111 training loss: 0.5854180850922507
gradient difference: 10.617636771975713
At round 112 accuracy: 0.7771084337349398
At round 112 training accuracy: 0.8043552812071331
At round 112 training loss: 0.5816470405702646
gradient difference: 10.376281107236746
At round 113 accuracy: 0.7816265060240963
At round 113 training accuracy: 0.8076131687242798
At round 113 training loss: 0.5813739452420367
gradient difference: 10.417494502714337
At round 114 accuracy: 0.7831325301204819
At round 114 training accuracy: 0.806241426611797
At round 114 training loss: 0.580962239509124
gradient difference: 10.443704966967823
At round 115 accuracy: 0.7876506024096386
At round 115 training accuracy: 0.8093278463648834
At round 115 training loss: 0.5767634519312304
gradient difference: 10.02517361652691
At round 116 accuracy: 0.7876506024096386
At round 116 training accuracy: 0.8098422496570644
At round 116 training loss: 0.574952408113895
gradient difference: 9.892342937939564
At round 117 accuracy: 0.7876506024096386
At round 117 training accuracy: 0.8094993141289437
At round 117 training loss: 0.5731882350575191
gradient difference: 9.841356069945236
At round 118 accuracy: 0.7846385542168675
At round 118 training accuracy: 0.8096707818930041
At round 118 training loss: 0.5707407702183395
gradient difference: 9.811596159305063
At round 119 accuracy: 0.7801204819277109
At round 119 training accuracy: 0.8105281207133059
At round 119 training loss: 0.5677189213279328
gradient difference: 9.583612130572034
At round 120 accuracy: 0.7816265060240963
At round 120 training accuracy: 0.8096707818930041
At round 120 training loss: 0.567452720540163
gradient difference: 9.635224663139308
At round 121 accuracy: 0.7846385542168675
At round 121 training accuracy: 0.8106995884773662
At round 121 training loss: 0.564168072415807
gradient difference: 9.309859379309836
At round 122 accuracy: 0.7876506024096386
At round 122 training accuracy: 0.811556927297668
At round 122 training loss: 0.562811142016031
gradient difference: 9.21205029128009
At round 123 accuracy: 0.786144578313253
At round 123 training accuracy: 0.8100137174211248
At round 123 training loss: 0.562035216544742
gradient difference: 9.210663183259191
At round 124 accuracy: 0.786144578313253
At round 124 training accuracy: 0.8106995884773662
At round 124 training loss: 0.5562493550582867
gradient difference: 8.627571194825325
At round 125 accuracy: 0.7831325301204819
At round 125 training accuracy: 0.8118998628257887
At round 125 training loss: 0.556916921322722
gradient difference: 8.689095736193213
At round 126 accuracy: 0.786144578313253
At round 126 training accuracy: 0.8137860082304527
At round 126 training loss: 0.5544618585109277
gradient difference: 8.490802737976077
At round 127 accuracy: 0.7846385542168675
At round 127 training accuracy: 0.8132716049382716
At round 127 training loss: 0.551374451241958
gradient difference: 8.275986087897214
At round 128 accuracy: 0.7876506024096386
At round 128 training accuracy: 0.8139574759945131
At round 128 training loss: 0.5493497011847078
gradient difference: 8.072217099640456
At round 129 accuracy: 0.7906626506024096
At round 129 training accuracy: 0.8137860082304527
At round 129 training loss: 0.5477129296474389
gradient difference: 7.993243210086976
At round 130 accuracy: 0.7921686746987951
At round 130 training accuracy: 0.8155006858710563
At round 130 training loss: 0.5438364193540333
gradient difference: 7.533396151738044
At round 131 accuracy: 0.7966867469879518
At round 131 training accuracy: 0.8185871056241426
At round 131 training loss: 0.542378025213116
gradient difference: 7.478559036221487
At round 132 accuracy: 0.7936746987951807
At round 132 training accuracy: 0.8182441700960219
At round 132 training loss: 0.5403778037629301
gradient difference: 7.389899170461254
At round 133 accuracy: 0.7936746987951807
At round 133 training accuracy: 0.8173868312757202
At round 133 training loss: 0.5405232214622151
gradient difference: 7.425446074017447
At round 134 accuracy: 0.7921686746987951
At round 134 training accuracy: 0.8141289437585734
At round 134 training loss: 0.5408942743762817
gradient difference: 7.463115655231913
At round 135 accuracy: 0.7921686746987951
At round 135 training accuracy: 0.8124142661179699
At round 135 training loss: 0.5399837621784065
gradient difference: 7.486973991429267
At round 136 accuracy: 0.7906626506024096
At round 136 training accuracy: 0.8108710562414266
At round 136 training loss: 0.5377496461817083
gradient difference: 7.366439565849593
At round 137 accuracy: 0.7921686746987951
At round 137 training accuracy: 0.8184156378600823
At round 137 training loss: 0.535119480102429
gradient difference: 7.341086212896661
At round 138 accuracy: 0.7936746987951807
At round 138 training accuracy: 0.8192729766803841
At round 138 training loss: 0.5343189600536368
gradient difference: 7.4003760325175545
At round 139 accuracy: 0.7951807228915663
At round 139 training accuracy: 0.8201303155006858
At round 139 training loss: 0.5328370745450933
gradient difference: 7.2670286438179135
At round 140 accuracy: 0.7906626506024096
At round 140 training accuracy: 0.8175582990397805
At round 140 training loss: 0.5328275343043595
gradient difference: 7.3218045922744635
At round 141 accuracy: 0.7906626506024096
At round 141 training accuracy: 0.8194444444444444
At round 141 training loss: 0.5290952701292758
gradient difference: 6.917937847120917
At round 142 accuracy: 0.7891566265060241
At round 142 training accuracy: 0.8203017832647462
At round 142 training loss: 0.5286378656179914
gradient difference: 6.886458149063431
At round 143 accuracy: 0.7891566265060241
At round 143 training accuracy: 0.8192729766803841
At round 143 training loss: 0.527433998038001
gradient difference: 6.803861521967692
At round 144 accuracy: 0.7891566265060241
At round 144 training accuracy: 0.8191015089163237
At round 144 training loss: 0.5267628519987465
gradient difference: 6.750187004757006
At round 145 accuracy: 0.7906626506024096
At round 145 training accuracy: 0.8192729766803841
At round 145 training loss: 0.5256547160297811
gradient difference: 6.684989046142338
At round 146 accuracy: 0.7891566265060241
At round 146 training accuracy: 0.8196159122085048
At round 146 training loss: 0.524256158416033
gradient difference: 6.543863678435317
At round 147 accuracy: 0.7921686746987951
At round 147 training accuracy: 0.8197873799725651
At round 147 training loss: 0.522850887026556
gradient difference: 6.586701073655562
At round 148 accuracy: 0.7936746987951807
At round 148 training accuracy: 0.8199588477366255
At round 148 training loss: 0.5201129547798743
gradient difference: 6.282534845519079
At round 149 accuracy: 0.7921686746987951
At round 149 training accuracy: 0.8206447187928669
At round 149 training loss: 0.519095751325011
gradient difference: 6.236259948279243
At round 150 accuracy: 0.7981927710843374
At round 150 training accuracy: 0.821159122085048
At round 150 training loss: 0.5183958612281486
gradient difference: 6.2834541726091
At round 151 accuracy: 0.7966867469879518
At round 151 training accuracy: 0.8203017832647462
At round 151 training loss: 0.5183009820616905
gradient difference: 6.335415748082295
At round 152 accuracy: 0.7966867469879518
At round 152 training accuracy: 0.8206447187928669
At round 152 training loss: 0.5156140502502973
gradient difference: 6.193574446624413
At round 153 accuracy: 0.7921686746987951
At round 153 training accuracy: 0.8218449931412894
At round 153 training loss: 0.5160117721725166
gradient difference: 6.27597531487721
At round 154 accuracy: 0.7936746987951807
At round 154 training accuracy: 0.8218449931412894
At round 154 training loss: 0.5151113348181675
gradient difference: 6.211221355527047
At round 155 accuracy: 0.8012048192771084
At round 155 training accuracy: 0.8242455418381345
At round 155 training loss: 0.5100088334255577
gradient difference: 5.706777684495132
At round 156 accuracy: 0.7996987951807228
At round 156 training accuracy: 0.8237311385459534
At round 156 training loss: 0.5096667910860889
gradient difference: 5.704370808439139
At round 157 accuracy: 0.8012048192771084
At round 157 training accuracy: 0.8232167352537723
At round 157 training loss: 0.5090119984000602
gradient difference: 5.657876370055051
At round 158 accuracy: 0.7996987951807228
At round 158 training accuracy: 0.8244170096021948
At round 158 training loss: 0.5070499251010561
gradient difference: 5.405377299357358
At round 159 accuracy: 0.7996987951807228
At round 159 training accuracy: 0.8221879286694102
At round 159 training loss: 0.5068211947795801
gradient difference: 5.295164437074969
At round 160 accuracy: 0.7996987951807228
At round 160 training accuracy: 0.8244170096021948
At round 160 training loss: 0.5047591270303062
gradient difference: 5.257135771249699
At round 161 accuracy: 0.8012048192771084
At round 161 training accuracy: 0.823045267489712
At round 161 training loss: 0.5043642784958126
gradient difference: 5.306172587931525
At round 162 accuracy: 0.802710843373494
At round 162 training accuracy: 0.828360768175583
At round 162 training loss: 0.502827587616994
gradient difference: 5.321588664845673
At round 163 accuracy: 0.8042168674698795
At round 163 training accuracy: 0.8278463648834019
At round 163 training loss: 0.5021278944851151
gradient difference: 5.271176735491675
At round 164 accuracy: 0.8087349397590361
At round 164 training accuracy: 0.8275034293552812
At round 164 training loss: 0.4993251761268577
gradient difference: 4.9873434092893865
At round 165 accuracy: 0.8087349397590361
At round 165 training accuracy: 0.8266460905349794
At round 165 training loss: 0.4991884116622822
gradient difference: 5.013014639570326
At round 166 accuracy: 0.8087349397590361
At round 166 training accuracy: 0.8266460905349794
At round 166 training loss: 0.4983696589865677
gradient difference: 4.915934902556594
At round 167 accuracy: 0.8012048192771084
At round 167 training accuracy: 0.8247599451303155
At round 167 training loss: 0.49923271410066933
gradient difference: 4.900447900963305
At round 168 accuracy: 0.7996987951807228
At round 168 training accuracy: 0.8257887517146777
At round 168 training loss: 0.49805902995904383
gradient difference: 4.847775461261988
At round 169 accuracy: 0.8012048192771084
At round 169 training accuracy: 0.8269890260631001
At round 169 training loss: 0.49626273114747776
gradient difference: 4.8485995213582225
At round 170 accuracy: 0.8012048192771084
At round 170 training accuracy: 0.8256172839506173
At round 170 training loss: 0.49623692319239776
gradient difference: 4.887835365638441
At round 171 accuracy: 0.8012048192771084
At round 171 training accuracy: 0.8247599451303155
At round 171 training loss: 0.49612467538962074
gradient difference: 4.861238022362693
At round 172 accuracy: 0.7996987951807228
At round 172 training accuracy: 0.8221879286694102
At round 172 training loss: 0.4964208781742523
gradient difference: 4.857947872491994
At round 173 accuracy: 0.8012048192771084
At round 173 training accuracy: 0.8213305898491083
At round 173 training loss: 0.49650565601006375
gradient difference: 4.906082459111908
At round 174 accuracy: 0.802710843373494
At round 174 training accuracy: 0.8215020576131687
At round 174 training loss: 0.49534310383111524
gradient difference: 4.818727736036322
At round 175 accuracy: 0.802710843373494
At round 175 training accuracy: 0.8197873799725651
At round 175 training loss: 0.4946716366393558
gradient difference: 4.786964894137484
At round 176 accuracy: 0.8012048192771084
At round 176 training accuracy: 0.8192729766803841
At round 176 training loss: 0.49505344712809723
gradient difference: 4.7838064636541615
At round 177 accuracy: 0.8072289156626506
At round 177 training accuracy: 0.8242455418381345
At round 177 training loss: 0.49133290850703665
gradient difference: 4.792180224642319
At round 178 accuracy: 0.8072289156626506
At round 178 training accuracy: 0.8245884773662552
At round 178 training loss: 0.49141212612526575
gradient difference: 4.787526256344918
At round 179 accuracy: 0.8072289156626506
At round 179 training accuracy: 0.8237311385459534
At round 179 training loss: 0.49000734235246285
gradient difference: 4.749816555214252
At round 180 accuracy: 0.8147590361445783
At round 180 training accuracy: 0.8335048010973937
At round 180 training loss: 0.4886823037780747
gradient difference: 4.725337375459996
At round 181 accuracy: 0.8162650602409639
At round 181 training accuracy: 0.8350480109739369
At round 181 training loss: 0.4889720370742302
gradient difference: 4.773571596445594
At round 182 accuracy: 0.8132530120481928
At round 182 training accuracy: 0.8369341563786008
At round 182 training loss: 0.48901817182909996
gradient difference: 4.8058140196177455
At round 183 accuracy: 0.8147590361445783
At round 183 training accuracy: 0.8376200274348422
At round 183 training loss: 0.48795683508906873
gradient difference: 4.731120602713678
At round 184 accuracy: 0.8177710843373494
At round 184 training accuracy: 0.8359053497942387
At round 184 training loss: 0.4878243510855684
gradient difference: 4.755223219271087
At round 185 accuracy: 0.8177710843373494
At round 185 training accuracy: 0.8376200274348422
At round 185 training loss: 0.4868365757613518
gradient difference: 4.675381110242768
At round 186 accuracy: 0.8177710843373494
At round 186 training accuracy: 0.8372770919067215
At round 186 training loss: 0.4862355125725617
gradient difference: 4.648735641211984
At round 187 accuracy: 0.8132530120481928
At round 187 training accuracy: 0.8372770919067215
At round 187 training loss: 0.4853959678357981
gradient difference: 4.5982910940841375
At round 188 accuracy: 0.8177710843373494
At round 188 training accuracy: 0.8365912208504801
At round 188 training loss: 0.4845533141179971
gradient difference: 4.54242094216298
At round 189 accuracy: 0.8147590361445783
At round 189 training accuracy: 0.8350480109739369
At round 189 training loss: 0.4838279021353894
gradient difference: 4.531600838763066
At round 190 accuracy: 0.8147590361445783
At round 190 training accuracy: 0.836076817558299
At round 190 training loss: 0.48265820767124284
gradient difference: 4.449511748721297
At round 191 accuracy: 0.8147590361445783
At round 191 training accuracy: 0.8353909465020576
At round 191 training loss: 0.48183882999807515
gradient difference: 4.405333733819656
At round 192 accuracy: 0.8162650602409639
At round 192 training accuracy: 0.8338477366255144
At round 192 training loss: 0.4812394469000389
gradient difference: 4.35009198709623
At round 193 accuracy: 0.8162650602409639
At round 193 training accuracy: 0.8328189300411523
At round 193 training loss: 0.4805123574847473
gradient difference: 4.410730349758818
At round 194 accuracy: 0.8207831325301205
At round 194 training accuracy: 0.8376200274348422
At round 194 training loss: 0.48073211404020977
gradient difference: 4.437822402569972
At round 195 accuracy: 0.8207831325301205
At round 195 training accuracy: 0.8376200274348422
At round 195 training loss: 0.4800742145173254
gradient difference: 4.386222486976081
At round 196 accuracy: 0.8207831325301205
At round 196 training accuracy: 0.8386488340192044
At round 196 training loss: 0.478301328190587
gradient difference: 4.197967109217873
At round 197 accuracy: 0.8192771084337349
At round 197 training accuracy: 0.8386488340192044
At round 197 training loss: 0.47784209343178685
gradient difference: 4.153292240604523
At round 198 accuracy: 0.8162650602409639
At round 198 training accuracy: 0.8371056241426612
At round 198 training loss: 0.47708684451040007
gradient difference: 4.122328567111079
At round 199 accuracy: 0.8162650602409639
At round 199 training accuracy: 0.8376200274348422
At round 199 training loss: 0.4759781547334679
gradient difference: 4.008970021487662
At round 200 accuracy: 0.8162650602409639
At round 200 training accuracy: 0.8377914951989026
