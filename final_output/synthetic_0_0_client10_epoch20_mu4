Arguments:
	       batch_size : 10
	clients_per_round : 10
	          dataset : synthetic_0_0
	     drop_percent : 0.0
	       eval_every : 1
	    learning_rate : 0.01
	            model : mclr
	     model_params : (10,)
	               mu : 4.0
	       num_epochs : 20
	        num_iters : 1
	       num_rounds : 200
	        optimizer : fedprox
	             seed : 0
Using Federated prox to Train

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/3.64k flops)
  dense/kernel/Initializer/stateless_random_uniform (600/1.20k flops)
    dense/kernel/Initializer/stateless_random_uniform/mul (600/600 flops)
    dense/kernel/Initializer/stateless_random_uniform/sub (1/1 flops)
  PGD/update_dense/kernel/add (600/600 flops)
  PGD/update_dense/kernel/mul (600/600 flops)
  PGD/update_dense/kernel/mul_1 (600/600 flops)
  PGD/update_dense/kernel/sub (600/600 flops)
  PGD/update_dense/bias/add (10/10 flops)
  PGD/update_dense/bias/mul (10/10 flops)
  PGD/update_dense/bias/mul_1 (10/10 flops)
  PGD/update_dense/bias/sub (10/10 flops)
  gradients/sparse_softmax_cross_entropy_loss/value_grad/Neg (1/1 flops)
  gradients/sparse_softmax_cross_entropy_loss/value_grad/mul (1/1 flops)
  sparse_softmax_cross_entropy_loss/num_present/Equal (1/1 flops)

======================End of Report==========================
30 Clients in Total
Training with 10 workers ---
At round 0 accuracy: 0.09186746987951808
At round 0 training accuracy: 0.0941358024691358
At round 0 training loss: 3.3660748895201498
gradient difference: 49.98099832688448
At round 1 accuracy: 0.11144578313253012
At round 1 training accuracy: 0.1123113854595336
At round 1 training loss: 2.856526277744721
gradient difference: 48.53542927506306
At round 2 accuracy: 0.25903614457831325
At round 2 training accuracy: 0.26406035665294925
At round 2 training loss: 2.5389013367466147
gradient difference: 46.290599785801156
At round 3 accuracy: 0.27259036144578314
At round 3 training accuracy: 0.2717764060356653
At round 3 training loss: 2.2088843211327234
gradient difference: 44.69846169564395
At round 4 accuracy: 0.2605421686746988
At round 4 training accuracy: 0.2657750342935528
At round 4 training loss: 2.1469201402449967
gradient difference: 44.499859323419926
At round 5 accuracy: 0.3509036144578313
At round 5 training accuracy: 0.35768175582990397
At round 5 training loss: 2.007848514123872
gradient difference: 42.98042820973618
At round 6 accuracy: 0.3704819277108434
At round 6 training accuracy: 0.37911522633744854
At round 6 training loss: 1.9108189652854033
gradient difference: 42.18248256286136
At round 7 accuracy: 0.42620481927710846
At round 7 training accuracy: 0.431241426611797
At round 7 training loss: 1.8424608825472157
gradient difference: 41.39277849280618
At round 8 accuracy: 0.45331325301204817
At round 8 training accuracy: 0.4518175582990398
At round 8 training loss: 1.7646369130461794
gradient difference: 40.63579980186362
At round 9 accuracy: 0.4879518072289157
At round 9 training accuracy: 0.47856652949245543
At round 9 training loss: 1.708489492453829
gradient difference: 40.334223316054214
At round 10 accuracy: 0.5286144578313253
At round 10 training accuracy: 0.5337791495198903
At round 10 training loss: 1.6586332856913817
gradient difference: 39.76641348969562
At round 11 accuracy: 0.5421686746987951
At round 11 training accuracy: 0.5510973936899863
At round 11 training loss: 1.6171648117147683
gradient difference: 38.66644036911024
At round 12 accuracy: 0.5496987951807228
At round 12 training accuracy: 0.5644718792866941
At round 12 training loss: 1.5638558046460478
gradient difference: 37.64765931824601
At round 13 accuracy: 0.5481927710843374
At round 13 training accuracy: 0.5629286694101509
At round 13 training loss: 1.5306119156288511
gradient difference: 37.46853461935322
At round 14 accuracy: 0.5617469879518072
At round 14 training accuracy: 0.5720164609053497
At round 14 training loss: 1.4998698006748172
gradient difference: 36.819165244777686
At round 15 accuracy: 0.5632530120481928
At round 15 training accuracy: 0.5694444444444444
At round 15 training loss: 1.4851568703052929
gradient difference: 36.560285857421306
At round 16 accuracy: 0.5587349397590361
At round 16 training accuracy: 0.5639574759945131
At round 16 training loss: 1.4727666094306402
gradient difference: 36.339920722841235
At round 17 accuracy: 0.5557228915662651
At round 17 training accuracy: 0.5656721536351166
At round 17 training loss: 1.4375075341313464
gradient difference: 35.4875180118901
At round 18 accuracy: 0.572289156626506
At round 18 training accuracy: 0.583161865569273
At round 18 training loss: 1.3895129581098284
gradient difference: 34.948692750135805
At round 19 accuracy: 0.5768072289156626
At round 19 training accuracy: 0.5869341563786008
At round 19 training loss: 1.3474429201319384
gradient difference: 34.45352419192308
At round 20 accuracy: 0.5798192771084337
At round 20 training accuracy: 0.5883058984910837
At round 20 training loss: 1.322765035378467
gradient difference: 34.30661198262139
At round 21 accuracy: 0.5768072289156626
At round 21 training accuracy: 0.5886488340192044
At round 21 training loss: 1.2868131807148049
gradient difference: 33.978001639123086
At round 22 accuracy: 0.5858433734939759
At round 22 training accuracy: 0.5949931412894376
At round 22 training loss: 1.2704356448783634
gradient difference: 32.57202869612786
At round 23 accuracy: 0.588855421686747
At round 23 training accuracy: 0.598079561042524
At round 23 training loss: 1.254045995023863
gradient difference: 32.25519979072211
At round 24 accuracy: 0.588855421686747
At round 24 training accuracy: 0.5997942386831275
At round 24 training loss: 1.2219685048007998
gradient difference: 31.740406606494435
At round 25 accuracy: 0.588855421686747
At round 25 training accuracy: 0.60099451303155
At round 25 training loss: 1.181926561499063
gradient difference: 30.866220417602857
At round 26 accuracy: 0.5903614457831325
At round 26 training accuracy: 0.6025377229080933
At round 26 training loss: 1.1637069401708118
gradient difference: 30.642490787744716
At round 27 accuracy: 0.5903614457831325
At round 27 training accuracy: 0.6033950617283951
At round 27 training loss: 1.1510958547482908
gradient difference: 30.400304983867294
At round 28 accuracy: 0.608433734939759
At round 28 training accuracy: 0.622599451303155
At round 28 training loss: 1.130302888838209
gradient difference: 29.557081754237984
At round 29 accuracy: 0.608433734939759
At round 29 training accuracy: 0.6244855967078189
At round 29 training loss: 1.1113613190407903
gradient difference: 29.428474877368572
At round 30 accuracy: 0.6144578313253012
At round 30 training accuracy: 0.6282578875171467
At round 30 training loss: 1.1006628764353805
gradient difference: 29.30557929979443
At round 31 accuracy: 0.6129518072289156
At round 31 training accuracy: 0.6315157750342936
At round 31 training loss: 1.0897476108813728
gradient difference: 29.215298353241746
At round 32 accuracy: 0.6265060240963856
At round 32 training accuracy: 0.6400891632373114
At round 32 training loss: 1.0736205038373117
gradient difference: 28.937366978493163
At round 33 accuracy: 0.6265060240963856
At round 33 training accuracy: 0.6430041152263375
At round 33 training loss: 1.065106185333259
gradient difference: 28.944888972051885
At round 34 accuracy: 0.6295180722891566
At round 34 training accuracy: 0.6478052126200274
At round 34 training loss: 1.0526837284660122
gradient difference: 28.77405433763901
At round 35 accuracy: 0.6355421686746988
At round 35 training accuracy: 0.6531207133058985
At round 35 training loss: 1.0281073211312846
gradient difference: 28.08043774913859
At round 36 accuracy: 0.6355421686746988
At round 36 training accuracy: 0.6555212620027435
At round 36 training loss: 1.0149523027979646
gradient difference: 27.772243875145808
At round 37 accuracy: 0.6445783132530121
At round 37 training accuracy: 0.6632373113854595
At round 37 training loss: 1.0014550975410188
gradient difference: 27.310270552461308
At round 38 accuracy: 0.6445783132530121
At round 38 training accuracy: 0.668724279835391
At round 38 training loss: 0.9920196892203066
gradient difference: 27.099529217960693
At round 39 accuracy: 0.6460843373493976
At round 39 training accuracy: 0.6688957475994513
At round 39 training loss: 0.985652359271491
gradient difference: 27.044387585033952
At round 40 accuracy: 0.6506024096385542
At round 40 training accuracy: 0.6685528120713305
At round 40 training loss: 0.9813602407142233
gradient difference: 27.02672762994857
At round 41 accuracy: 0.6581325301204819
At round 41 training accuracy: 0.6738683127572016
At round 41 training loss: 0.9615305622474933
gradient difference: 26.298947497827097
At round 42 accuracy: 0.661144578313253
At round 42 training accuracy: 0.6748971193415638
At round 42 training loss: 0.9553628563024973
gradient difference: 26.08910414027656
At round 43 accuracy: 0.6596385542168675
At round 43 training accuracy: 0.6766117969821673
At round 43 training loss: 0.949227211173542
gradient difference: 25.96223879272937
At round 44 accuracy: 0.6716867469879518
At round 44 training accuracy: 0.686556927297668
At round 44 training loss: 0.9364463903938659
gradient difference: 25.521045564836317
At round 45 accuracy: 0.6762048192771084
At round 45 training accuracy: 0.6879286694101509
At round 45 training loss: 0.929427381069623
gradient difference: 25.369937353059566
At round 46 accuracy: 0.677710843373494
At round 46 training accuracy: 0.6915294924554184
At round 46 training loss: 0.9210569067725296
gradient difference: 25.00944964441571
At round 47 accuracy: 0.6807228915662651
At round 47 training accuracy: 0.6925582990397805
At round 47 training loss: 0.9157736994420652
gradient difference: 24.886452339716485
At round 48 accuracy: 0.677710843373494
At round 48 training accuracy: 0.6905006858710563
At round 48 training loss: 0.9119598302799845
gradient difference: 24.833084963190153
At round 49 accuracy: 0.6716867469879518
At round 49 training accuracy: 0.6889574759945131
At round 49 training loss: 0.9091925005356951
gradient difference: 24.836575373530856
At round 50 accuracy: 0.6746987951807228
At round 50 training accuracy: 0.6922153635116598
At round 50 training loss: 0.9030435133402188
gradient difference: 24.471577754882432
At round 51 accuracy: 0.6792168674698795
At round 51 training accuracy: 0.6971879286694102
At round 51 training loss: 0.8966651215851664
gradient difference: 24.288135461882195
At round 52 accuracy: 0.6852409638554217
At round 52 training accuracy: 0.6970164609053497
At round 52 training loss: 0.8936349176150199
gradient difference: 24.23841990850171
At round 53 accuracy: 0.677710843373494
At round 53 training accuracy: 0.6989026063100137
At round 53 training loss: 0.8815845249959143
gradient difference: 23.7468834130783
At round 54 accuracy: 0.677710843373494
At round 54 training accuracy: 0.7001028806584362
At round 54 training loss: 0.8767674280693358
gradient difference: 23.649145021502466
At round 55 accuracy: 0.6807228915662651
At round 55 training accuracy: 0.7021604938271605
At round 55 training loss: 0.8730258084728225
gradient difference: 23.51371764894392
At round 56 accuracy: 0.677710843373494
At round 56 training accuracy: 0.7025034293552812
At round 56 training loss: 0.8698975012806309
gradient difference: 23.475991691678747
At round 57 accuracy: 0.6852409638554217
At round 57 training accuracy: 0.7031893004115226
At round 57 training loss: 0.8642331313377627
gradient difference: 23.455545189841928
At round 58 accuracy: 0.6852409638554217
At round 58 training accuracy: 0.7040466392318244
At round 58 training loss: 0.8602133448636836
gradient difference: 23.504317590297717
At round 59 accuracy: 0.6822289156626506
At round 59 training accuracy: 0.7042181069958847
At round 59 training loss: 0.8572690352572023
gradient difference: 23.44753299241713
At round 60 accuracy: 0.6837349397590361
At round 60 training accuracy: 0.7043895747599451
At round 60 training loss: 0.8543509565512886
gradient difference: 23.43134997290223
At round 61 accuracy: 0.6867469879518072
At round 61 training accuracy: 0.7074759945130316
At round 61 training loss: 0.8496669580861645
gradient difference: 23.264957075541883
At round 62 accuracy: 0.6882530120481928
At round 62 training accuracy: 0.7117626886145405
At round 62 training loss: 0.8418107549008366
gradient difference: 22.89594401699569
At round 63 accuracy: 0.6927710843373494
At round 63 training accuracy: 0.7136488340192044
At round 63 training loss: 0.8237907396468483
gradient difference: 21.767457669744015
At round 64 accuracy: 0.6912650602409639
At round 64 training accuracy: 0.7138203017832647
At round 64 training loss: 0.8170588550455511
gradient difference: 21.549296645787372
At round 65 accuracy: 0.6897590361445783
At round 65 training accuracy: 0.7146776406035665
At round 65 training loss: 0.8144213930430307
gradient difference: 21.51543346068029
At round 66 accuracy: 0.6912650602409639
At round 66 training accuracy: 0.7153635116598079
At round 66 training loss: 0.8096211986106517
gradient difference: 21.27178034729548
At round 67 accuracy: 0.6897590361445783
At round 67 training accuracy: 0.7146776406035665
At round 67 training loss: 0.807267842229535
gradient difference: 21.31757791862351
At round 68 accuracy: 0.6912650602409639
At round 68 training accuracy: 0.7151920438957476
At round 68 training loss: 0.8030031045661813
gradient difference: 21.038853065926034
At round 69 accuracy: 0.6927710843373494
At round 69 training accuracy: 0.715877914951989
At round 69 training loss: 0.798154290191911
gradient difference: 20.72826664931646
At round 70 accuracy: 0.6912650602409639
At round 70 training accuracy: 0.718278463648834
At round 70 training loss: 0.7941947815932119
gradient difference: 20.54258865190586
At round 71 accuracy: 0.6957831325301205
At round 71 training accuracy: 0.7220507544581619
At round 71 training loss: 0.7886406058153017
gradient difference: 20.123535347625843
At round 72 accuracy: 0.6957831325301205
At round 72 training accuracy: 0.7215363511659808
At round 72 training loss: 0.7841050528074182
gradient difference: 19.858749678707998
At round 73 accuracy: 0.697289156626506
At round 73 training accuracy: 0.725480109739369
At round 73 training loss: 0.777988612010822
gradient difference: 19.45214317604624
At round 74 accuracy: 0.7018072289156626
At round 74 training accuracy: 0.7275377229080933
At round 74 training loss: 0.7754218840156523
gradient difference: 19.306447688551312
At round 75 accuracy: 0.6957831325301205
At round 75 training accuracy: 0.7268518518518519
At round 75 training loss: 0.7724913652307278
gradient difference: 19.321098350161193
At round 76 accuracy: 0.7018072289156626
At round 76 training accuracy: 0.7270233196159122
At round 76 training loss: 0.7641219968012622
gradient difference: 19.01046526510274
At round 77 accuracy: 0.7093373493975904
At round 77 training accuracy: 0.7321673525377229
At round 77 training loss: 0.7566712635217314
gradient difference: 18.558943673889086
At round 78 accuracy: 0.7108433734939759
At round 78 training accuracy: 0.7349108367626886
At round 78 training loss: 0.7506614628858578
gradient difference: 18.402640831419344
At round 79 accuracy: 0.7093373493975904
At round 79 training accuracy: 0.7359396433470508
At round 79 training loss: 0.7476659797781002
gradient difference: 18.34875816616785
At round 80 accuracy: 0.713855421686747
At round 80 training accuracy: 0.7362825788751715
At round 80 training loss: 0.7431125485099371
gradient difference: 18.165424198372442
At round 81 accuracy: 0.713855421686747
At round 81 training accuracy: 0.7383401920438958
At round 81 training loss: 0.7395847732742893
gradient difference: 17.95488279124176
At round 82 accuracy: 0.7168674698795181
At round 82 training accuracy: 0.7391975308641975
At round 82 training loss: 0.7357483570960917
gradient difference: 17.665303352058682
At round 83 accuracy: 0.7153614457831325
At round 83 training accuracy: 0.7417695473251029
At round 83 training loss: 0.7332190823765531
gradient difference: 17.658499985104864
At round 84 accuracy: 0.7198795180722891
At round 84 training accuracy: 0.7424554183813443
At round 84 training loss: 0.7275000520472751
gradient difference: 17.329780060295093
At round 85 accuracy: 0.7274096385542169
At round 85 training accuracy: 0.754286694101509
At round 85 training loss: 0.719404545927226
gradient difference: 16.45711657819564
At round 86 accuracy: 0.7304216867469879
At round 86 training accuracy: 0.757201646090535
At round 86 training loss: 0.7139355863060608
gradient difference: 16.214685946056022
At round 87 accuracy: 0.7379518072289156
At round 87 training accuracy: 0.7616598079561042
At round 87 training loss: 0.7117388637941484
gradient difference: 15.990362239632256
At round 88 accuracy: 0.7454819277108434
At round 88 training accuracy: 0.76440329218107
At round 88 training loss: 0.7082574950430199
gradient difference: 15.763154819932078
At round 89 accuracy: 0.7424698795180723
At round 89 training accuracy: 0.7657750342935528
At round 89 training loss: 0.7038256442238192
gradient difference: 15.5391480555117
At round 90 accuracy: 0.7454819277108434
At round 90 training accuracy: 0.7669753086419753
At round 90 training loss: 0.7006297924607578
gradient difference: 15.297358165277636
At round 91 accuracy: 0.7469879518072289
At round 91 training accuracy: 0.766803840877915
At round 91 training loss: 0.69591548452882
gradient difference: 14.988054487857434
At round 92 accuracy: 0.7484939759036144
At round 92 training accuracy: 0.767318244170096
At round 92 training loss: 0.6917793469459447
gradient difference: 14.885347364281563
At round 93 accuracy: 0.7469879518072289
At round 93 training accuracy: 0.7676611796982168
At round 93 training loss: 0.6888876080369917
gradient difference: 14.749591164340469
At round 94 accuracy: 0.7515060240963856
At round 94 training accuracy: 0.7709190672153635
At round 94 training loss: 0.6852939761434114
gradient difference: 14.402003151900619
At round 95 accuracy: 0.7454819277108434
At round 95 training accuracy: 0.7698902606310014
At round 95 training loss: 0.6806246995956564
gradient difference: 14.112919077783733
At round 96 accuracy: 0.7545180722891566
At round 96 training accuracy: 0.7738340192043895
At round 96 training loss: 0.6762027136054636
gradient difference: 13.764122673873658
At round 97 accuracy: 0.7575301204819277
At round 97 training accuracy: 0.7757201646090535
At round 97 training loss: 0.6736283603406017
gradient difference: 13.756079770796534
At round 98 accuracy: 0.7590361445783133
At round 98 training accuracy: 0.7770919067215364
At round 98 training loss: 0.6706180685156361
gradient difference: 13.506392126523398
At round 99 accuracy: 0.7605421686746988
At round 99 training accuracy: 0.7782921810699589
At round 99 training loss: 0.6668988432018508
gradient difference: 13.387942898873664
At round 100 accuracy: 0.7575301204819277
At round 100 training accuracy: 0.779320987654321
At round 100 training loss: 0.665131841169476
gradient difference: 13.223421985304826
At round 101 accuracy: 0.7620481927710844
At round 101 training accuracy: 0.7818930041152263
At round 101 training loss: 0.6610667432775907
gradient difference: 12.941055088744582
At round 102 accuracy: 0.7620481927710844
At round 102 training accuracy: 0.78360768175583
At round 102 training loss: 0.6592530610748223
gradient difference: 12.855137612732412
At round 103 accuracy: 0.7635542168674698
At round 103 training accuracy: 0.7880658436213992
At round 103 training loss: 0.6555005597057324
gradient difference: 12.594469536766802
At round 104 accuracy: 0.7635542168674698
At round 104 training accuracy: 0.7875514403292181
At round 104 training loss: 0.6540255393578692
gradient difference: 12.59985727412842
At round 105 accuracy: 0.7635542168674698
At round 105 training accuracy: 0.7908093278463649
At round 105 training loss: 0.6522470810674182
gradient difference: 12.551871168867793
At round 106 accuracy: 0.7605421686746988
At round 106 training accuracy: 0.7887517146776406
At round 106 training loss: 0.6505664969690333
gradient difference: 12.536217620461713
At round 107 accuracy: 0.7575301204819277
At round 107 training accuracy: 0.7885802469135802
At round 107 training loss: 0.6472223680095929
gradient difference: 12.42867252177998
At round 108 accuracy: 0.7620481927710844
At round 108 training accuracy: 0.7923525377229081
At round 108 training loss: 0.6442883337467183
gradient difference: 12.21649766446592
At round 109 accuracy: 0.7680722891566265
At round 109 training accuracy: 0.7913237311385459
At round 109 training loss: 0.6403494528092352
gradient difference: 11.992015865193421
At round 110 accuracy: 0.766566265060241
At round 110 training accuracy: 0.7913237311385459
At round 110 training loss: 0.6362651077818396
gradient difference: 11.77623370750998
At round 111 accuracy: 0.766566265060241
At round 111 training accuracy: 0.7940672153635117
At round 111 training loss: 0.6349875789575805
gradient difference: 11.797773766094542
At round 112 accuracy: 0.7650602409638554
At round 112 training accuracy: 0.7940672153635117
At round 112 training loss: 0.6313389130613788
gradient difference: 11.577230472925883
At round 113 accuracy: 0.7680722891566265
At round 113 training accuracy: 0.7969821673525377
At round 113 training loss: 0.6303017395570879
gradient difference: 11.552921643300774
At round 114 accuracy: 0.7680722891566265
At round 114 training accuracy: 0.7968106995884774
At round 114 training loss: 0.6288886799576688
gradient difference: 11.529013269300991
At round 115 accuracy: 0.7680722891566265
At round 115 training accuracy: 0.7988683127572016
At round 115 training loss: 0.6252693454281456
gradient difference: 11.181415977671469
At round 116 accuracy: 0.7680722891566265
At round 116 training accuracy: 0.7978395061728395
At round 116 training loss: 0.623652040194439
gradient difference: 11.07793452337672
At round 117 accuracy: 0.7650602409638554
At round 117 training accuracy: 0.7974965706447188
At round 117 training loss: 0.6219317536709756
gradient difference: 11.025641980118092
At round 118 accuracy: 0.766566265060241
At round 118 training accuracy: 0.7969821673525377
At round 118 training loss: 0.6194320408153425
gradient difference: 10.994193491110229
At round 119 accuracy: 0.766566265060241
At round 119 training accuracy: 0.7981824417009602
At round 119 training loss: 0.6168209979251865
gradient difference: 10.804166426317462
At round 120 accuracy: 0.766566265060241
At round 120 training accuracy: 0.7980109739368999
At round 120 training loss: 0.6163058069787551
gradient difference: 10.835888106388714
At round 121 accuracy: 0.766566265060241
At round 121 training accuracy: 0.7969821673525377
At round 121 training loss: 0.6131114317934617
gradient difference: 10.52749011208668
At round 122 accuracy: 0.7680722891566265
At round 122 training accuracy: 0.7985253772290809
At round 122 training loss: 0.6117580417711007
gradient difference: 10.438074445535845
At round 123 accuracy: 0.7650602409638554
At round 123 training accuracy: 0.7993827160493827
At round 123 training loss: 0.6102978988488004
gradient difference: 10.400275281727026
At round 124 accuracy: 0.7725903614457831
At round 124 training accuracy: 0.8010973936899863
At round 124 training loss: 0.6052541872428983
gradient difference: 9.9073372627086
At round 125 accuracy: 0.7695783132530121
At round 125 training accuracy: 0.8002400548696845
At round 125 training loss: 0.6051893815067974
gradient difference: 9.930546795890056
At round 126 accuracy: 0.7710843373493976
At round 126 training accuracy: 0.8017832647462277
At round 126 training loss: 0.6028474920115655
gradient difference: 9.759371266725685
At round 127 accuracy: 0.7740963855421686
At round 127 training accuracy: 0.803840877914952
At round 127 training loss: 0.5997595558867921
gradient difference: 9.5479837916971
At round 128 accuracy: 0.7786144578313253
At round 128 training accuracy: 0.8045267489711934
At round 128 training loss: 0.5975694413065644
gradient difference: 9.365305487059427
At round 129 accuracy: 0.7831325301204819
At round 129 training accuracy: 0.8050411522633745
At round 129 training loss: 0.5955772876627079
gradient difference: 9.289500574581044
At round 130 accuracy: 0.7831325301204819
At round 130 training accuracy: 0.8070987654320988
At round 130 training loss: 0.5917891657863972
gradient difference: 8.858711605804652
At round 131 accuracy: 0.7846385542168675
At round 131 training accuracy: 0.8084705075445816
At round 131 training loss: 0.5900785276386503
gradient difference: 8.78988984620735
At round 132 accuracy: 0.7846385542168675
At round 132 training accuracy: 0.8084705075445816
At round 132 training loss: 0.5876619973905328
gradient difference: 8.654314226623804
At round 133 accuracy: 0.7816265060240963
At round 133 training accuracy: 0.8081275720164609
At round 133 training loss: 0.5876191567683232
gradient difference: 8.681190049483279
At round 134 accuracy: 0.7786144578313253
At round 134 training accuracy: 0.8058984910836763
At round 134 training loss: 0.5878083198433499
gradient difference: 8.688966643723985
At round 135 accuracy: 0.7786144578313253
At round 135 training accuracy: 0.8053840877914952
At round 135 training loss: 0.5866082782112317
gradient difference: 8.696686482312613
At round 136 accuracy: 0.7816265060240963
At round 136 training accuracy: 0.8050411522633745
At round 136 training loss: 0.5837201388988424
gradient difference: 8.551874571470352
At round 137 accuracy: 0.7846385542168675
At round 137 training accuracy: 0.8088134430727023
At round 137 training loss: 0.5812096453092312
gradient difference: 8.493489294571427
At round 138 accuracy: 0.7831325301204819
At round 138 training accuracy: 0.8100137174211248
At round 138 training loss: 0.5799572628182429
gradient difference: 8.518769529349937
At round 139 accuracy: 0.7831325301204819
At round 139 training accuracy: 0.8108710562414266
At round 139 training loss: 0.5784481360426298
gradient difference: 8.398293922464752
At round 140 accuracy: 0.7816265060240963
At round 140 training accuracy: 0.8094993141289437
At round 140 training loss: 0.5779874169188507
gradient difference: 8.417757520844635
At round 141 accuracy: 0.7846385542168675
At round 141 training accuracy: 0.8103566529492455
At round 141 training loss: 0.5745620894934395
gradient difference: 8.060529226312594
At round 142 accuracy: 0.7846385542168675
At round 142 training accuracy: 0.8108710562414266
At round 142 training loss: 0.5740709022797046
gradient difference: 8.01773298331352
At round 143 accuracy: 0.7831325301204819
At round 143 training accuracy: 0.8094993141289437
At round 143 training loss: 0.5726624763840924
gradient difference: 7.919994727210392
At round 144 accuracy: 0.7831325301204819
At round 144 training accuracy: 0.8108710562414266
At round 144 training loss: 0.5719228786019038
gradient difference: 7.863956360320555
At round 145 accuracy: 0.7831325301204819
At round 145 training accuracy: 0.8117283950617284
At round 145 training loss: 0.5705192167173294
gradient difference: 7.79868604156325
At round 146 accuracy: 0.7831325301204819
At round 146 training accuracy: 0.8127572016460906
At round 146 training loss: 0.5689817578812859
gradient difference: 7.646457457652559
At round 147 accuracy: 0.7846385542168675
At round 147 training accuracy: 0.8129286694101509
At round 147 training loss: 0.5674753915537686
gradient difference: 7.651108355660744
At round 148 accuracy: 0.7846385542168675
At round 148 training accuracy: 0.8125857338820301
At round 148 training loss: 0.5646818526873597
gradient difference: 7.343403016536163
At round 149 accuracy: 0.7846385542168675
At round 149 training accuracy: 0.8127572016460906
At round 149 training loss: 0.5638036454787583
gradient difference: 7.3071309946978165
At round 150 accuracy: 0.786144578313253
At round 150 training accuracy: 0.8127572016460906
At round 150 training loss: 0.5626726586391462
gradient difference: 7.33548906289072
At round 151 accuracy: 0.786144578313253
At round 151 training accuracy: 0.8118998628257887
At round 151 training loss: 0.5623151929336836
gradient difference: 7.367591402308272
At round 152 accuracy: 0.786144578313253
At round 152 training accuracy: 0.8144718792866941
At round 152 training loss: 0.559307084950607
gradient difference: 7.207802668715924
At round 153 accuracy: 0.7846385542168675
At round 153 training accuracy: 0.8160150891632373
At round 153 training loss: 0.5590196944573902
gradient difference: 7.250500247832933
At round 154 accuracy: 0.786144578313253
At round 154 training accuracy: 0.8170438957475995
At round 154 training loss: 0.5581150471602677
gradient difference: 7.196182693012101
At round 155 accuracy: 0.7906626506024096
At round 155 training accuracy: 0.8173868312757202
At round 155 training loss: 0.5536754018373516
gradient difference: 6.725981299924343
At round 156 accuracy: 0.7906626506024096
At round 156 training accuracy: 0.8168724279835391
At round 156 training loss: 0.5532486318385124
gradient difference: 6.72511847682379
At round 157 accuracy: 0.7906626506024096
At round 157 training accuracy: 0.8172153635116598
At round 157 training loss: 0.5523137929587432
gradient difference: 6.662570261194674
At round 158 accuracy: 0.7921686746987951
At round 158 training accuracy: 0.818758573388203
At round 158 training loss: 0.5502519428459174
gradient difference: 6.4212792402325585
At round 159 accuracy: 0.7906626506024096
At round 159 training accuracy: 0.8180727023319616
At round 159 training loss: 0.5495239525116132
gradient difference: 6.287477775826919
At round 160 accuracy: 0.7906626506024096
At round 160 training accuracy: 0.8185871056241426
At round 160 training loss: 0.5475044128637051
gradient difference: 6.237867223479733
At round 161 accuracy: 0.7906626506024096
At round 161 training accuracy: 0.8189300411522634
At round 161 training loss: 0.5467677209701961
gradient difference: 6.273435039402074
At round 162 accuracy: 0.7921686746987951
At round 162 training accuracy: 0.8218449931412894
At round 162 training loss: 0.5453996647328818
gradient difference: 6.277040934655621
At round 163 accuracy: 0.7921686746987951
At round 163 training accuracy: 0.8209876543209876
At round 163 training loss: 0.5447015495924854
gradient difference: 6.229629692608145
At round 164 accuracy: 0.8042168674698795
At round 164 training accuracy: 0.8227023319615913
At round 164 training loss: 0.5418737898695708
gradient difference: 5.962877213216801
At round 165 accuracy: 0.8042168674698795
At round 165 training accuracy: 0.821673525377229
At round 165 training loss: 0.5412783767585355
gradient difference: 5.971716885224293
At round 166 accuracy: 0.8012048192771084
At round 166 training accuracy: 0.8225308641975309
At round 166 training loss: 0.5403711850964432
gradient difference: 5.881755838120832
At round 167 accuracy: 0.802710843373494
At round 167 training accuracy: 0.8203017832647462
At round 167 training loss: 0.5410366434340758
gradient difference: 5.837944790158036
At round 168 accuracy: 0.8012048192771084
At round 168 training accuracy: 0.8203017832647462
At round 168 training loss: 0.5397090183900585
gradient difference: 5.779678483406328
At round 169 accuracy: 0.7996987951807228
At round 169 training accuracy: 0.8221879286694102
At round 169 training loss: 0.5375243510203934
gradient difference: 5.754082455800555
At round 170 accuracy: 0.8012048192771084
At round 170 training accuracy: 0.8213305898491083
At round 170 training loss: 0.5372475026039961
gradient difference: 5.774998090002899
At round 171 accuracy: 0.8012048192771084
At round 171 training accuracy: 0.8213305898491083
At round 171 training loss: 0.5368980491097598
gradient difference: 5.740138281482763
At round 172 accuracy: 0.8012048192771084
At round 172 training accuracy: 0.8197873799725651
At round 172 training loss: 0.5367335064687341
gradient difference: 5.718988688716523
At round 173 accuracy: 0.8012048192771084
At round 173 training accuracy: 0.8192729766803841
At round 173 training loss: 0.5364316446905948
gradient difference: 5.75604439936594
At round 174 accuracy: 0.8042168674698795
At round 174 training accuracy: 0.818758573388203
At round 174 training loss: 0.5350551648849216
gradient difference: 5.663890525369351
At round 175 accuracy: 0.802710843373494
At round 175 training accuracy: 0.818758573388203
At round 175 training loss: 0.5338635703865617
gradient difference: 5.614603621869321
At round 176 accuracy: 0.8012048192771084
At round 176 training accuracy: 0.8184156378600823
At round 176 training loss: 0.533841601864528
gradient difference: 5.6000863335484645
At round 177 accuracy: 0.8057228915662651
At round 177 training accuracy: 0.821673525377229
At round 177 training loss: 0.5304152320477018
gradient difference: 5.579452281117493
At round 178 accuracy: 0.8057228915662651
At round 178 training accuracy: 0.8213305898491083
At round 178 training loss: 0.5303033851911103
gradient difference: 5.569509346408806
At round 179 accuracy: 0.8042168674698795
At round 179 training accuracy: 0.821673525377229
At round 179 training loss: 0.5290315383157357
gradient difference: 5.527438996545147
At round 180 accuracy: 0.8102409638554217
At round 180 training accuracy: 0.825960219478738
At round 180 training loss: 0.5276340442062554
gradient difference: 5.481048377726045
At round 181 accuracy: 0.8117469879518072
At round 181 training accuracy: 0.8271604938271605
At round 181 training loss: 0.5275335831763108
gradient difference: 5.511522837482008
At round 182 accuracy: 0.8102409638554217
At round 182 training accuracy: 0.8271604938271605
At round 182 training loss: 0.5274690076388132
gradient difference: 5.527185129493481
At round 183 accuracy: 0.8102409638554217
At round 183 training accuracy: 0.8264746227709191
At round 183 training loss: 0.5264278146449799
gradient difference: 5.462971269404864
At round 184 accuracy: 0.8102409638554217
At round 184 training accuracy: 0.8264746227709191
At round 184 training loss: 0.526015690277779
gradient difference: 5.469692269708552
At round 185 accuracy: 0.8117469879518072
At round 185 training accuracy: 0.8273319615912208
At round 185 training loss: 0.5249113044053167
gradient difference: 5.390741699207606
At round 186 accuracy: 0.8102409638554217
At round 186 training accuracy: 0.8273319615912208
At round 186 training loss: 0.52439036457167
gradient difference: 5.366077811699325
At round 187 accuracy: 0.8102409638554217
At round 187 training accuracy: 0.825960219478738
At round 187 training loss: 0.5236261589772008
gradient difference: 5.326373190229088
At round 188 accuracy: 0.8087349397590361
At round 188 training accuracy: 0.8254458161865569
At round 188 training loss: 0.5226784080042746
gradient difference: 5.260809318694798
At round 189 accuracy: 0.8087349397590361
At round 189 training accuracy: 0.8261316872427984
At round 189 training loss: 0.5221007773726131
gradient difference: 5.264575282353663
At round 190 accuracy: 0.8087349397590361
At round 190 training accuracy: 0.825960219478738
At round 190 training loss: 0.5210484853483844
gradient difference: 5.2010923058714695
At round 191 accuracy: 0.8072289156626506
At round 191 training accuracy: 0.8254458161865569
At round 191 training loss: 0.5199883274014768
gradient difference: 5.135275843542911
At round 192 accuracy: 0.8072289156626506
At round 192 training accuracy: 0.8254458161865569
At round 192 training loss: 0.5193769801858383
gradient difference: 5.088727229462714
At round 193 accuracy: 0.8057228915662651
At round 193 training accuracy: 0.8256172839506173
At round 193 training loss: 0.5184832406153619
gradient difference: 5.126009015544822
At round 194 accuracy: 0.8072289156626506
At round 194 training accuracy: 0.8276748971193416
At round 194 training loss: 0.518454532105468
gradient difference: 5.1316703539027895
At round 195 accuracy: 0.8072289156626506
At round 195 training accuracy: 0.8276748971193416
At round 195 training loss: 0.5178375077147717
gradient difference: 5.089683810153994
At round 196 accuracy: 0.8087349397590361
At round 196 training accuracy: 0.8292181069958847
At round 196 training loss: 0.5158839360759715
gradient difference: 4.891317859641133
At round 197 accuracy: 0.8102409638554217
At round 197 training accuracy: 0.8302469135802469
At round 197 training loss: 0.5153478537702139
gradient difference: 4.842438080186458
At round 198 accuracy: 0.8117469879518072
At round 198 training accuracy: 0.8323045267489712
At round 198 training loss: 0.5140564308820524
gradient difference: 4.797964311001548
At round 199 accuracy: 0.8117469879518072
At round 199 training accuracy: 0.8319615912208504
At round 199 training loss: 0.5130325262085056
gradient difference: 4.694739524576592
At round 200 accuracy: 0.8102409638554217
At round 200 training accuracy: 0.8326474622770919
