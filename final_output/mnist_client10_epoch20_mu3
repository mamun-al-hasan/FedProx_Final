Arguments:
	       batch_size : 10
	clients_per_round : 10
	          dataset : mnist
	     drop_percent : 0.0
	       eval_every : 1
	    learning_rate : 0.01
	            model : mclr
	     model_params : (10,)
	               mu : 3.0
	       num_epochs : 20
	        num_iters : 1
	       num_rounds : 200
	        optimizer : fedprox
	             seed : 0
Using Federated prox to Train

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/47.08k flops)
  dense/kernel/Initializer/stateless_random_uniform (7.84k/15.68k flops)
    dense/kernel/Initializer/stateless_random_uniform/mul (7.84k/7.84k flops)
    dense/kernel/Initializer/stateless_random_uniform/sub (1/1 flops)
  PGD/update_dense/kernel/add (7.84k/7.84k flops)
  PGD/update_dense/kernel/mul (7.84k/7.84k flops)
  PGD/update_dense/kernel/mul_1 (7.84k/7.84k flops)
  PGD/update_dense/kernel/sub (7.84k/7.84k flops)
  PGD/update_dense/bias/add (10/10 flops)
  PGD/update_dense/bias/mul (10/10 flops)
  PGD/update_dense/bias/mul_1 (10/10 flops)
  PGD/update_dense/bias/sub (10/10 flops)
  gradients/sparse_softmax_cross_entropy_loss/value_grad/Neg (1/1 flops)
  gradients/sparse_softmax_cross_entropy_loss/value_grad/mul (1/1 flops)
  sparse_softmax_cross_entropy_loss/num_present/Equal (1/1 flops)

======================End of Report==========================
1000 Clients in Total
Training with 10 workers ---
At round 0 accuracy: 0.12562677869630032
At round 0 training accuracy: 0.12760230883974316
At round 0 training loss: 2.785880700883501
gradient difference: 89.47495366335541
At round 1 accuracy: 0.28526900664046617
At round 1 training accuracy: 0.28284908230105715
At round 1 training loss: 2.256746628593089
gradient difference: 79.35835191139898
At round 2 accuracy: 0.4098116275918146
At round 2 training accuracy: 0.4138724949737337
At round 2 training loss: 1.971905354140055
gradient difference: 71.10737951535394
At round 3 accuracy: 0.49478249085241904
At round 3 training accuracy: 0.5007620468253453
At round 3 training loss: 1.714111425864402
gradient difference: 64.6779437999586
At round 4 accuracy: 0.535031847133758
At round 4 training accuracy: 0.5398696413515792
At round 4 training loss: 1.5928074025157415
gradient difference: 60.192729416117764
At round 5 accuracy: 0.593169806206803
At round 5 training accuracy: 0.5904403657824762
At round 5 training loss: 1.4419598651583483
gradient difference: 55.47293694583085
At round 6 accuracy: 0.6071283371730587
At round 6 training accuracy: 0.6082592904857643
At round 6 training loss: 1.3393509203258969
gradient difference: 53.036085595646696
At round 7 accuracy: 0.6317929258707142
At round 7 training accuracy: 0.6315260393021597
At round 7 training loss: 1.2815655881729975
gradient difference: 51.40849472402556
At round 8 accuracy: 0.6716357229976961
At round 8 training accuracy: 0.6677151566249433
At round 8 training loss: 1.1580974022272619
gradient difference: 47.86289452523256
At round 9 accuracy: 0.7250304919365768
At round 9 training accuracy: 0.7229554445813606
At round 9 training loss: 1.0637675806792053
gradient difference: 44.415400319297945
At round 10 accuracy: 0.7442742919094728
At round 10 training accuracy: 0.7479408521953435
At round 10 training loss: 0.9734384185606225
gradient difference: 40.774242785053104
At round 11 accuracy: 0.7185255454668654
At round 11 training accuracy: 0.7195829820351515
At round 11 training loss: 0.9817606378154606
gradient difference: 41.090771023721
At round 12 accuracy: 0.7190676243393413
At round 12 training accuracy: 0.7164375121603217
At round 12 training loss: 0.9559951628029165
gradient difference: 40.54394911447905
At round 13 accuracy: 0.7244884130641008
At round 13 training accuracy: 0.7235553537842921
At round 13 training loss: 0.9269297571664473
gradient difference: 39.480303700128516
At round 14 accuracy: 0.7178479468762705
At round 14 training accuracy: 0.7188047214475647
At round 14 training loss: 0.9413721193129715
gradient difference: 39.74002227553973
At round 15 accuracy: 0.7469846862718525
At round 15 training accuracy: 0.7457519942927557
At round 15 training loss: 0.8715145868904044
gradient difference: 36.818939588874805
At round 16 accuracy: 0.7486109228892804
At round 16 training accuracy: 0.7461249108243077
At round 16 training loss: 0.8584937465507195
gradient difference: 36.55418410970852
At round 17 accuracy: 0.772326873560103
At round 17 training accuracy: 0.7745314222712238
At round 17 training loss: 0.7907730648711462
gradient difference: 33.88512248751958
At round 18 accuracy: 0.79143515381488
At round 18 training accuracy: 0.7929016148907192
At round 18 training loss: 0.7424472660684939
gradient difference: 32.65491864672038
At round 19 accuracy: 0.7923837918417129
At round 19 training accuracy: 0.7919450029184772
At round 19 training loss: 0.7137550418684648
gradient difference: 31.49750518935673
At round 20 accuracy: 0.8063423228079686
At round 20 training accuracy: 0.8032946364874506
At round 20 training loss: 0.6890170310055993
gradient difference: 30.369665713876163
At round 21 accuracy: 0.8166418213850115
At round 21 training accuracy: 0.8138822232310785
At round 21 training loss: 0.6603987762712102
gradient difference: 29.46615795468577
At round 22 accuracy: 0.8184035777205584
At round 22 training accuracy: 0.8147901939165965
At round 22 training loss: 0.6612449364847934
gradient difference: 29.46478200273748
At round 23 accuracy: 0.8175904594118444
At round 23 training accuracy: 0.8157954471755626
At round 23 training loss: 0.6595491194806795
gradient difference: 29.262661349545883
At round 24 accuracy: 0.8227402087003659
At round 24 training accuracy: 0.8185680005188404
At round 24 training loss: 0.6480353708806066
gradient difference: 29.399682778735066
At round 25 accuracy: 0.8345304241767177
At round 25 training accuracy: 0.8318308580322978
At round 25 training loss: 0.6175972426434487
gradient difference: 27.587306145254033
At round 26 accuracy: 0.8079685594253964
At round 26 training accuracy: 0.8064887476490045
At round 26 training loss: 0.6549567618038924
gradient difference: 29.862448740466366
At round 27 accuracy: 0.8131183087139179
At round 27 training accuracy: 0.81448213243401
At round 27 training loss: 0.6404924546488316
gradient difference: 29.422882519968145
At round 28 accuracy: 0.8143379861769887
At round 28 training accuracy: 0.8152441792593553
At round 28 training loss: 0.6406575246649487
gradient difference: 29.042095990234518
At round 29 accuracy: 0.8135248678682748
At round 29 training accuracy: 0.8132661002659057
At round 29 training loss: 0.652002510589506
gradient difference: 29.532808046310556
At round 30 accuracy: 0.822333649546009
At round 30 training accuracy: 0.821000064855049
At round 30 training loss: 0.636585721818085
gradient difference: 29.2133725458126
At round 31 accuracy: 0.8306003523512671
At round 31 training accuracy: 0.827064011933329
At round 31 training loss: 0.6174299794975437
gradient difference: 28.12618161257684
At round 32 accuracy: 0.8284320368613634
At round 32 training accuracy: 0.8283773266748816
At round 32 training loss: 0.6090446632180048
gradient difference: 27.73125837406735
At round 33 accuracy: 0.8465916790893075
At round 33 training accuracy: 0.8414456190414423
At round 33 training loss: 0.5825922572096793
gradient difference: 26.48012769031646
At round 34 accuracy: 0.8361566607941455
At round 34 training accuracy: 0.8351708930540243
At round 34 training loss: 0.5818347341054063
gradient difference: 26.405875487708755
At round 35 accuracy: 0.8451009621899986
At round 35 training accuracy: 0.8448829366366172
At round 35 training loss: 0.5560019725756213
gradient difference: 25.277047477923315
At round 36 accuracy: 0.8469982382436645
At round 36 training accuracy: 0.8425805823983397
At round 36 training loss: 0.570326538467592
gradient difference: 26.006033969203944
At round 37 accuracy: 0.8242309255996747
At round 37 training accuracy: 0.8226538686036708
At round 37 training loss: 0.5922895383860014
gradient difference: 26.99591117001912
At round 38 accuracy: 0.8575687762569454
At round 38 training accuracy: 0.8559731500097283
At round 38 training loss: 0.5378092692041366
gradient difference: 23.583268599184812
At round 39 accuracy: 0.871391787505082
At round 39 training accuracy: 0.8699169855373241
At round 39 training loss: 0.49812149063873284
gradient difference: 22.220679959441718
At round 40 accuracy: 0.8666485973709175
At round 40 training accuracy: 0.8639827485569752
At round 40 training loss: 0.509432990920098
gradient difference: 22.729424917038912
At round 41 accuracy: 0.8596015720287302
At round 41 training accuracy: 0.8594753226538686
At round 41 training loss: 0.513151039682221
gradient difference: 22.827930652330313
At round 42 accuracy: 0.8692234720151782
At round 42 training accuracy: 0.8648096504312861
At round 42 training loss: 0.5008148504191591
gradient difference: 22.436044326126808
At round 43 accuracy: 0.8699010706057732
At round 43 training accuracy: 0.8657986899280109
At round 43 training loss: 0.49827453259843035
gradient difference: 22.21498025197489
At round 44 accuracy: 0.8696300311695352
At round 44 training accuracy: 0.8641610999416305
At round 44 training loss: 0.49889155114728556
gradient difference: 22.375821836244207
At round 45 accuracy: 0.8697655508876542
At round 45 training accuracy: 0.8648096504312861
At round 45 training loss: 0.4967963764317997
gradient difference: 22.478262218971874
At round 46 accuracy: 0.8656999593440846
At round 46 training accuracy: 0.8648420779557688
At round 46 training loss: 0.4941126912046685
gradient difference: 22.33885276298288
At round 47 accuracy: 0.851334869223472
At round 47 training accuracy: 0.8528601076593813
At round 47 training loss: 0.517288704193093
gradient difference: 23.250364753179216
At round 48 accuracy: 0.8493020734516872
At round 48 training accuracy: 0.8513684415331734
At round 48 training loss: 0.5210855199958243
gradient difference: 23.57040101271388
At round 49 accuracy: 0.8568911776663505
At round 49 training accuracy: 0.8597671703742137
At round 49 training loss: 0.5074171722756121
gradient difference: 22.996046732579796
At round 50 accuracy: 0.8623119663911099
At round 50 training accuracy: 0.8642583825150788
At round 50 training loss: 0.4900130731852695
gradient difference: 22.41621285174617
At round 51 accuracy: 0.8604146903374441
At round 51 training accuracy: 0.8616803943186977
At round 51 training loss: 0.4912370552319548
gradient difference: 22.578923081044785
At round 52 accuracy: 0.8628540452635859
At round 52 training accuracy: 0.8638854659835268
At round 52 training loss: 0.48818095822153745
gradient difference: 22.562895740476808
At round 53 accuracy: 0.8686813931427023
At round 53 training accuracy: 0.8685712432712887
At round 53 training loss: 0.4773726393944336
gradient difference: 21.50324235975847
At round 54 accuracy: 0.8647513213172516
At round 54 training accuracy: 0.8654906284454245
At round 54 training loss: 0.48270723780685854
gradient difference: 21.66058535429127
At round 55 accuracy: 0.8773546551023174
At round 55 training accuracy: 0.8781211492314677
At round 55 training loss: 0.45344855266529155
gradient difference: 19.969295508707578
At round 56 accuracy: 0.8755928987667706
At round 56 training accuracy: 0.8768726895388806
At round 56 training loss: 0.45343068925950974
gradient difference: 19.799981141106052
At round 57 accuracy: 0.8774901748204363
At round 57 training accuracy: 0.8784292107140541
At round 57 training loss: 0.44824353711488346
gradient difference: 19.66273415036845
At round 58 accuracy: 0.8705786691963681
At round 58 training accuracy: 0.8718139957195667
At round 58 training loss: 0.4598543728867329
gradient difference: 19.9827450401015
At round 59 accuracy: 0.8666485973709175
At round 59 training accuracy: 0.8705006809780141
At round 59 training loss: 0.4624897181939804
gradient difference: 20.235394264262915
At round 60 accuracy: 0.831684510096219
At round 60 training accuracy: 0.8350736104805759
At round 60 training loss: 0.5198896544439388
gradient difference: 23.005500318773898
At round 61 accuracy: 0.8395446537471202
At round 61 training accuracy: 0.842840002594202
At round 61 training loss: 0.5003492284937147
gradient difference: 22.44937113818761
At round 62 accuracy: 0.8639382030085377
At round 62 training accuracy: 0.8647772229068033
At round 62 training loss: 0.45993098586031106
gradient difference: 20.473776064206167
At round 63 accuracy: 0.8665130776527985
At round 63 training accuracy: 0.8698845580128413
At round 63 training loss: 0.44834062032834276
gradient difference: 20.18918750281137
At round 64 accuracy: 0.8685458734245832
At round 64 training accuracy: 0.8711330177054284
At round 64 training loss: 0.44485798421681816
gradient difference: 20.125933412511703
At round 65 accuracy: 0.8701721100420111
At round 65 training accuracy: 0.8722517673000844
At round 65 training loss: 0.4444531337222759
gradient difference: 19.804046119545735
At round 66 accuracy: 0.881149207209649
At round 66 training accuracy: 0.8801316557494001
At round 66 training loss: 0.4294360730356367
gradient difference: 18.942844987757546
At round 67 accuracy: 0.8777612142566743
At round 67 training accuracy: 0.8787859134833647
At round 67 training loss: 0.4302464974366432
gradient difference: 18.950481156720855
At round 68 accuracy: 0.8758639382030086
At round 68 training accuracy: 0.8768402620143978
At round 68 training loss: 0.435624198302227
gradient difference: 19.615704780477497
At round 69 accuracy: 0.8768125762298414
At round 69 training accuracy: 0.8788994098190545
At round 69 training loss: 0.4354915518928649
gradient difference: 19.41964030599808
At round 70 accuracy: 0.8795229705922212
At round 70 training accuracy: 0.883033919190609
At round 70 training loss: 0.426352321629801
gradient difference: 19.02881928184653
At round 71 accuracy: 0.8837240818539097
At round 71 training accuracy: 0.88444451650561
At round 71 training loss: 0.41943340242086363
gradient difference: 18.82183029446513
At round 72 accuracy: 0.8877896733974794
At round 72 training accuracy: 0.8865523055969907
At round 72 training loss: 0.42025611109667266
gradient difference: 18.846681845102072
At round 73 accuracy: 0.8864344762162895
At round 73 training accuracy: 0.8845742266035411
At round 73 training loss: 0.4271669717699703
gradient difference: 19.131130615949047
At round 74 accuracy: 0.8829109635451958
At round 74 training accuracy: 0.8811206952461249
At round 74 training loss: 0.4354337359256054
gradient difference: 19.558219094502714
At round 75 accuracy: 0.8881962325518363
At round 75 training accuracy: 0.8865523055969907
At round 75 training loss: 0.42242252674431907
gradient difference: 18.67605886261987
At round 76 accuracy: 0.890635587477978
At round 76 training accuracy: 0.8884655295414748
At round 76 training loss: 0.4114678822832518
gradient difference: 18.184989222036595
At round 77 accuracy: 0.890635587477978
At round 77 training accuracy: 0.8894059277514754
At round 77 training loss: 0.40828840790041154
gradient difference: 17.98440010793551
At round 78 accuracy: 0.8867055156525274
At round 78 training accuracy: 0.8859686101563007
At round 78 training loss: 0.40969797515597434
gradient difference: 18.031040080878945
At round 79 accuracy: 0.8895514297330261
At round 79 training accuracy: 0.888141254296647
At round 79 training loss: 0.40348725527009366
gradient difference: 17.875715718545052
At round 80 accuracy: 0.8873831142431223
At round 80 training accuracy: 0.8881250405344056
At round 80 training loss: 0.40512691764212333
gradient difference: 17.967185195504282
At round 81 accuracy: 0.891042146632335
At round 81 training accuracy: 0.8905571048706141
At round 81 training loss: 0.40131835995724724
gradient difference: 17.66342701128258
At round 82 accuracy: 0.8892803902967882
At round 82 training accuracy: 0.8905571048706141
At round 82 training loss: 0.3995235916221666
gradient difference: 17.581429789886933
At round 83 accuracy: 0.891584225504811
At round 83 training accuracy: 0.8916596407030287
At round 83 training loss: 0.396023480473018
gradient difference: 17.495311501833307
At round 84 accuracy: 0.8886027917061933
At round 84 training accuracy: 0.8921946948569947
At round 84 training loss: 0.3930178707716435
gradient difference: 17.400002029224733
At round 85 accuracy: 0.8884672719880743
At round 85 training accuracy: 0.8890005836954407
At round 85 training loss: 0.3993328746291167
gradient difference: 17.48703578701466
At round 86 accuracy: 0.8877896733974794
At round 86 training accuracy: 0.8893248589402685
At round 86 training loss: 0.39918643472938126
gradient difference: 17.696290764199823
At round 87 accuracy: 0.8856213579075756
At round 87 training accuracy: 0.8883358194435437
At round 87 training loss: 0.40194187521040214
gradient difference: 17.809845984090305
At round 88 accuracy: 0.8862989564981705
At round 88 training accuracy: 0.8878980478630262
At round 88 training loss: 0.40309000532451167
gradient difference: 17.89859897921462
At round 89 accuracy: 0.8875186339612413
At round 89 training accuracy: 0.8883682469680264
At round 89 training loss: 0.40444496843210176
gradient difference: 17.90299882522587
At round 90 accuracy: 0.8881962325518363
At round 90 training accuracy: 0.8898274855697516
At round 90 training loss: 0.4031489990531168
gradient difference: 17.881150661914898
At round 91 accuracy: 0.8867055156525274
At round 91 training accuracy: 0.8889681561709579
At round 91 training loss: 0.4020505356501803
gradient difference: 17.806008848922193
At round 92 accuracy: 0.8861634367800515
At round 92 training accuracy: 0.8893410727025098
At round 92 training loss: 0.4008449151777627
gradient difference: 17.704600119565843
At round 93 accuracy: 0.8778967339747933
At round 93 training accuracy: 0.8792885401128477
At round 93 training loss: 0.4154510366943547
gradient difference: 18.329365341218466
At round 94 accuracy: 0.8835885621357907
At round 94 training accuracy: 0.8865847331214735
At round 94 training loss: 0.40516478563495506
gradient difference: 17.91462203033546
At round 95 accuracy: 0.8795229705922212
At round 95 training accuracy: 0.8828555678059536
At round 95 training loss: 0.4103018576367468
gradient difference: 18.049960600702697
At round 96 accuracy: 0.8768125762298414
At round 96 training accuracy: 0.8799370906025034
At round 96 training loss: 0.41770780113262884
gradient difference: 18.40184275489
At round 97 accuracy: 0.871391787505082
At round 97 training accuracy: 0.8757863674687074
At round 97 training loss: 0.4262367524241067
gradient difference: 18.552849468819005
At round 98 accuracy: 0.8647513213172516
At round 98 training accuracy: 0.8673552111031844
At round 98 training loss: 0.4388872704490593
gradient difference: 19.094611276304462
At round 99 accuracy: 0.8647513213172516
At round 99 training accuracy: 0.8670795771450808
At round 99 training loss: 0.4426448538976312
gradient difference: 19.125468155054865
At round 100 accuracy: 0.8762704973573655
At round 100 training accuracy: 0.8811044814838835
At round 100 training loss: 0.4099286845724249
gradient difference: 18.032003233943215
At round 101 accuracy: 0.8787098522835073
At round 101 training accuracy: 0.8840715999740579
At round 101 training loss: 0.4076755420763943
gradient difference: 18.053212834265597
At round 102 accuracy: 0.8751863396124137
At round 102 training accuracy: 0.8807964200012971
At round 102 training loss: 0.41284652357118035
gradient difference: 18.23817884006715
At round 103 accuracy: 0.880607128337173
At round 103 training accuracy: 0.8862442441144043
At round 103 training loss: 0.4097900119967946
gradient difference: 18.020614873736974
At round 104 accuracy: 0.8783032931291503
At round 104 training accuracy: 0.8842499513587133
At round 104 training loss: 0.40891768123226824
gradient difference: 18.06374988650301
At round 105 accuracy: 0.881555766364006
At round 105 training accuracy: 0.886730656981646
At round 105 training loss: 0.40689293382708325
gradient difference: 18.225114961375095
At round 106 accuracy: 0.8835885621357907
At round 106 training accuracy: 0.8892600038913029
At round 106 training loss: 0.4013746178045345
gradient difference: 18.055495614605505
At round 107 accuracy: 0.8860279170619325
At round 107 training accuracy: 0.8913191516959595
At round 107 training loss: 0.39279199528798237
gradient difference: 17.511568621860587
At round 108 accuracy: 0.8849437593169807
At round 108 training accuracy: 0.889843699331993
At round 108 training loss: 0.3967096649625054
gradient difference: 17.6977418470424
At round 109 accuracy: 0.8871120748068844
At round 109 training accuracy: 0.8905246773461314
At round 109 training loss: 0.3946458699935601
gradient difference: 17.764938047118388
At round 110 accuracy: 0.8898224691692641
At round 110 training accuracy: 0.891173227835787
At round 110 training loss: 0.37930831085155814
gradient difference: 17.30889761398536
At round 111 accuracy: 0.8925328635316438
At round 111 training accuracy: 0.8956320124521694
At round 111 training loss: 0.37185261075553616
gradient difference: 16.96187721777602
At round 112 accuracy: 0.8949722184577856
At round 112 training accuracy: 0.8984694208444127
At round 112 training loss: 0.3664171943780849
gradient difference: 16.583676164255884
At round 113 accuracy: 0.8929394226860008
At round 113 training accuracy: 0.8987774823269992
At round 113 training loss: 0.36757436036537866
gradient difference: 16.54106315975075
At round 114 accuracy: 0.891313186068573
At round 114 training accuracy: 0.8970426097671704
At round 114 training loss: 0.37530702779002983
gradient difference: 16.84876352070033
At round 115 accuracy: 0.8953787776121426
At round 115 training accuracy: 0.8991666126207926
At round 115 training loss: 0.3695174191296642
gradient difference: 16.654064604630047
At round 116 accuracy: 0.8922618240954059
At round 116 training accuracy: 0.8975128088721707
At round 116 training loss: 0.3735104297000314
gradient difference: 16.84684748463884
At round 117 accuracy: 0.8921263043772869
At round 117 training accuracy: 0.896150852843894
At round 117 training loss: 0.3764480820572077
gradient difference: 17.016855881571388
At round 118 accuracy: 0.8937525409947147
At round 118 training accuracy: 0.897691160256826
At round 118 training loss: 0.37205531316765805
gradient difference: 16.940943788128997
At round 119 accuracy: 0.8948366987396666
At round 119 training accuracy: 0.8998475906349309
At round 119 training loss: 0.36985614044154846
gradient difference: 16.826028751212572
At round 120 accuracy: 0.891177666350454
At round 120 training accuracy: 0.8937836435566509
At round 120 training loss: 0.38116592241059627
gradient difference: 17.07389279083746
At round 121 accuracy: 0.89036454804174
At round 121 training accuracy: 0.8926486801997535
At round 121 training loss: 0.38726881162971777
gradient difference: 17.23125126827412
At round 122 accuracy: 0.8887383114243123
At round 122 training accuracy: 0.8922271223814774
At round 122 training loss: 0.3742702593187954
gradient difference: 16.86423229623557
At round 123 accuracy: 0.8894159100149072
At round 123 training accuracy: 0.8920001297100979
At round 123 training loss: 0.375644491432862
gradient difference: 16.855892368612032
At round 124 accuracy: 0.8895514297330261
At round 124 training accuracy: 0.8930378104935469
At round 124 training loss: 0.37470764569906534
gradient difference: 16.912581562557193
At round 125 accuracy: 0.8928039029678818
At round 125 training accuracy: 0.8949996757247551
At round 125 training loss: 0.3715562044187261
gradient difference: 16.91832817311613
At round 126 accuracy: 0.8945656593034287
At round 126 training accuracy: 0.8965724106621701
At round 126 training loss: 0.3643741391560703
gradient difference: 16.66922654755216
At round 127 accuracy: 0.8951077381759046
At round 127 training accuracy: 0.8971723198651015
At round 127 training loss: 0.36619112965063094
gradient difference: 16.87436858188147
At round 128 accuracy: 0.8861634367800515
At round 128 training accuracy: 0.8870062909397497
At round 128 training loss: 0.3777745446094563
gradient difference: 17.484667357501113
At round 129 accuracy: 0.8884672719880743
At round 129 training accuracy: 0.8892600038913029
At round 129 training loss: 0.3781232288066143
gradient difference: 17.503341271469075
At round 130 accuracy: 0.8955142973302616
At round 130 training accuracy: 0.8954212335430313
At round 130 training loss: 0.36772148104926217
gradient difference: 17.06198945411649
At round 131 accuracy: 0.8971405339476893
At round 131 training accuracy: 0.897691160256826
At round 131 training loss: 0.3638537635449186
gradient difference: 16.815262461549906
At round 132 accuracy: 0.8984957311288793
At round 132 training accuracy: 0.8991990401452753
At round 132 training loss: 0.36022142738951773
gradient difference: 16.3603518872305
At round 133 accuracy: 0.8978181325382844
At round 133 training accuracy: 0.8975938776833776
At round 133 training loss: 0.36433222748226707
gradient difference: 16.555123411587513
At round 134 accuracy: 0.8982246916926413
At round 134 training accuracy: 0.8988585511382061
At round 134 training loss: 0.3605467049341316
gradient difference: 16.325474786053633
At round 135 accuracy: 0.8957853367664995
At round 135 training accuracy: 0.8957779363123419
At round 135 training loss: 0.36741448251827824
gradient difference: 16.70240836137694
At round 136 accuracy: 0.8951077381759046
At round 136 training accuracy: 0.895615798689928
At round 136 training loss: 0.3650211489773761
gradient difference: 16.7283573137229
At round 137 accuracy: 0.8933459818403577
At round 137 training accuracy: 0.8932648031649264
At round 137 training loss: 0.3732223853155595
gradient difference: 17.129911034447954
At round 138 accuracy: 0.8942946198671907
At round 138 training accuracy: 0.8971398923406187
At round 138 training loss: 0.36515473439880947
gradient difference: 16.78418198145936
At round 139 accuracy: 0.8997154085919501
At round 139 training accuracy: 0.9018580971528634
At round 139 training loss: 0.3567242234751954
gradient difference: 16.393039841430504
At round 140 accuracy: 0.8997154085919501
At round 140 training accuracy: 0.9012095466632077
At round 140 training loss: 0.3573084598714228
gradient difference: 16.570887783515275
At round 141 accuracy: 0.8993088494375931
At round 141 training accuracy: 0.8998638043971723
At round 141 training loss: 0.35659742963246216
gradient difference: 16.550100842426918
At round 142 accuracy: 0.8970050142295704
At round 142 training accuracy: 0.8972209611518257
At round 142 training loss: 0.368132807903094
gradient difference: 17.11180372378924
At round 143 accuracy: 0.8945656593034287
At round 143 training accuracy: 0.8982262144107919
At round 143 training loss: 0.36840423292543406
gradient difference: 17.151224154684368
At round 144 accuracy: 0.8982246916926413
At round 144 training accuracy: 0.8994260328166548
At round 144 training loss: 0.36281540098774223
gradient difference: 17.028834824731437
At round 145 accuracy: 0.8979536522564033
At round 145 training accuracy: 0.9001232245930346
At round 145 training loss: 0.3632132347351663
gradient difference: 17.00045559018076
At round 146 accuracy: 0.8978181325382844
At round 146 training accuracy: 0.9015824631947598
At round 146 training loss: 0.3589894755501115
gradient difference: 16.796794338630363
At round 147 accuracy: 0.8964629353570944
At round 147 training accuracy: 0.8998151631104482
At round 147 training loss: 0.36846721986392894
gradient difference: 17.168546031167313
At round 148 accuracy: 0.8974115733839274
At round 148 training accuracy: 0.8995233153901031
At round 148 training loss: 0.3659969045236024
gradient difference: 17.060225406631066
At round 149 accuracy: 0.8933459818403577
At round 149 training accuracy: 0.8968966859069979
At round 149 training loss: 0.3690878740144959
gradient difference: 17.09738597078039
At round 150 accuracy: 0.8928039029678818
At round 150 training accuracy: 0.8946754004799273
At round 150 training loss: 0.3711760362689769
gradient difference: 17.247433630901163
At round 151 accuracy: 0.8952432578940236
At round 151 training accuracy: 0.8936539334587198
At round 151 training loss: 0.3764428764411682
gradient difference: 17.5431225107737
At round 152 accuracy: 0.8925328635316438
At round 152 training accuracy: 0.8948699656268241
At round 152 training loss: 0.37501447317822867
gradient difference: 17.669081425775488
At round 153 accuracy: 0.891177666350454
At round 153 training accuracy: 0.8920649847590635
At round 153 training loss: 0.37532132902975157
gradient difference: 17.65820289756224
At round 154 accuracy: 0.891448705786692
At round 154 training accuracy: 0.8929243141578572
At round 154 training loss: 0.3730550805966476
gradient difference: 17.393697696979952
At round 155 accuracy: 0.8896869494511451
At round 155 training accuracy: 0.8906868149685453
At round 155 training loss: 0.38172123846012684
gradient difference: 17.476177175171163
At round 156 accuracy: 0.8909066269142161
At round 156 training accuracy: 0.8922757636682016
At round 156 training loss: 0.3765738286422926
gradient difference: 17.34270065458924
At round 157 accuracy: 0.8880607128337173
At round 157 training accuracy: 0.8891465075556132
At round 157 training loss: 0.3832851632953281
gradient difference: 17.46845561670974
At round 158 accuracy: 0.890500067759859
At round 158 training accuracy: 0.8909300214021662
At round 158 training loss: 0.3745619272318517
gradient difference: 17.303530157114253
At round 159 accuracy: 0.8933459818403577
At round 159 training accuracy: 0.8936701472209612
At round 159 training loss: 0.3669127173186879
gradient difference: 17.04676130854546
At round 160 accuracy: 0.8967339747933324
At round 160 training accuracy: 0.8985342758933783
At round 160 training loss: 0.3573473349344187
gradient difference: 16.711863913486717
At round 161 accuracy: 0.8968694945114514
At round 161 training accuracy: 0.9002367209287243
At round 161 training loss: 0.35222494873144006
gradient difference: 16.312238122166114
At round 162 accuracy: 0.8942946198671907
At round 162 training accuracy: 0.8996692392502756
At round 162 training loss: 0.3538892924660222
gradient difference: 13.159393755706041
At round 163 accuracy: 0.8970050142295704
At round 163 training accuracy: 0.9019229522018289
At round 163 training loss: 0.348975472730059
gradient difference: 12.747819786101797
At round 164 accuracy: 0.8944301395853097
At round 164 training accuracy: 0.898728841040275
At round 164 training loss: 0.3548485507798536
gradient difference: 12.91996552286262
At round 165 accuracy: 0.8938880607128337
At round 165 training accuracy: 0.8988261236137234
At round 165 training loss: 0.352880802762112
gradient difference: 12.81232889221355
At round 166 accuracy: 0.8909066269142161
At round 166 training accuracy: 0.8966048381866528
At round 166 training loss: 0.3581152809016422
gradient difference: 12.959657002803315
At round 167 accuracy: 0.8892803902967882
At round 167 training accuracy: 0.8945294766197548
At round 167 training loss: 0.36147103710019723
gradient difference: 12.99072177984839
At round 168 accuracy: 0.890771107196097
At round 168 training accuracy: 0.897026396004929
At round 168 training loss: 0.35596234450584013
gradient difference: 12.951731246258515
At round 169 accuracy: 0.8957853367664995
At round 169 training accuracy: 0.9010149815163111
At round 169 training loss: 0.34934561778449075
gradient difference: 12.771954321567158
At round 170 accuracy: 0.8959208564846185
At round 170 training accuracy: 0.9013068292366561
At round 170 training loss: 0.3483760677261371
gradient difference: 12.708360058351236
At round 171 accuracy: 0.9006640466187831
At round 171 training accuracy: 0.9055224074194176
At round 171 training loss: 0.339448534848043
gradient difference: 12.38202924471738
At round 172 accuracy: 0.901612684645616
At round 172 training accuracy: 0.9055061936571762
At round 172 training loss: 0.3396461265742932
gradient difference: 12.46870195152448
At round 173 accuracy: 0.8993088494375931
At round 173 training accuracy: 0.9050684220766587
At round 173 training loss: 0.34105922111988185
gradient difference: 12.56772043247027
At round 174 accuracy: 0.8976826128201654
At round 174 training accuracy: 0.9048090018807964
At round 174 training loss: 0.3418659977012682
gradient difference: 12.588629477934235
At round 175 accuracy: 0.8899579888873831
At round 175 training accuracy: 0.8959725014592386
At round 175 training loss: 0.3560817894128901
gradient difference: 13.110564098701536
At round 176 accuracy: 0.8959208564846185
At round 176 training accuracy: 0.9028633504118295
At round 176 training loss: 0.34119691471590685
gradient difference: 12.599314257771072
At round 177 accuracy: 0.8984957311288793
At round 177 training accuracy: 0.9038848174330372
At round 177 training loss: 0.34082309513505205
gradient difference: 12.467192237118452
At round 178 accuracy: 0.8979536522564033
At round 178 training accuracy: 0.9035119009014851
At round 178 training loss: 0.34154837275754074
gradient difference: 12.449757422590267
At round 179 accuracy: 0.8970050142295704
At round 179 training accuracy: 0.9024417925935534
At round 179 training loss: 0.3415066180020782
gradient difference: 12.471959483016464
At round 180 accuracy: 0.8862989564981705
At round 180 training accuracy: 0.889097866268889
At round 180 training loss: 0.3645626262845187
gradient difference: 13.46978913595837
At round 181 accuracy: 0.8867055156525274
At round 181 training accuracy: 0.8910597315000973
At round 181 training loss: 0.3603147886550871
gradient difference: 13.254820499940198
At round 182 accuracy: 0.8877896733974794
At round 182 training accuracy: 0.8907840975419936
At round 182 training loss: 0.3596827183845942
gradient difference: 13.131511358195814
At round 183 accuracy: 0.891584225504811
At round 183 training accuracy: 0.8935242233607886
At round 183 training loss: 0.35491547441476406
gradient difference: 12.954723506182653
At round 184 accuracy: 0.89036454804174
At round 184 training accuracy: 0.8924379012906155
At round 184 training loss: 0.3554591629913289
gradient difference: 12.999878471832416
At round 185 accuracy: 0.8922618240954059
At round 185 training accuracy: 0.894578117906479
At round 185 training loss: 0.35116543385288207
gradient difference: 12.846352492849597
At round 186 accuracy: 0.8884672719880743
At round 186 training accuracy: 0.8899085543809585
At round 186 training loss: 0.3583630771799664
gradient difference: 13.113123249467675
At round 187 accuracy: 0.890771107196097
At round 187 training accuracy: 0.8937025747454439
At round 187 training loss: 0.3515026631744652
gradient difference: 12.612028148809024
At round 188 accuracy: 0.8978181325382844
At round 188 training accuracy: 0.8992801089564822
At round 188 training loss: 0.34268786525758477
gradient difference: 12.264196917819394
At round 189 accuracy: 0.8873831142431223
At round 189 training accuracy: 0.8897139892340619
At round 189 training loss: 0.35670871854692043
gradient difference: 12.930212784474449
At round 190 accuracy: 0.8959208564846185
At round 190 training accuracy: 0.8990044749983787
At round 190 training loss: 0.34033184865312266
gradient difference: 12.158195080971275
At round 191 accuracy: 0.901341645209378
At round 191 training accuracy: 0.9029444192230365
At round 191 training loss: 0.3346882961309658
gradient difference: 12.004578915221432
At round 192 accuracy: 0.8986312508469982
At round 192 training accuracy: 0.9039658862442441
At round 192 training loss: 0.33488256389654997
gradient difference: 11.948543305552146
At round 193 accuracy: 0.9018837240818539
At round 193 training accuracy: 0.9055548349439004
At round 193 training loss: 0.3325560513830797
gradient difference: 11.754468057327177
At round 194 accuracy: 0.901612684645616
At round 194 training accuracy: 0.9058791101887282
At round 194 training loss: 0.3332135785156098
gradient difference: 11.771917372650933
At round 195 accuracy: 0.8998509283100691
At round 195 training accuracy: 0.9030092742720021
At round 195 training loss: 0.33782565734541325
gradient difference: 11.915104997477668
At round 196 accuracy: 0.8991733297194742
At round 196 training accuracy: 0.9032686944678643
At round 196 training loss: 0.3371640244739607
gradient difference: 11.926140565830293
At round 197 accuracy: 0.900257487464426
At round 197 training accuracy: 0.9024093650690707
At round 197 training loss: 0.3360407752426063
gradient difference: 11.884090909125936
At round 198 accuracy: 0.8994443691557121
At round 198 training accuracy: 0.9022472274466568
At round 198 training loss: 0.3384961506005852
gradient difference: 11.99074370660311
At round 199 accuracy: 0.8995798888738311
At round 199 training accuracy: 0.9031389843699332
At round 199 training loss: 0.3383684823622263
gradient difference: 11.988165338883713
At round 200 accuracy: 0.901206125491259
At round 200 training accuracy: 0.9051170633633828
