Arguments:
	       batch_size : 10
	clients_per_round : 10
	          dataset : synthetic_1_1
	     drop_percent : 0.0
	       eval_every : 1
	    learning_rate : 0.01
	            model : mclr
	     model_params : (10,)
	               mu : 5.0
	       num_epochs : 20
	        num_iters : 1
	       num_rounds : 200
	        optimizer : fedprox
	             seed : 0
Using Federated prox to Train

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/3.64k flops)
  dense/kernel/Initializer/stateless_random_uniform (600/1.20k flops)
    dense/kernel/Initializer/stateless_random_uniform/mul (600/600 flops)
    dense/kernel/Initializer/stateless_random_uniform/sub (1/1 flops)
  PGD/update_dense/kernel/add (600/600 flops)
  PGD/update_dense/kernel/mul (600/600 flops)
  PGD/update_dense/kernel/mul_1 (600/600 flops)
  PGD/update_dense/kernel/sub (600/600 flops)
  PGD/update_dense/bias/add (10/10 flops)
  PGD/update_dense/bias/mul (10/10 flops)
  PGD/update_dense/bias/mul_1 (10/10 flops)
  PGD/update_dense/bias/sub (10/10 flops)
  gradients/sparse_softmax_cross_entropy_loss/value_grad/Neg (1/1 flops)
  gradients/sparse_softmax_cross_entropy_loss/value_grad/mul (1/1 flops)
  sparse_softmax_cross_entropy_loss/num_present/Equal (1/1 flops)

======================End of Report==========================
30 Clients in Total
Training with 10 workers ---
At round 0 accuracy: 0.011299435028248588
At round 0 training accuracy: 0.010963230318069029
At round 0 training loss: 3.479582038513699
gradient difference: 148.6245655643957
At round 1 accuracy: 0.010492332526230832
At round 1 training accuracy: 0.00997067448680352
At round 1 training loss: 3.1429808665015697
gradient difference: 141.95842028886187
At round 2 accuracy: 0.007667473769168685
At round 2 training accuracy: 0.00834649221745996
At round 2 training loss: 3.0375547096150037
gradient difference: 139.78805751711755
At round 3 accuracy: 0.06820016142050041
At round 3 training accuracy: 0.06424543198736747
At round 3 training loss: 2.966376202248462
gradient difference: 133.75466396987656
At round 4 accuracy: 0.07223567393058919
At round 4 training accuracy: 0.0697496052334762
At round 4 training loss: 2.753328380234054
gradient difference: 133.70361028255883
At round 5 accuracy: 0.6501210653753027
At round 5 training accuracy: 0.6487705842544552
At round 5 training loss: 1.504749412359676
gradient difference: 116.8414445387241
At round 6 accuracy: 0.655367231638418
At round 6 training accuracy: 0.6521994134897361
At round 6 training loss: 1.3817970308444314
gradient difference: 118.15755055426557
At round 7 accuracy: 0.655367231638418
At round 7 training accuracy: 0.6524701105346267
At round 7 training loss: 1.3702474872710444
gradient difference: 114.31674332066524
At round 8 accuracy: 0.655367231638418
At round 8 training accuracy: 0.652921272276111
At round 8 training loss: 1.3460470501023385
gradient difference: 117.39367450125847
At round 9 accuracy: 0.6577885391444713
At round 9 training accuracy: 0.65323708549515
At round 9 training loss: 1.2771432501277993
gradient difference: 114.77161581930065
At round 10 accuracy: 0.6565778853914447
At round 10 training accuracy: 0.6530115046244078
At round 10 training loss: 1.261701775648619
gradient difference: 108.84170205037634
At round 11 accuracy: 0.6533494753833737
At round 11 training accuracy: 0.6469659372885179
At round 11 training loss: 1.2417422347377498
gradient difference: 106.60067550273999
At round 12 accuracy: 0.7429378531073446
At round 12 training accuracy: 0.734762012181367
At round 12 training loss: 1.1574654735307137
gradient difference: 100.58577610938143
At round 13 accuracy: 0.7429378531073446
At round 13 training accuracy: 0.73841642228739
At round 13 training loss: 1.1805295153622053
gradient difference: 102.63161473892893
At round 14 accuracy: 0.7380952380952381
At round 14 training accuracy: 0.7349424768779608
At round 14 training loss: 1.0716124638972206
gradient difference: 100.27920861753904
At round 15 accuracy: 0.7405165456012913
At round 15 training accuracy: 0.7362959621024138
At round 15 training loss: 1.089188246005764
gradient difference: 101.24661321532307
At round 16 accuracy: 0.7380952380952381
At round 16 training accuracy: 0.7352582900969997
At round 16 training loss: 1.0023134142046128
gradient difference: 100.16528429295474
At round 17 accuracy: 0.7409200968523002
At round 17 training accuracy: 0.7384615384615385
At round 17 training loss: 0.9890368230867784
gradient difference: 97.17374841636992
At round 18 accuracy: 0.7542372881355932
At round 18 training accuracy: 0.751139183397248
At round 18 training loss: 1.004899740278277
gradient difference: 93.08216481596926
At round 19 accuracy: 0.7562550443906376
At round 19 training accuracy: 0.7553801037672006
At round 19 training loss: 1.0154551556087255
gradient difference: 91.6402012307436
At round 20 accuracy: 0.7570621468926554
At round 20 training accuracy: 0.7561019625535754
At round 20 training loss: 1.0321833304914643
gradient difference: 89.46025313827886
At round 21 accuracy: 0.7546408393866021
At round 21 training accuracy: 0.7532596435822242
At round 21 training loss: 0.9550775699561158
gradient difference: 89.7156501785827
At round 22 accuracy: 0.7554479418886199
At round 22 training accuracy: 0.7549289420257163
At round 22 training loss: 0.9535410401435515
gradient difference: 89.78306780769698
At round 23 accuracy: 0.7574656981436643
At round 23 training accuracy: 0.7573652154297316
At round 23 training loss: 0.9693054147573941
gradient difference: 89.9173880723197
At round 24 accuracy: 0.7578692493946732
At round 24 training accuracy: 0.7573652154297316
At round 24 training loss: 0.8835813778438499
gradient difference: 89.69020542716663
At round 25 accuracy: 0.7574656981436643
At round 25 training accuracy: 0.7569140536882473
At round 25 training loss: 0.8994540005861812
gradient difference: 84.40432156241897
At round 26 accuracy: 0.7639225181598063
At round 26 training accuracy: 0.7644935709451839
At round 26 training loss: 0.9666871909441218
gradient difference: 83.8302073958036
At round 27 accuracy: 0.7619047619047619
At round 27 training accuracy: 0.7618768328445747
At round 27 training loss: 0.8330043994167602
gradient difference: 83.72270032847095
At round 28 accuracy: 0.7606941081517352
At round 28 training accuracy: 0.7601624182269343
At round 28 training loss: 0.8272296455491082
gradient difference: 86.41645232929868
At round 29 accuracy: 0.7586763518966909
At round 29 training accuracy: 0.7580870742161064
At round 29 training loss: 0.8237832674492355
gradient difference: 87.9156373557137
At round 30 accuracy: 0.8200161420500404
At round 30 training accuracy: 0.8190841416647868
At round 30 training loss: 0.6633771736009272
gradient difference: 85.22474115709473
At round 31 accuracy: 0.8050847457627118
At round 31 training accuracy: 0.805323708549515
At round 31 training loss: 0.6593108273603941
gradient difference: 83.96809763562746
At round 32 accuracy: 0.7873284907183212
At round 32 training accuracy: 0.7858335213173923
At round 32 training loss: 0.6702594702665461
gradient difference: 83.54518745910578
At round 33 accuracy: 0.781678773204197
At round 33 training accuracy: 0.7818632979923302
At round 33 training loss: 0.6690889188863073
gradient difference: 85.15701836327074
At round 34 accuracy: 0.7917675544794189
At round 34 training accuracy: 0.7912474622152041
At round 34 training loss: 0.6587783974851465
gradient difference: 86.86192104042499
At round 35 accuracy: 0.7861178369652946
At round 35 training accuracy: 0.7865553801037672
At round 35 training loss: 0.6596512387534605
gradient difference: 87.37278217361562
At round 36 accuracy: 0.8297013720742534
At round 36 training accuracy: 0.8291901646740356
At round 36 training loss: 0.5915135347329545
gradient difference: 86.08569694179207
At round 37 accuracy: 0.8292978208232445
At round 37 training accuracy: 0.828919467629145
At round 37 training loss: 0.5849308748399058
gradient difference: 85.48425694881657
At round 38 accuracy: 0.8301049233252623
At round 38 training accuracy: 0.8272050530115046
At round 38 training loss: 0.5848786178719989
gradient difference: 85.82218283373099
At round 39 accuracy: 0.8353510895883777
At round 39 training accuracy: 0.8333859688698398
At round 39 training loss: 0.5750740813047279
gradient difference: 82.56076227566876
At round 40 accuracy: 0.8373688458434221
At round 40 training accuracy: 0.8354613128806677
At round 40 training loss: 0.5721997715826904
gradient difference: 81.68740215025346
At round 41 accuracy: 0.8365617433414043
At round 41 training accuracy: 0.8341980600045116
At round 41 training loss: 0.5699608149430081
gradient difference: 82.24494542202156
At round 42 accuracy: 0.8357546408393866
At round 42 training accuracy: 0.8334310850439882
At round 42 training loss: 0.5704928159135582
gradient difference: 82.75199770627866
At round 43 accuracy: 0.8365617433414043
At round 43 training accuracy: 0.8350101511391834
At round 43 training loss: 0.5684081313372358
gradient difference: 81.73315781960538
At round 44 accuracy: 0.8333333333333334
At round 44 training accuracy: 0.8323031806902774
At round 44 training loss: 0.5715082613005478
gradient difference: 83.82123689820898
At round 45 accuracy: 0.83454398708636
At round 45 training accuracy: 0.832664110083465
At round 45 training loss: 0.5521607558573446
gradient difference: 82.67541331411032
At round 46 accuracy: 0.8317191283292978
At round 46 training accuracy: 0.8275208662305437
At round 46 training loss: 0.5604967494473994
gradient difference: 85.66611583857957
At round 47 accuracy: 0.8393866020984665
At round 47 training accuracy: 0.8374464245431987
At round 47 training loss: 0.5484659907304485
gradient difference: 82.88834540943623
At round 48 accuracy: 0.841000807102502
At round 48 training accuracy: 0.838935258290097
At round 48 training loss: 0.5439639453738154
gradient difference: 80.6050228706365
At round 49 accuracy: 0.8393866020984665
At round 49 training accuracy: 0.836950146627566
At round 49 training loss: 0.543665781100851
gradient difference: 80.07473029725692
At round 50 accuracy: 0.8401937046004843
At round 50 training accuracy: 0.8375366568914956
At round 50 training loss: 0.5363085552031532
gradient difference: 80.55488680348216
At round 51 accuracy: 0.8389830508474576
At round 51 training accuracy: 0.836950146627566
At round 51 training loss: 0.5272382528961256
gradient difference: 79.64218558260545
At round 52 accuracy: 0.8418079096045198
At round 52 training accuracy: 0.8377171215880893
At round 52 training loss: 0.5258644221203068
gradient difference: 79.68850385746921
At round 53 accuracy: 0.8426150121065376
At round 53 training accuracy: 0.8377171215880893
At round 53 training loss: 0.5273277771343853
gradient difference: 79.9480985236273
At round 54 accuracy: 0.8434221146085553
At round 54 training accuracy: 0.8406496729077374
At round 54 training loss: 0.5269492688604975
gradient difference: 79.84713581911224
At round 55 accuracy: 0.844632768361582
At round 55 training accuracy: 0.8415068802165576
At round 55 training loss: 0.509497548992387
gradient difference: 77.53297350999466
At round 56 accuracy: 0.8470540758676351
At round 56 training accuracy: 0.8452515226708776
At round 56 training loss: 0.5056590776645973
gradient difference: 75.61551382192742
At round 57 accuracy: 0.8478611783696529
At round 57 training accuracy: 0.8451161741484322
At round 57 training loss: 0.5054989811511172
gradient difference: 76.9896297647917
At round 58 accuracy: 0.8490718321226796
At round 58 training accuracy: 0.8471012858109632
At round 58 training loss: 0.4975700583585412
gradient difference: 70.53792011923356
At round 59 accuracy: 0.8498789346246973
At round 59 training accuracy: 0.8474170990300023
At round 59 training loss: 0.4926263463831959
gradient difference: 71.22412738831258
At round 60 accuracy: 0.8506860371267151
At round 60 training accuracy: 0.846875704940221
At round 60 training loss: 0.4894426043573309
gradient difference: 72.61103562564743
At round 61 accuracy: 0.8518966908797417
At round 61 training accuracy: 0.846244078502143
At round 61 training loss: 0.48901741234545726
gradient difference: 73.36007691844947
At round 62 accuracy: 0.8531073446327684
At round 62 training accuracy: 0.8472817505075569
At round 62 training loss: 0.4851365226320292
gradient difference: 72.54371209844594
At round 63 accuracy: 0.854317998385795
At round 63 training accuracy: 0.8487705842544552
At round 63 training loss: 0.4811150253901438
gradient difference: 72.79723297483474
At round 64 accuracy: 0.8559322033898306
At round 64 training accuracy: 0.848860816602752
At round 64 training loss: 0.47901155742785745
gradient difference: 72.55444742844581
At round 65 accuracy: 0.8551251008878128
At round 65 training accuracy: 0.8489961651251974
At round 65 training loss: 0.47660939216936465
gradient difference: 71.36190868242967
At round 66 accuracy: 0.8567393058918482
At round 66 training accuracy: 0.8503496503496504
At round 66 training loss: 0.47437037013802225
gradient difference: 70.93500311532958
At round 67 accuracy: 0.8563357546408394
At round 67 training accuracy: 0.8514324385292127
At round 67 training loss: 0.47299172152669927
gradient difference: 70.29538132340278
At round 68 accuracy: 0.8563357546408394
At round 68 training accuracy: 0.852605459057072
At round 68 training loss: 0.4702189830707261
gradient difference: 69.89065548661537
At round 69 accuracy: 0.8583535108958837
At round 69 training accuracy: 0.852199413489736
At round 69 training loss: 0.4704794516170533
gradient difference: 70.8459025987079
At round 70 accuracy: 0.857546408393866
At round 70 training accuracy: 0.8534175501917437
At round 70 training loss: 0.4668979419513397
gradient difference: 70.25143530285146
At round 71 accuracy: 0.8571428571428571
At round 71 training accuracy: 0.85323708549515
At round 71 training loss: 0.46448300862161723
gradient difference: 68.80114740529592
At round 72 accuracy: 0.857546408393866
At round 72 training accuracy: 0.8541845251522671
At round 72 training loss: 0.4631729216129764
gradient difference: 67.80124214917174
At round 73 accuracy: 0.8587570621468926
At round 73 training accuracy: 0.8554928942025716
At round 73 training loss: 0.4609823511170125
gradient difference: 68.00602171636696
At round 74 accuracy: 0.857546408393866
At round 74 training accuracy: 0.8563952176855403
At round 74 training loss: 0.45845960134809666
gradient difference: 65.2137298999771
At round 75 accuracy: 0.8595641646489104
At round 75 training accuracy: 0.8577035867358448
At round 75 training loss: 0.4581285330436043
gradient difference: 64.10857813204322
At round 76 accuracy: 0.860774818401937
At round 76 training accuracy: 0.8581096323031807
At round 76 training loss: 0.4548359640754428
gradient difference: 64.32653221198207
At round 77 accuracy: 0.8615819209039548
At round 77 training accuracy: 0.8582900969997744
At round 77 training loss: 0.45159567940216316
gradient difference: 65.08533983716737
At round 78 accuracy: 0.8615819209039548
At round 78 training accuracy: 0.8586961425671104
At round 78 training loss: 0.450461026166326
gradient difference: 65.48498957923758
At round 79 accuracy: 0.8627925746569814
At round 79 training accuracy: 0.8584705616963682
At round 79 training loss: 0.4494881604551704
gradient difference: 66.71991583299089
At round 80 accuracy: 0.8623890234059726
At round 80 training accuracy: 0.8581547484773291
At round 80 training loss: 0.44923890526020954
gradient difference: 67.55943538488
At round 81 accuracy: 0.8535108958837773
At round 81 training accuracy: 0.8548161515903452
At round 81 training loss: 0.4500649429099731
gradient difference: 66.9346763943972
At round 82 accuracy: 0.8535108958837773
At round 82 training accuracy: 0.8549966162869389
At round 82 training loss: 0.4486673956144052
gradient difference: 65.87249922280111
At round 83 accuracy: 0.8539144471347861
At round 83 training accuracy: 0.8555831265508684
At round 83 training loss: 0.4472070967918482
gradient difference: 65.43346022114132
At round 84 accuracy: 0.8535108958837773
At round 84 training accuracy: 0.8571621926460636
At round 84 training loss: 0.4460213206723558
gradient difference: 66.29507616927985
At round 85 accuracy: 0.854317998385795
At round 85 training accuracy: 0.8576584705616964
At round 85 training loss: 0.4448493496702886
gradient difference: 65.68948800265463
At round 86 accuracy: 0.8555286521388217
At round 86 training accuracy: 0.857207308820212
At round 86 training loss: 0.4448391534574624
gradient difference: 66.82479362943121
At round 87 accuracy: 0.857546408393866
At round 87 training accuracy: 0.8567561470787277
At round 87 training loss: 0.44552024890896214
gradient difference: 67.70122313429674
At round 88 accuracy: 0.864406779661017
At round 88 training accuracy: 0.8609068351003835
At round 88 training loss: 0.4412862434569799
gradient difference: 67.0730177874129
At round 89 accuracy: 0.864406779661017
At round 89 training accuracy: 0.8608166027520866
At round 89 training loss: 0.4415758607099402
gradient difference: 67.67769177994467
At round 90 accuracy: 0.8652138821630347
At round 90 training accuracy: 0.8607714865779382
At round 90 training loss: 0.4421146515722391
gradient difference: 68.14745244069877
At round 91 accuracy: 0.8652138821630347
At round 91 training accuracy: 0.8605007895330476
At round 91 training loss: 0.44260666095965767
gradient difference: 68.6013022619189
At round 92 accuracy: 0.8640032284100081
At round 92 training accuracy: 0.8601398601398601
At round 92 training loss: 0.43900450985822403
gradient difference: 67.42091429153866
At round 93 accuracy: 0.864406779661017
At round 93 training accuracy: 0.8598240469208212
At round 93 training loss: 0.4395673767199453
gradient difference: 67.81447189538194
At round 94 accuracy: 0.864406779661017
At round 94 training accuracy: 0.8598691630949695
At round 94 training loss: 0.43997824910146616
gradient difference: 68.2544377949154
At round 95 accuracy: 0.864406779661017
At round 95 training accuracy: 0.8600947439657117
At round 95 training loss: 0.44104513678940915
gradient difference: 68.60041595193198
At round 96 accuracy: 0.8656174334140436
At round 96 training accuracy: 0.8606361380554929
At round 96 training loss: 0.4381521766499851
gradient difference: 68.19061623462812
At round 97 accuracy: 0.8664245359160614
At round 97 training accuracy: 0.8615384615384616
At round 97 training loss: 0.4366986251226295
gradient difference: 67.75250359454782
At round 98 accuracy: 0.8660209846650525
At round 98 training accuracy: 0.8613579968418678
At round 98 training loss: 0.43640084655242395
gradient difference: 67.83005041264913
At round 99 accuracy: 0.8668280871670703
At round 99 training accuracy: 0.8614933453643131
At round 99 training loss: 0.4383423708434732
gradient difference: 68.46822518838236
At round 100 accuracy: 0.8668280871670703
At round 100 training accuracy: 0.86158357771261
At round 100 training loss: 0.43592053296339855
gradient difference: 68.13987815027602
At round 101 accuracy: 0.8656174334140436
At round 101 training accuracy: 0.8639747349424769
At round 101 training loss: 0.42991458012352884
gradient difference: 65.14950657404373
At round 102 accuracy: 0.8664245359160614
At round 102 training accuracy: 0.8646965937288518
At round 102 training loss: 0.42883814140908905
gradient difference: 64.64370913862348
At round 103 accuracy: 0.8656174334140436
At round 103 training accuracy: 0.8646965937288518
At round 103 training loss: 0.42706421289450963
gradient difference: 64.51038027549124
At round 104 accuracy: 0.867231638418079
At round 104 training accuracy: 0.864516129032258
At round 104 training loss: 0.4247286099223716
gradient difference: 64.15838370692225
At round 105 accuracy: 0.8656174334140436
At round 105 training accuracy: 0.8649672907737423
At round 105 training loss: 0.4235471932792701
gradient difference: 63.86152801463133
At round 106 accuracy: 0.8696529459241323
At round 106 training accuracy: 0.8667268215655313
At round 106 training loss: 0.4229476629780131
gradient difference: 62.858302962867825
At round 107 accuracy: 0.8692493946731235
At round 107 training accuracy: 0.8670877509587187
At round 107 training loss: 0.41467060560489544
gradient difference: 60.503006281583
At round 108 accuracy: 0.867231638418079
At round 108 training accuracy: 0.8645612452064065
At round 108 training loss: 0.41517504293117785
gradient difference: 60.16620210806992
At round 109 accuracy: 0.8676351896690879
At round 109 training accuracy: 0.8640649672907738
At round 109 training loss: 0.416058767512155
gradient difference: 60.67381245373771
At round 110 accuracy: 0.8676351896690879
At round 110 training accuracy: 0.8647868260771486
At round 110 training loss: 0.4108325696679934
gradient difference: 59.439384867105204
At round 111 accuracy: 0.8680387409200968
At round 111 training accuracy: 0.8663658921723438
At round 111 training loss: 0.4104839007166178
gradient difference: 56.438300969202075
At round 112 accuracy: 0.867231638418079
At round 112 training accuracy: 0.8661403113016016
At round 112 training loss: 0.4080053249792108
gradient difference: 57.26567340140169
At round 113 accuracy: 0.8680387409200968
At round 113 training accuracy: 0.8669975186104218
At round 113 training loss: 0.40673725696508756
gradient difference: 56.98805250159534
At round 114 accuracy: 0.8724778046811945
At round 114 training accuracy: 0.8700203022783668
At round 114 training loss: 0.4063655536390712
gradient difference: 54.96638751428064
At round 115 accuracy: 0.8736884584342212
At round 115 training accuracy: 0.8692082111436951
At round 115 training loss: 0.40264376269427804
gradient difference: 56.25290822809672
At round 116 accuracy: 0.8728813559322034
At round 116 training accuracy: 0.8686217008797654
At round 116 training loss: 0.4009097867196364
gradient difference: 57.969388116117536
At round 117 accuracy: 0.8720742534301856
At round 117 training accuracy: 0.8683510038348748
At round 117 training loss: 0.4011669372469557
gradient difference: 58.43771121055091
At round 118 accuracy: 0.8712671509281679
At round 118 training accuracy: 0.8677644935709452
At round 118 training loss: 0.40085820745270867
gradient difference: 59.24178982034048
At round 119 accuracy: 0.8716707021791767
At round 119 training accuracy: 0.8676742612226483
At round 119 training loss: 0.4011790298196066
gradient difference: 59.81944110752803
At round 120 accuracy: 0.8732849071832123
At round 120 training accuracy: 0.8690728626212497
At round 120 training loss: 0.3992409808907852
gradient difference: 58.16849827558999
At round 121 accuracy: 0.8728813559322034
At round 121 training accuracy: 0.8695691405368825
At round 121 training loss: 0.39899380445050087
gradient difference: 57.93813580917298
At round 122 accuracy: 0.8720742534301856
At round 122 training accuracy: 0.8697947214076246
At round 122 training loss: 0.3970466005383322
gradient difference: 57.8878337223116
At round 123 accuracy: 0.8761097659402745
At round 123 training accuracy: 0.8729979697721634
At round 123 training loss: 0.39514435255955066
gradient difference: 57.4644949056365
At round 124 accuracy: 0.8761097659402745
At round 124 training accuracy: 0.872546808030679
At round 124 training loss: 0.3948990048793162
gradient difference: 58.055488620967786
At round 125 accuracy: 0.8757062146892656
At round 125 training accuracy: 0.8731333182946086
At round 125 training loss: 0.3931816501926548
gradient difference: 56.02213569464609
At round 126 accuracy: 0.8761097659402745
At round 126 training accuracy: 0.8735393638619445
At round 126 training loss: 0.39232752763260953
gradient difference: 55.77614938905127
At round 127 accuracy: 0.8773204196933011
At round 127 training accuracy: 0.8746672682156553
At round 127 training loss: 0.3896773668940116
gradient difference: 54.93740324617765
At round 128 accuracy: 0.8765133171912833
At round 128 training accuracy: 0.874216106474171
At round 128 training loss: 0.3893185418163872
gradient difference: 55.24422296476064
At round 129 accuracy: 0.8761097659402745
At round 129 training accuracy: 0.8718700654184525
At round 129 training loss: 0.38951049745150085
gradient difference: 55.06438490678463
At round 130 accuracy: 0.8744955609362389
At round 130 training accuracy: 0.8707421610647417
At round 130 training loss: 0.38955551238683067
gradient difference: 56.22747426825141
At round 131 accuracy: 0.87409200968523
At round 131 training accuracy: 0.8700203022783668
At round 131 training loss: 0.3908664019882155
gradient difference: 57.776690777461646
At round 132 accuracy: 0.87409200968523
At round 132 training accuracy: 0.870245883149109
At round 132 training loss: 0.388479552141517
gradient difference: 56.538399832204746
At round 133 accuracy: 0.8753026634382567
At round 133 training accuracy: 0.8715542521994135
At round 133 training loss: 0.38545790718414163
gradient difference: 54.64173399665733
At round 134 accuracy: 0.8765133171912833
At round 134 training accuracy: 0.8717798330701556
At round 134 training loss: 0.38524344556357987
gradient difference: 54.54986239211383
At round 135 accuracy: 0.8773204196933011
At round 135 training accuracy: 0.8724114595082337
At round 135 training loss: 0.38326233214714067
gradient difference: 53.13040035302492
At round 136 accuracy: 0.8785310734463276
At round 136 training accuracy: 0.8750733137829912
At round 136 training loss: 0.38139056087098216
gradient difference: 53.00396223931856
At round 137 accuracy: 0.8789346246973365
At round 137 training accuracy: 0.8755695916986239
At round 137 training loss: 0.3810500188993058
gradient difference: 52.62097837182312
At round 138 accuracy: 0.8777239709443099
At round 138 training accuracy: 0.8746672682156553
At round 138 training loss: 0.38076349504841606
gradient difference: 52.41547321488954
At round 139 accuracy: 0.8773204196933011
At round 139 training accuracy: 0.8748026167381006
At round 139 training loss: 0.3797083973222383
gradient difference: 52.39907396652941
At round 140 accuracy: 0.8781275221953188
At round 140 training accuracy: 0.8756147078727724
At round 140 training loss: 0.37811908621888063
gradient difference: 51.32684317870338
At round 141 accuracy: 0.8781275221953188
At round 141 training accuracy: 0.8759756372659598
At round 141 training loss: 0.37810605765565536
gradient difference: 51.912220131965825
At round 142 accuracy: 0.8773204196933011
At round 142 training accuracy: 0.8746221520415068
At round 142 training loss: 0.37708858376280224
gradient difference: 53.201452789162445
At round 143 accuracy: 0.8769168684422922
At round 143 training accuracy: 0.8735393638619445
At round 143 training loss: 0.37668121640112256
gradient difference: 53.02467801014156
At round 144 accuracy: 0.8765133171912833
At round 144 training accuracy: 0.8736747123843898
At round 144 training loss: 0.37380293773328804
gradient difference: 51.850339716372396
At round 145 accuracy: 0.8761097659402745
At round 145 training accuracy: 0.8738100609068351
At round 145 training loss: 0.3731114528120277
gradient difference: 52.02321281857998
At round 146 accuracy: 0.8765133171912833
At round 146 training accuracy: 0.8736295962102414
At round 146 training loss: 0.3727960842173524
gradient difference: 52.103548699577075
At round 147 accuracy: 0.8765133171912833
At round 147 training accuracy: 0.8742612226483194
At round 147 training loss: 0.37230575138317684
gradient difference: 52.08652478187073
At round 148 accuracy: 0.8769168684422922
At round 148 training accuracy: 0.8740356417775773
At round 148 training loss: 0.37188478260728164
gradient difference: 52.03835624999424
At round 149 accuracy: 0.8769168684422922
At round 149 training accuracy: 0.8740356417775773
At round 149 training loss: 0.3709602837086719
gradient difference: 51.98071347022769
At round 150 accuracy: 0.8773204196933011
At round 150 training accuracy: 0.8743965711707647
At round 150 training loss: 0.3702735054255634
gradient difference: 51.56964146677229
At round 151 accuracy: 0.8777239709443099
At round 151 training accuracy: 0.87453191969321
At round 151 training loss: 0.3695150481002099
gradient difference: 50.89131309947596
At round 152 accuracy: 0.8793381759483454
At round 152 training accuracy: 0.8767877284006316
At round 152 training loss: 0.36818018961229737
gradient difference: 50.79096516096074
At round 153 accuracy: 0.8793381759483454
At round 153 training accuracy: 0.8768328445747801
At round 153 training loss: 0.36780958905271527
gradient difference: 50.95806119040158
At round 154 accuracy: 0.8785310734463276
At round 154 training accuracy: 0.8764267990074441
At round 154 training loss: 0.3677239159856159
gradient difference: 52.228545395547066
At round 155 accuracy: 0.8797417271993543
At round 155 training accuracy: 0.8774193548387097
At round 155 training loss: 0.36642510128120737
gradient difference: 51.679467829176666
At round 156 accuracy: 0.8817594834543987
At round 156 training accuracy: 0.8779156327543425
At round 156 training loss: 0.365710011748826
gradient difference: 50.23544465401615
At round 157 accuracy: 0.8813559322033898
At round 157 training accuracy: 0.8819309722535529
At round 157 training loss: 0.3658743796083961
gradient difference: 49.81942372406138
At round 158 accuracy: 0.8797417271993543
At round 158 training accuracy: 0.8823370178208888
At round 158 training loss: 0.3658776204321284
gradient difference: 50.03702688422287
At round 159 accuracy: 0.8853914447134786
At round 159 training accuracy: 0.8832393413038574
At round 159 training loss: 0.3648127203419411
gradient difference: 49.86336640783514
At round 160 accuracy: 0.8825665859564165
At round 160 training accuracy: 0.8827881795623731
At round 160 training loss: 0.364996923855175
gradient difference: 50.70749817159669
At round 161 accuracy: 0.8833736884584342
At round 161 training accuracy: 0.8831491089555605
At round 161 training loss: 0.3646996367116854
gradient difference: 49.45116604750759
At round 162 accuracy: 0.8837772397094431
At round 162 training accuracy: 0.8825174825174825
At round 162 training loss: 0.3619566674304955
gradient difference: 50.74138794266814
At round 163 accuracy: 0.884180790960452
At round 163 training accuracy: 0.8824272501691857
At round 163 training loss: 0.3612294412599765
gradient difference: 50.652479747415434
At round 164 accuracy: 0.8837772397094431
At round 164 training accuracy: 0.8821565531242951
At round 164 training loss: 0.36161425792986723
gradient difference: 52.12972248497374
At round 165 accuracy: 0.8866020984665053
At round 165 training accuracy: 0.8832393413038574
At round 165 training loss: 0.36149086726910507
gradient difference: 51.62491914364387
At round 166 accuracy: 0.8809523809523809
At round 166 training accuracy: 0.8783216783216783
At round 166 training loss: 0.36024336274112423
gradient difference: 51.31383383965922
At round 167 accuracy: 0.880548829701372
At round 167 training accuracy: 0.8772840063162644
At round 167 training loss: 0.3613832581938725
gradient difference: 52.713508398498064
At round 168 accuracy: 0.880548829701372
At round 168 training accuracy: 0.8772840063162644
At round 168 training loss: 0.36135673827796116
gradient difference: 52.830278135415966
At round 169 accuracy: 0.880548829701372
At round 169 training accuracy: 0.8768779607489285
At round 169 training loss: 0.36272693056652705
gradient difference: 54.09026023488027
At round 170 accuracy: 0.8809523809523809
At round 170 training accuracy: 0.8770584254455223
At round 170 training loss: 0.36107316625584146
gradient difference: 52.60627089252729
At round 171 accuracy: 0.8821630347054076
At round 171 training accuracy: 0.8790886532822016
At round 171 training loss: 0.36051234555061584
gradient difference: 52.2640182198942
At round 172 accuracy: 0.8829701372074253
At round 172 training accuracy: 0.879224001804647
At round 172 training loss: 0.35924565698770805
gradient difference: 50.9796816977654
At round 173 accuracy: 0.8821630347054076
At round 173 training accuracy: 0.8798105120685766
At round 173 training loss: 0.35904897059224244
gradient difference: 50.78711066919246
At round 174 accuracy: 0.8825665859564165
At round 174 training accuracy: 0.8802616738100609
At round 174 training loss: 0.35855612954943733
gradient difference: 50.952015801475504
At round 175 accuracy: 0.8849878934624698
At round 175 training accuracy: 0.884457478005865
At round 175 training loss: 0.3566341305137716
gradient difference: 49.89295488221459
At round 176 accuracy: 0.8853914447134786
At round 176 training accuracy: 0.8847281750507557
At round 176 training loss: 0.35548196902699025
gradient difference: 48.99197824195678
At round 177 accuracy: 0.8833736884584342
At round 177 training accuracy: 0.881840739905256
At round 177 training loss: 0.3546342400602447
gradient difference: 48.8051885793431
At round 178 accuracy: 0.8833736884584342
At round 178 training accuracy: 0.8811188811188811
At round 178 training loss: 0.3544272347441006
gradient difference: 48.68918426070683
At round 179 accuracy: 0.8829701372074253
At round 179 training accuracy: 0.8813895781637717
At round 179 training loss: 0.3543366077442807
gradient difference: 49.36383010483321
At round 180 accuracy: 0.8829701372074253
At round 180 training accuracy: 0.8814798105120686
At round 180 training loss: 0.354135636695884
gradient difference: 49.54328936843615
At round 181 accuracy: 0.8821630347054076
At round 181 training accuracy: 0.8816602752086623
At round 181 training loss: 0.35359113910717616
gradient difference: 49.060414525796126
At round 182 accuracy: 0.8837772397094431
At round 182 training accuracy: 0.8829235280848184
At round 182 training loss: 0.3521677947593292
gradient difference: 46.99164887459711
At round 183 accuracy: 0.8833736884584342
At round 183 training accuracy: 0.8826528310399279
At round 183 training loss: 0.3518521121083117
gradient difference: 47.04288976604304
At round 184 accuracy: 0.8861985472154964
At round 184 training accuracy: 0.8835551545228965
At round 184 training loss: 0.3517647780331779
gradient difference: 45.439100663797724
At round 185 accuracy: 0.8866020984665053
At round 185 training accuracy: 0.8828332957365216
At round 185 training loss: 0.351070124553943
gradient difference: 46.338355077651826
At round 186 accuracy: 0.8878127522195319
At round 186 training accuracy: 0.8837807353936387
At round 186 training loss: 0.35033037017114055
gradient difference: 44.515928584220106
At round 187 accuracy: 0.887409200968523
At round 187 training accuracy: 0.8836905030453418
At round 187 training loss: 0.3496826847677787
gradient difference: 44.22739989117027
At round 188 accuracy: 0.8946731234866828
At round 188 training accuracy: 0.8891044439431536
At round 188 training loss: 0.3491247488545855
gradient difference: 43.82189582393661
At round 189 accuracy: 0.8946731234866828
At round 189 training accuracy: 0.8897360703812317
At round 189 training loss: 0.3483609919092361
gradient difference: 43.330410251504865
At round 190 accuracy: 0.8934624697336562
At round 190 training accuracy: 0.8891495601173021
At round 190 training loss: 0.3471327013008594
gradient difference: 44.121473673786525
At round 191 accuracy: 0.8926553672316384
At round 191 training accuracy: 0.8890593277690052
At round 191 training loss: 0.3466942210830269
gradient difference: 44.11782790474183
At round 192 accuracy: 0.8922518159806295
At round 192 training accuracy: 0.8890142115948567
At round 192 training loss: 0.3448102109964467
gradient difference: 42.01944890589193
At round 193 accuracy: 0.893866020984665
At round 193 training accuracy: 0.8900969997744191
At round 193 training loss: 0.3449165743712429
gradient difference: 41.039181300010135
At round 194 accuracy: 0.8942695722356739
At round 194 training accuracy: 0.8900969997744191
At round 194 training loss: 0.3445763897013777
gradient difference: 41.07523777533004
At round 195 accuracy: 0.8930589184826473
At round 195 training accuracy: 0.8894202571621926
At round 195 training loss: 0.34307236302431404
gradient difference: 42.090591223631584
At round 196 accuracy: 0.8918482647296206
At round 196 training accuracy: 0.8897360703812317
At round 196 training loss: 0.3424408138648748
gradient difference: 43.30696191775749
At round 197 accuracy: 0.8914447134786118
At round 197 training accuracy: 0.8906835100383488
At round 197 training loss: 0.3424514012651158
gradient difference: 43.51380406895302
At round 198 accuracy: 0.8930589184826473
At round 198 training accuracy: 0.8905481615159034
At round 198 training loss: 0.3427136764950511
gradient difference: 44.76776517712191
At round 199 accuracy: 0.8918482647296206
At round 199 training accuracy: 0.8899616512519738
At round 199 training loss: 0.34333476156547593
gradient difference: 45.80949799618952
At round 200 accuracy: 0.8914447134786118
At round 200 training accuracy: 0.890187232122716
