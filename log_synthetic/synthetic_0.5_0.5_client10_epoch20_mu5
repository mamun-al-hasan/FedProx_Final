Arguments:
	       batch_size : 10
	clients_per_round : 10
	          dataset : synthetic_0.5_0.5
	     drop_percent : 0.0
	       eval_every : 1
	    learning_rate : 0.01
	            model : mclr
	     model_params : (10,)
	               mu : 5.0
	       num_epochs : 20
	        num_iters : 1
	       num_rounds : 200
	        optimizer : fedprox
	             seed : 0
Using Federated prox to Train

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/3.64k flops)
  dense/kernel/Initializer/stateless_random_uniform (600/1.20k flops)
    dense/kernel/Initializer/stateless_random_uniform/mul (600/600 flops)
    dense/kernel/Initializer/stateless_random_uniform/sub (1/1 flops)
  PGD/update_dense/kernel/add (600/600 flops)
  PGD/update_dense/kernel/mul (600/600 flops)
  PGD/update_dense/kernel/mul_1 (600/600 flops)
  PGD/update_dense/kernel/sub (600/600 flops)
  PGD/update_dense/bias/add (10/10 flops)
  PGD/update_dense/bias/mul (10/10 flops)
  PGD/update_dense/bias/mul_1 (10/10 flops)
  PGD/update_dense/bias/sub (10/10 flops)
  gradients/sparse_softmax_cross_entropy_loss/value_grad/Neg (1/1 flops)
  gradients/sparse_softmax_cross_entropy_loss/value_grad/mul (1/1 flops)
  sparse_softmax_cross_entropy_loss/num_present/Equal (1/1 flops)

======================End of Report==========================
30 Clients in Total
Training with 10 workers ---
At round 0 accuracy: 0.04693733865289838
At round 0 training accuracy: 0.04931162644610794
At round 0 training loss: 1.7335920322068055
gradient difference: 143.68560131796275
At round 1 accuracy: 0.041070171321286084
At round 1 training accuracy: 0.042087630215149455
At round 1 training loss: 1.8311730181382089
gradient difference: 148.41551854461608
At round 2 accuracy: 0.05609011969021357
At round 2 training accuracy: 0.05643092707951631
At round 2 training loss: 1.9071817208045017
gradient difference: 151.36031438355568
At round 3 accuracy: 0.9199718375968082
At round 3 training accuracy: 0.9228655185049469
At round 3 training loss: 0.3560310293556302
gradient difference: 60.883230883160344
At round 4 accuracy: 0.9244308847688336
At round 4 training accuracy: 0.9272365597026645
At round 4 training loss: 0.34457647447866124
gradient difference: 59.39102092212762
At round 5 accuracy: 0.9249002581553626
At round 5 training accuracy: 0.9275768203947024
At round 5 training loss: 0.3526596359583693
gradient difference: 57.91819738714893
At round 6 accuracy: 0.9244308847688336
At round 6 training accuracy: 0.9271842119038894
At round 6 training loss: 0.25240049466692777
gradient difference: 56.052984982834424
At round 7 accuracy: 0.9281858718610655
At round 7 training accuracy: 0.930665340522431
At round 7 training loss: 0.24536533426353582
gradient difference: 54.34965903731145
At round 8 accuracy: 0.9354611593522647
At round 8 training accuracy: 0.9376537716589017
At round 8 training loss: 0.24079118904255015
gradient difference: 53.04469254655328
At round 9 accuracy: 0.9371039662051162
At round 9 training accuracy: 0.9384389886405277
At round 9 training loss: 0.2341310941591638
gradient difference: 51.97435436344804
At round 10 accuracy: 0.9359305327387937
At round 10 training accuracy: 0.9376275977595142
At round 10 training loss: 0.21187221147166801
gradient difference: 52.15104835641753
At round 11 accuracy: 0.9389814597512322
At round 11 training accuracy: 0.9412919436737686
At round 11 training loss: 0.20495559250403436
gradient difference: 50.94115693557346
At round 12 accuracy: 0.9392161464444966
At round 12 training accuracy: 0.9419201172590692
At round 12 training loss: 0.1998800559368468
gradient difference: 50.13034380295307
At round 13 accuracy: 0.9389814597512322
At round 13 training accuracy: 0.941710726063969
At round 13 training loss: 0.19142962079482514
gradient difference: 50.37337653481972
At round 14 accuracy: 0.9420323867636705
At round 14 training accuracy: 0.9447468983929226
At round 14 training loss: 0.1894153313743365
gradient difference: 48.492239796221234
At round 15 accuracy: 0.942267073456935
At round 15 training accuracy: 0.9445113332984348
At round 15 training loss: 0.1874171487704471
gradient difference: 47.831890330507356
At round 16 accuracy: 0.942736446843464
At round 16 training accuracy: 0.9454274197769984
At round 16 training loss: 0.18614011999873528
gradient difference: 47.69761329006929
At round 17 accuracy: 0.9448486270828444
At round 17 training accuracy: 0.9464482018531121
At round 17 training loss: 0.17929224287936518
gradient difference: 46.27211980331241
At round 18 accuracy: 0.9443792536963154
At round 18 training accuracy: 0.9459247238653614
At round 18 training loss: 0.17305931573996855
gradient difference: 46.44796419783218
At round 19 accuracy: 0.9448486270828444
At round 19 training accuracy: 0.9463173323561744
At round 19 training loss: 0.16768644309471972
gradient difference: 45.29656295371028
At round 20 accuracy: 0.9446139403895799
At round 20 training accuracy: 0.9460817672616867
At round 20 training loss: 0.16467429457411473
gradient difference: 45.439648394210444
At round 21 accuracy: 0.9443792536963154
At round 21 training accuracy: 0.9457676804690363
At round 21 training loss: 0.16277012897174958
gradient difference: 45.537810783195916
At round 22 accuracy: 0.9443792536963154
At round 22 training accuracy: 0.9456106370727111
At round 22 training loss: 0.16173978849153123
gradient difference: 45.63915647298238
At round 23 accuracy: 0.9443792536963154
At round 23 training accuracy: 0.9455321153745485
At round 23 training loss: 0.16099776414621372
gradient difference: 45.6977291176826
At round 24 accuracy: 0.9462567472424314
At round 24 training accuracy: 0.9483065487096267
At round 24 training loss: 0.15208534690057254
gradient difference: 44.24833118689435
At round 25 accuracy: 0.9511851678009857
At round 25 training accuracy: 0.9524420248128567
At round 25 training loss: 0.14757353092885975
gradient difference: 43.26807406357965
At round 26 accuracy: 0.9535320347336306
At round 26 training accuracy: 0.9550594147516097
At round 26 training loss: 0.14465734621381496
gradient difference: 42.669207258551744
At round 27 accuracy: 0.9544707815066886
At round 27 training accuracy: 0.9557922839344606
At round 27 training loss: 0.142721346264891
gradient difference: 42.31406223028776
At round 28 accuracy: 0.9544707815066886
At round 28 training accuracy: 0.9554520232424226
At round 28 training loss: 0.14261510426975485
gradient difference: 41.79268806176658
At round 29 accuracy: 0.9544707815066886
At round 29 training accuracy: 0.9551641103491598
At round 29 training loss: 0.14142734769198698
gradient difference: 41.84103547319767
At round 30 accuracy: 0.9542360948134241
At round 30 training accuracy: 0.9551379364497723
At round 30 training loss: 0.140623732935686
gradient difference: 41.89066790833475
At round 31 accuracy: 0.9563482750528045
At round 31 training accuracy: 0.9567083704130241
At round 31 training loss: 0.13514571300146272
gradient difference: 40.58038354085574
At round 32 accuracy: 0.95611358835954
At round 32 training accuracy: 0.9579647175836256
At round 32 training loss: 0.13213329669171006
gradient difference: 39.95711282175378
At round 33 accuracy: 0.9584604552921849
At round 33 training accuracy: 0.9605297597236037
At round 33 training loss: 0.12868706524247878
gradient difference: 38.310684316930754
At round 34 accuracy: 0.9610420089180943
At round 34 training accuracy: 0.9618384546929801
At round 34 training loss: 0.12682086118577737
gradient difference: 37.72574584601313
At round 35 accuracy: 0.9622154423844168
At round 35 training accuracy: 0.9630686279641941
At round 35 training loss: 0.12459188339366806
gradient difference: 36.537975044699145
At round 36 accuracy: 0.9629195024642103
At round 36 training accuracy: 0.9634350625556195
At round 36 training loss: 0.12208570136664225
gradient difference: 36.00748153964412
At round 37 accuracy: 0.9629195024642103
At round 37 training accuracy: 0.9634350625556195
At round 37 training loss: 0.1207831197699868
gradient difference: 36.02850221975486
At round 38 accuracy: 0.9638582492372683
At round 38 training accuracy: 0.9643511490341831
At round 38 training loss: 0.11887746233587085
gradient difference: 35.744971744049934
At round 39 accuracy: 0.9647969960103262
At round 39 training accuracy: 0.9653981050096844
At round 39 training loss: 0.11746328559567179
gradient difference: 35.31830885004494
At round 40 accuracy: 0.9650316827035907
At round 40 training accuracy: 0.9654242789090719
At round 40 training loss: 0.11650652505495623
gradient difference: 35.34525348455517
At round 41 accuracy: 0.9652663693968552
At round 41 training accuracy: 0.9661048002931477
At round 41 training loss: 0.11531710107831028
gradient difference: 35.122810131618785
At round 42 accuracy: 0.9652663693968552
At round 42 training accuracy: 0.9659477568968224
At round 42 training loss: 0.11404012568823091
gradient difference: 35.03580402349293
At round 43 accuracy: 0.9652663693968552
At round 43 training accuracy: 0.96597393079621
At round 43 training loss: 0.11325043617936738
gradient difference: 35.04426545466437
At round 44 accuracy: 0.9652663693968552
At round 44 training accuracy: 0.96597393079621
At round 44 training loss: 0.11223717533017451
gradient difference: 34.63973679031309
At round 45 accuracy: 0.9652663693968552
At round 45 training accuracy: 0.9659477568968224
At round 45 training loss: 0.11152717746670489
gradient difference: 34.65802658013775
At round 46 accuracy: 0.9645623093170618
At round 46 training accuracy: 0.9652148877139717
At round 46 training loss: 0.1113113121603741
gradient difference: 34.63045312267047
At round 47 accuracy: 0.9643276226237972
At round 47 training accuracy: 0.9652934094121343
At round 47 training loss: 0.10929876161398058
gradient difference: 34.19816897641638
At round 48 accuracy: 0.9652663693968552
At round 48 training accuracy: 0.9661048002931477
At round 48 training loss: 0.10787952136713354
gradient difference: 33.87422319343401
At round 49 accuracy: 0.9657357427833841
At round 49 training accuracy: 0.9658168873998848
At round 49 training loss: 0.10726212710805172
gradient difference: 33.880882073827635
At round 50 accuracy: 0.9659704294766487
At round 50 training accuracy: 0.9657645396011098
At round 50 training loss: 0.10667666853415529
gradient difference: 33.82873674394734
At round 51 accuracy: 0.9676132363295001
At round 51 training accuracy: 0.9677799298539497
At round 51 training loss: 0.10399280267780332
gradient difference: 32.334182580895146
At round 52 accuracy: 0.9678479230227646
At round 52 training accuracy: 0.9678584515521123
At round 52 training loss: 0.10340831915354629
gradient difference: 32.333625960622946
At round 53 accuracy: 0.9680826097160291
At round 53 training accuracy: 0.9680678427472125
At round 53 training loss: 0.10184453425505094
gradient difference: 31.92301616017084
At round 54 accuracy: 0.9680826097160291
At round 54 training accuracy: 0.9682248861435376
At round 54 training loss: 0.1012569392701926
gradient difference: 31.929418817570323
At round 55 accuracy: 0.9683172964092936
At round 55 training accuracy: 0.9691147987227137
At round 55 training loss: 0.10016935438035496
gradient difference: 31.605912153674126
At round 56 accuracy: 0.9678479230227646
At round 56 training accuracy: 0.9694027116159766
At round 56 training loss: 0.09861738843464922
gradient difference: 30.925542984146468
At round 57 accuracy: 0.9692560431823516
At round 57 training accuracy: 0.9708684499816783
At round 57 training loss: 0.0976676794525595
gradient difference: 30.390692001090876
At round 58 accuracy: 0.9692560431823516
At round 58 training accuracy: 0.9706067109878029
At round 58 training loss: 0.09747160071355865
gradient difference: 30.30093584395842
At round 59 accuracy: 0.9704294766486741
At round 59 training accuracy: 0.9720462754541171
At round 59 training loss: 0.09582460869204296
gradient difference: 29.61118033763702
At round 60 accuracy: 0.9701947899554095
At round 60 training accuracy: 0.9722294927498298
At round 60 training loss: 0.09534958910642327
gradient difference: 29.53084499609821
At round 61 accuracy: 0.9711335367284675
At round 61 training accuracy: 0.9727006229388054
At round 61 training loss: 0.09460173117642175
gradient difference: 29.274463993980124
At round 62 accuracy: 0.971837596808261
At round 62 training accuracy: 0.9734858399204314
At round 62 training loss: 0.09365964963037034
gradient difference: 28.78084410139637
At round 63 accuracy: 0.971837596808261
At round 63 training accuracy: 0.9735120138198189
At round 63 training loss: 0.0930212806216867
gradient difference: 28.778960455617764
At round 64 accuracy: 0.9725416568880545
At round 64 training accuracy: 0.9737475789143066
At round 64 training loss: 0.09237641301166476
gradient difference: 28.54948255989856
At round 65 accuracy: 0.9734804036611124
At round 65 training accuracy: 0.9747160131916452
At round 65 training loss: 0.09140339364154697
gradient difference: 27.926473820777264
At round 66 accuracy: 0.9732457169678479
At round 66 training accuracy: 0.9752656650787834
At round 66 training loss: 0.09028553111741561
gradient difference: 27.459358440982854
At round 67 accuracy: 0.9730110302745835
At round 67 training accuracy: 0.9756059257708214
At round 67 training loss: 0.08906039649602418
gradient difference: 27.177699974698513
At round 68 accuracy: 0.9725416568880545
At round 68 training accuracy: 0.9756320996702089
At round 68 training loss: 0.08798540504849656
gradient difference: 26.69461941972672
At round 69 accuracy: 0.9727763435813189
At round 69 training accuracy: 0.9755797518714338
At round 69 training loss: 0.08748605982537716
gradient difference: 26.55133565638705
At round 70 accuracy: 0.9730110302745835
At round 70 training accuracy: 0.9758676647646967
At round 70 training loss: 0.08699651736255562
gradient difference: 26.351276699325012
At round 71 accuracy: 0.9732457169678479
At round 71 training accuracy: 0.9758414908653091
At round 71 training loss: 0.08675490947997412
gradient difference: 26.220045929256067
At round 72 accuracy: 0.9730110302745835
At round 72 training accuracy: 0.9762079254567345
At round 72 training loss: 0.08628522180789541
gradient difference: 26.133448881848526
At round 73 accuracy: 0.9739497770476414
At round 73 training accuracy: 0.9767314034444852
At round 73 training loss: 0.08565764149840287
gradient difference: 25.917588597201874
At round 74 accuracy: 0.9737150903543769
At round 74 training accuracy: 0.9767052295450976
At round 74 training loss: 0.08503280864384087
gradient difference: 25.911075660636964
At round 75 accuracy: 0.9741844637409058
At round 75 training accuracy: 0.9765481861487725
At round 75 training loss: 0.0843820269074538
gradient difference: 25.5193024574458
At round 76 accuracy: 0.9744191504341704
At round 76 training accuracy: 0.9768622729414228
At round 76 training loss: 0.08406131122385996
gradient difference: 25.37834259174418
At round 77 accuracy: 0.9748885238206993
At round 77 training accuracy: 0.9769407946395854
At round 77 training loss: 0.08374498175806287
gradient difference: 25.16005546307345
At round 78 accuracy: 0.9744191504341704
At round 78 training accuracy: 0.9770978380359105
At round 78 training loss: 0.0834523271150126
gradient difference: 25.073237290980423
At round 79 accuracy: 0.9748885238206993
At round 79 training accuracy: 0.9768884468408103
At round 79 training loss: 0.08338715084309951
gradient difference: 24.85825616839429
At round 80 accuracy: 0.9744191504341704
At round 80 training accuracy: 0.9771501858346856
At round 80 training loss: 0.08302447516557927
gradient difference: 24.713036478930537
At round 81 accuracy: 0.9751232105139639
At round 81 training accuracy: 0.9772025336334608
At round 81 training loss: 0.08279930374934698
gradient difference: 24.737393293072707
At round 82 accuracy: 0.9748885238206993
At round 82 training accuracy: 0.9770454902371355
At round 82 training loss: 0.08247640981928192
gradient difference: 24.574801002902348
At round 83 accuracy: 0.9753578972072283
At round 83 training accuracy: 0.9775427943254986
At round 83 training loss: 0.08132294386453486
gradient difference: 24.048686334413805
At round 84 accuracy: 0.9753578972072283
At round 84 training accuracy: 0.9778045333193739
At round 84 training loss: 0.08073444612634695
gradient difference: 23.67379378386942
At round 85 accuracy: 0.9758272705937573
At round 85 training accuracy: 0.977909228916924
At round 85 training loss: 0.0800186780338698
gradient difference: 23.145702165699113
At round 86 accuracy: 0.9767660173668153
At round 86 training accuracy: 0.978249489608962
At round 86 training loss: 0.07953959852716411
gradient difference: 22.88546440918781
At round 87 accuracy: 0.9774700774466087
At round 87 training accuracy: 0.9785112286028372
At round 87 training loss: 0.0793496231391602
gradient difference: 22.56000826664011
At round 88 accuracy: 0.9772353907533443
At round 88 training accuracy: 0.9784850547034497
At round 88 training loss: 0.07910735943468114
gradient difference: 22.570146947311905
At round 89 accuracy: 0.9777047641398733
At round 89 training accuracy: 0.9787206197979375
At round 89 training loss: 0.07883256839291673
gradient difference: 22.387157284299366
At round 90 accuracy: 0.9777047641398733
At round 90 training accuracy: 0.9787206197979375
At round 90 training loss: 0.07810345263302908
gradient difference: 22.389584396939266
At round 91 accuracy: 0.9777047641398733
At round 91 training accuracy: 0.978746793697325
At round 91 training loss: 0.07750145625430313
gradient difference: 22.392189697811897
At round 92 accuracy: 0.9770007040600798
At round 92 training accuracy: 0.9786682719991624
At round 92 training loss: 0.07666231875337548
gradient difference: 21.856172618779063
At round 93 accuracy: 0.9770007040600798
At round 93 training accuracy: 0.9787991414961
At round 93 training loss: 0.0761187092480993
gradient difference: 21.79415446954868
At round 94 accuracy: 0.9784088242196668
At round 94 training accuracy: 0.9793226194838507
At round 94 training loss: 0.07502735611562737
gradient difference: 20.735697951885406
At round 95 accuracy: 0.9786435109129312
At round 95 training accuracy: 0.9795058367795634
At round 95 training loss: 0.0748409006862556
gradient difference: 20.69529976261497
At round 96 accuracy: 0.9788781976061958
At round 96 training accuracy: 0.9796367062765011
At round 96 training loss: 0.07467548046900707
gradient difference: 20.604054136371154
At round 97 accuracy: 0.9788781976061958
At round 97 training accuracy: 0.9796367062765011
At round 97 training loss: 0.07368328635527843
gradient difference: 20.077418482667316
At round 98 accuracy: 0.9786435109129312
At round 98 training accuracy: 0.9798199235722138
At round 98 training loss: 0.07319981966944611
gradient difference: 19.97960759126705
At round 99 accuracy: 0.9788781976061958
At round 99 training accuracy: 0.9798460974716013
At round 99 training loss: 0.07288964969633864
gradient difference: 19.755835984340475
At round 100 accuracy: 0.9788781976061958
At round 100 training accuracy: 0.9798460974716013
At round 100 training loss: 0.0723727408883865
gradient difference: 19.763802683573196
At round 101 accuracy: 0.9791128842994602
At round 101 training accuracy: 0.9797675757734388
At round 101 training loss: 0.07231402440089815
gradient difference: 19.727860066013275
At round 102 accuracy: 0.9793475709927247
At round 102 training accuracy: 0.9801078364654766
At round 102 training loss: 0.07191703289533993
gradient difference: 19.526951954491437
At round 103 accuracy: 0.9793475709927247
At round 103 training accuracy: 0.9803172276605768
At round 103 training loss: 0.07175535678341677
gradient difference: 19.375033652494988
At round 104 accuracy: 0.9800516310725181
At round 104 training accuracy: 0.98081453174894
At round 104 training loss: 0.07099447736960927
gradient difference: 18.87743672253782
At round 105 accuracy: 0.9798169443792537
At round 105 training accuracy: 0.9811547924409778
At round 105 training loss: 0.07058431710422879
gradient difference: 18.46133650009821
At round 106 accuracy: 0.9798169443792537
At round 106 training accuracy: 0.9811809663403653
At round 106 training loss: 0.07012462010509618
gradient difference: 18.466196905743722
At round 107 accuracy: 0.9798169443792537
At round 107 training accuracy: 0.9811547924409778
At round 107 training loss: 0.06974013656061777
gradient difference: 18.472884707082812
At round 108 accuracy: 0.9802863177657827
At round 108 training accuracy: 0.9813380097366906
At round 108 training loss: 0.0694223500530115
gradient difference: 18.16531742279089
At round 109 accuracy: 0.9800516310725181
At round 109 training accuracy: 0.9815735748311784
At round 109 training loss: 0.06929283524381112
gradient difference: 18.005232794787922
At round 110 accuracy: 0.9800516310725181
At round 110 training accuracy: 0.9815735748311784
At round 110 training loss: 0.06892522776258844
gradient difference: 18.007715797298466
At round 111 accuracy: 0.9800516310725181
At round 111 training accuracy: 0.9816259226299534
At round 111 training loss: 0.06862830567469443
gradient difference: 17.911151102282155
At round 112 accuracy: 0.9800516310725181
At round 112 training accuracy: 0.9816259226299534
At round 112 training loss: 0.06833141990507781
gradient difference: 17.91448314624571
At round 113 accuracy: 0.9800516310725181
At round 113 training accuracy: 0.9816520965293409
At round 113 training loss: 0.06808634063250953
gradient difference: 17.90832829295852
At round 114 accuracy: 0.9802863177657827
At round 114 training accuracy: 0.9815474009317908
At round 114 training loss: 0.06797021510774374
gradient difference: 17.86168176228149
At round 115 accuracy: 0.9802863177657827
At round 115 training accuracy: 0.9815212270324033
At round 115 training loss: 0.06774012734188663
gradient difference: 17.85951219500154
At round 116 accuracy: 0.9802863177657827
At round 116 training accuracy: 0.9818614877244412
At round 116 training loss: 0.06754724708491218
gradient difference: 17.72685265006173
At round 117 accuracy: 0.9805210044590472
At round 117 training accuracy: 0.9822540962152542
At round 117 training loss: 0.0667051287458108
gradient difference: 17.264877790537415
At round 118 accuracy: 0.9802863177657827
At round 118 training accuracy: 0.9822802701146417
At round 118 training loss: 0.06613714488373158
gradient difference: 16.911233084727858
At round 119 accuracy: 0.9802863177657827
At round 119 training accuracy: 0.9822279223158666
At round 119 training loss: 0.06596965393163051
gradient difference: 16.90987971777547
At round 120 accuracy: 0.9802863177657827
At round 120 training accuracy: 0.982097052818929
At round 120 training loss: 0.06571056099659195
gradient difference: 16.813482820408108
At round 121 accuracy: 0.9802863177657827
At round 121 training accuracy: 0.9822802701146417
At round 121 training loss: 0.06544805584670019
gradient difference: 16.776028581872048
At round 122 accuracy: 0.9802863177657827
At round 122 training accuracy: 0.9822540962152542
At round 122 training loss: 0.06528103145845711
gradient difference: 16.775217935141143
At round 123 accuracy: 0.9802863177657827
At round 123 training accuracy: 0.9822802701146417
At round 123 training loss: 0.06515018453067385
gradient difference: 16.77601102637669
At round 124 accuracy: 0.9802863177657827
At round 124 training accuracy: 0.9822802701146417
At round 124 training loss: 0.06503224746197218
gradient difference: 16.780814526607987
At round 125 accuracy: 0.9802863177657827
At round 125 training accuracy: 0.9822802701146417
At round 125 training loss: 0.0649417213594805
gradient difference: 16.835799664364437
At round 126 accuracy: 0.9800516310725181
At round 126 training accuracy: 0.9823849657121918
At round 126 training loss: 0.06473065745237343
gradient difference: 16.74718667890175
At round 127 accuracy: 0.9802863177657827
At round 127 training accuracy: 0.9823849657121918
At round 127 training loss: 0.06462094207368
gradient difference: 16.750877802888258
At round 128 accuracy: 0.9802863177657827
At round 128 training accuracy: 0.9823326179134168
At round 128 training loss: 0.06437840196965841
gradient difference: 16.674183494484303
At round 129 accuracy: 0.9805210044590472
At round 129 training accuracy: 0.9823849657121918
At round 129 training loss: 0.06410947232670491
gradient difference: 16.546502567223783
At round 130 accuracy: 0.9802863177657827
At round 130 training accuracy: 0.9824896613097419
At round 130 training loss: 0.0637353835726039
gradient difference: 16.164571842921664
At round 131 accuracy: 0.9802863177657827
At round 131 training accuracy: 0.9824634874103544
At round 131 training loss: 0.06363826241467899
gradient difference: 16.166839145260877
At round 132 accuracy: 0.9802863177657827
At round 132 training accuracy: 0.9824896613097419
At round 132 training loss: 0.06355752860162685
gradient difference: 16.180398698789
At round 133 accuracy: 0.9802863177657827
At round 133 training accuracy: 0.9822802701146417
At round 133 training loss: 0.06340877964539755
gradient difference: 16.22030962914395
At round 134 accuracy: 0.9805210044590472
At round 134 training accuracy: 0.9824111396115793
At round 134 training loss: 0.06317746996795381
gradient difference: 16.122281143643008
At round 135 accuracy: 0.9805210044590472
At round 135 training accuracy: 0.9822540962152542
At round 135 training loss: 0.06287966683064633
gradient difference: 16.079520747499714
At round 136 accuracy: 0.9805210044590472
At round 136 training accuracy: 0.9822802701146417
At round 136 training loss: 0.06276893427515347
gradient difference: 16.074760302778113
At round 137 accuracy: 0.9805210044590472
At round 137 training accuracy: 0.9825158352091294
At round 137 training loss: 0.06260897121401454
gradient difference: 16.00009370948212
At round 138 accuracy: 0.9805210044590472
At round 138 training accuracy: 0.9825158352091294
At round 138 training loss: 0.06251055158726931
gradient difference: 15.998649404922514
At round 139 accuracy: 0.9805210044590472
At round 139 training accuracy: 0.982542009108517
At round 139 training loss: 0.062428366119750514
gradient difference: 16.000116457205007
At round 140 accuracy: 0.9807556911523116
At round 140 training accuracy: 0.9824896613097419
At round 140 training loss: 0.062315114227255726
gradient difference: 15.920951840356738
At round 141 accuracy: 0.9805210044590472
At round 141 training accuracy: 0.9826205308066795
At round 141 training loss: 0.06198865721679493
gradient difference: 15.735064553372782
At round 142 accuracy: 0.9805210044590472
At round 142 training accuracy: 0.982594356907292
At round 142 training loss: 0.061898278502133815
gradient difference: 15.732852193280541
At round 143 accuracy: 0.9805210044590472
At round 143 training accuracy: 0.982594356907292
At round 143 training loss: 0.06180993199322318
gradient difference: 15.726741535920306
At round 144 accuracy: 0.9805210044590472
At round 144 training accuracy: 0.982594356907292
At round 144 training loss: 0.06172476657275757
gradient difference: 15.72171312186497
At round 145 accuracy: 0.9805210044590472
At round 145 training accuracy: 0.9825681830079045
At round 145 training loss: 0.06165579636682404
gradient difference: 15.72084557003158
At round 146 accuracy: 0.9805210044590472
At round 146 training accuracy: 0.9826990525048421
At round 146 training loss: 0.06147136659211532
gradient difference: 15.63491957507931
At round 147 accuracy: 0.9805210044590472
At round 147 training accuracy: 0.98293461759933
At round 147 training loss: 0.06130755972523052
gradient difference: 15.471201743932214
At round 148 accuracy: 0.9809903778455762
At round 148 training accuracy: 0.982986965398105
At round 148 training loss: 0.06109972783748868
gradient difference: 15.389052861666066
At round 149 accuracy: 0.9805210044590472
At round 149 training accuracy: 0.9829607914987175
At round 149 training loss: 0.060687654242495774
gradient difference: 15.1461193304477
At round 150 accuracy: 0.9805210044590472
At round 150 training accuracy: 0.9828037481023923
At round 150 training loss: 0.060431311701687244
gradient difference: 15.044228137197537
At round 151 accuracy: 0.9814597512321052
At round 151 training accuracy: 0.9828299220017799
At round 151 training loss: 0.06011494093898838
gradient difference: 14.911758119413216
At round 152 accuracy: 0.9814597512321052
At round 152 training accuracy: 0.9828299220017799
At round 152 training loss: 0.060044342199587056
gradient difference: 14.908166779261514
At round 153 accuracy: 0.9812250645388406
At round 153 training accuracy: 0.9828560959011674
At round 153 training loss: 0.05997553737393128
gradient difference: 14.93181646030018
At round 154 accuracy: 0.9812250645388406
At round 154 training accuracy: 0.9828822698005549
At round 154 training loss: 0.05991782205014208
gradient difference: 14.936483299777338
At round 155 accuracy: 0.9812250645388406
At round 155 training accuracy: 0.9828560959011674
At round 155 training loss: 0.05986126219030667
gradient difference: 14.93842253037145
At round 156 accuracy: 0.9812250645388406
At round 156 training accuracy: 0.9829607914987175
At round 156 training loss: 0.05983765865895492
gradient difference: 15.018324662565256
At round 157 accuracy: 0.9812250645388406
At round 157 training accuracy: 0.98293461759933
At round 157 training loss: 0.059540325303694644
gradient difference: 14.836344030248139
At round 158 accuracy: 0.9812250645388406
At round 158 training accuracy: 0.9829607914987175
At round 158 training loss: 0.05947572128807605
gradient difference: 14.831027155421197
At round 159 accuracy: 0.9812250645388406
At round 159 training accuracy: 0.9829607914987175
At round 159 training loss: 0.05942296064074495
gradient difference: 14.834829653817033
At round 160 accuracy: 0.9812250645388406
At round 160 training accuracy: 0.9830131392974926
At round 160 training loss: 0.059205060639302726
gradient difference: 14.723340044425246
At round 161 accuracy: 0.9812250645388406
At round 161 training accuracy: 0.9830393131968801
At round 161 training loss: 0.05906315761587748
gradient difference: 14.664252073651356
At round 162 accuracy: 0.9814597512321052
At round 162 training accuracy: 0.9831963565932053
At round 162 training loss: 0.058845661738222685
gradient difference: 14.501766319878937
At round 163 accuracy: 0.9814597512321052
At round 163 training accuracy: 0.9832225304925928
At round 163 training loss: 0.05879345078310196
gradient difference: 14.503249251190102
At round 164 accuracy: 0.9816944379253696
At round 164 training accuracy: 0.9834057477883055
At round 164 training loss: 0.05845451035934964
gradient difference: 14.182655656506858
At round 165 accuracy: 0.9814597512321052
At round 165 training accuracy: 0.9834057477883055
At round 165 training loss: 0.0583864291330452
gradient difference: 14.170962022735011
At round 166 accuracy: 0.9814597512321052
At round 166 training accuracy: 0.9834057477883055
At round 166 training loss: 0.05833957790595799
gradient difference: 14.176795645034852
At round 167 accuracy: 0.9814597512321052
At round 167 training accuracy: 0.9834057477883055
At round 167 training loss: 0.05829276656561329
gradient difference: 14.172864455843532
At round 168 accuracy: 0.9807556911523116
At round 168 training accuracy: 0.9834057477883055
At round 168 training loss: 0.05780069697528721
gradient difference: 13.802483791597671
At round 169 accuracy: 0.9805210044590472
At round 169 training accuracy: 0.9834580955870805
At round 169 training loss: 0.057654391823400396
gradient difference: 13.757098858001207
At round 170 accuracy: 0.9805210044590472
At round 170 training accuracy: 0.9834580955870805
At round 170 training loss: 0.05761420544647664
gradient difference: 13.757408625860505
At round 171 accuracy: 0.9805210044590472
At round 171 training accuracy: 0.9834580955870805
At round 171 training loss: 0.057565550420896484
gradient difference: 13.755150104334838
At round 172 accuracy: 0.9805210044590472
At round 172 training accuracy: 0.9834580955870805
At round 172 training loss: 0.057525528484384
gradient difference: 13.756500247583272
At round 173 accuracy: 0.9805210044590472
At round 173 training accuracy: 0.9834057477883055
At round 173 training loss: 0.05738915825934819
gradient difference: 13.722680530928756
At round 174 accuracy: 0.9805210044590472
At round 174 training accuracy: 0.9834057477883055
At round 174 training loss: 0.05734934449330395
gradient difference: 13.720942111895534
At round 175 accuracy: 0.9805210044590472
At round 175 training accuracy: 0.9834057477883055
At round 175 training loss: 0.05716216039290587
gradient difference: 13.579656551885387
At round 176 accuracy: 0.9805210044590472
At round 176 training accuracy: 0.9834057477883055
At round 176 training loss: 0.0569270034156295
gradient difference: 13.414281280506138
At round 177 accuracy: 0.9809903778455762
At round 177 training accuracy: 0.9833272260901429
At round 177 training loss: 0.05660996854586741
gradient difference: 13.220159797941287
At round 178 accuracy: 0.9805210044590472
At round 178 training accuracy: 0.9832487043919803
At round 178 training loss: 0.05649439514469383
gradient difference: 13.174356050331891
At round 179 accuracy: 0.9805210044590472
At round 179 training accuracy: 0.9834580955870805
At round 179 training loss: 0.05622641623898159
gradient difference: 12.983476039256336
At round 180 accuracy: 0.9807556911523116
At round 180 training accuracy: 0.9834057477883055
At round 180 training loss: 0.05601140467625727
gradient difference: 12.782613331336226
At round 181 accuracy: 0.9807556911523116
At round 181 training accuracy: 0.9833272260901429
At round 181 training loss: 0.05586034003170696
gradient difference: 12.75752515508807
At round 182 accuracy: 0.9807556911523116
At round 182 training accuracy: 0.9831701826938177
At round 182 training loss: 0.05571350646628664
gradient difference: 12.69336839632989
At round 183 accuracy: 0.9805210044590472
At round 183 training accuracy: 0.9831963565932053
At round 183 training loss: 0.05546229860302891
gradient difference: 12.580258485939417
At round 184 accuracy: 0.9805210044590472
At round 184 training accuracy: 0.9832225304925928
At round 184 training loss: 0.055356524433427624
gradient difference: 12.559985738284176
At round 185 accuracy: 0.9807556911523116
At round 185 training accuracy: 0.9833533999895304
At round 185 training loss: 0.055191515569861994
gradient difference: 12.401441517791335
At round 186 accuracy: 0.9807556911523116
At round 186 training accuracy: 0.9833533999895304
At round 186 training loss: 0.055146608176626394
gradient difference: 12.405238996424941
At round 187 accuracy: 0.9807556911523116
At round 187 training accuracy: 0.9833272260901429
At round 187 training loss: 0.055103110565466774
gradient difference: 12.41331820224322
At round 188 accuracy: 0.9807556911523116
At round 188 training accuracy: 0.9833533999895304
At round 188 training loss: 0.05494020547683341
gradient difference: 12.295674026215178
At round 189 accuracy: 0.9809903778455762
At round 189 training accuracy: 0.9835104433858556
At round 189 training loss: 0.05481788315752735
gradient difference: 12.295395634063901
At round 190 accuracy: 0.9812250645388406
At round 190 training accuracy: 0.9835366172852431
At round 190 training loss: 0.05467515234641386
gradient difference: 12.267024405816683
At round 191 accuracy: 0.9819291246186341
At round 191 training accuracy: 0.9836413128827933
At round 191 training loss: 0.05449647087806155
gradient difference: 12.082245424131694
At round 192 accuracy: 0.9819291246186341
At round 192 training accuracy: 0.9836151389834058
At round 192 training loss: 0.054454047195113424
gradient difference: 12.09104779169047
At round 193 accuracy: 0.9816944379253696
At round 193 training accuracy: 0.9836151389834058
At round 193 training loss: 0.05432625613470043
gradient difference: 12.05312262964196
At round 194 accuracy: 0.9812250645388406
At round 194 training accuracy: 0.9835889650840182
At round 194 training loss: 0.05428697549709859
gradient difference: 12.05306164462059
At round 195 accuracy: 0.9812250645388406
At round 195 training accuracy: 0.9836674867821809
At round 195 training loss: 0.05416723533052885
gradient difference: 12.057187890968736
At round 196 accuracy: 0.9812250645388406
At round 196 training accuracy: 0.9836413128827933
At round 196 training loss: 0.054129260863056916
gradient difference: 12.049225422912965
At round 197 accuracy: 0.9814597512321052
At round 197 training accuracy: 0.9836413128827933
At round 197 training loss: 0.05404228335651944
gradient difference: 12.101986578254165
At round 198 accuracy: 0.9816944379253696
At round 198 training accuracy: 0.9837198345809559
At round 198 training loss: 0.053998342841674325
gradient difference: 12.11618280317141
At round 199 accuracy: 0.9812250645388406
At round 199 training accuracy: 0.983772182379731
At round 199 training loss: 0.053809635587593996
gradient difference: 12.004005644089084
At round 200 accuracy: 0.9814597512321052
At round 200 training accuracy: 0.9837983562791185
