Arguments:
	        auto_tune : 0.1
	       batch_size : 10
	clients_per_round : 10
	          dataset : synthetic_iid
	     drop_percent : 0.0
	       eval_every : 1
	    learning_rate : 0.01
	            model : mclr
	     model_params : (10,)
	               mu : 1.0
	       num_epochs : 20
	        num_iters : 1
	       num_rounds : 200
	        optimizer : fedprox_auto_mu
	             seed : 0
Using Federated prox to Train

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/3.64k flops)
  dense/kernel/Initializer/stateless_random_uniform (600/1.20k flops)
    dense/kernel/Initializer/stateless_random_uniform/mul (600/600 flops)
    dense/kernel/Initializer/stateless_random_uniform/sub (1/1 flops)
  PGD/update_dense/kernel/add (600/600 flops)
  PGD/update_dense/kernel/mul (600/600 flops)
  PGD/update_dense/kernel/mul_1 (600/600 flops)
  PGD/update_dense/kernel/sub (600/600 flops)
  PGD/update_dense/bias/add (10/10 flops)
  PGD/update_dense/bias/mul (10/10 flops)
  PGD/update_dense/bias/mul_1 (10/10 flops)
  PGD/update_dense/bias/sub (10/10 flops)
  gradients/sparse_softmax_cross_entropy_loss/value_grad/Neg (1/1 flops)
  gradients/sparse_softmax_cross_entropy_loss/value_grad/mul (1/1 flops)
  sparse_softmax_cross_entropy_loss/num_present/Equal (1/1 flops)

======================End of Report==========================
30 Clients in Total
Training with 10 workers ---
At round 0 accuracy: 0.04627766599597585
At round 0 training accuracy: 0.06398706398706398
At round 0 training loss: 2.3676777861910843
learning rate of μ: 0.1
Gradient difference: 0.04150136419749851
At round 1 accuracy: 0.15694164989939638
At round 1 training accuracy: 0.16285516285516285
At round 1 training loss: 2.2066500494788
learning rate of μ: 0.1
Gradient difference: 0.040023904476801424
At round 2 accuracy: 0.30784708249496984
At round 2 training accuracy: 0.319011319011319
At round 2 training loss: 2.075232623658298
learning rate of μ: 0.1
Gradient difference: 0.038562038355646595
At round 3 accuracy: 0.43661971830985913
At round 3 training accuracy: 0.42065142065142064
At round 3 training loss: 1.9598826827284754
learning rate of μ: 0.1
Gradient difference: 0.03713877028242618
At round 4 accuracy: 0.48490945674044267
At round 4 training accuracy: 0.4670824670824671
At round 4 training loss: 1.8597431263270459
learning rate of μ: 0.1
Gradient difference: 0.035775476928977826
At round 5 accuracy: 0.5110663983903421
At round 5 training accuracy: 0.49064449064449067
At round 5 training loss: 1.7779942429018891
learning rate of μ: 0.1
Gradient difference: 0.03461384675015132
At round 6 accuracy: 0.5251509054325956
At round 6 training accuracy: 0.5118965118965119
At round 6 training loss: 1.7083335177235859
learning rate of μ: 0.1
Gradient difference: 0.03357743220391366
