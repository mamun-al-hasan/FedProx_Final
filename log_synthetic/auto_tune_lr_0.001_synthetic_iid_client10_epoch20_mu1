Arguments:
	        auto_tune : 0.001
	       batch_size : 10
	clients_per_round : 10
	          dataset : synthetic_iid
	     drop_percent : 0.0
	       eval_every : 1
	    learning_rate : 0.01
	            model : mclr
	     model_params : (10,)
	               mu : 1.0
	       num_epochs : 20
	        num_iters : 1
	       num_rounds : 200
	        optimizer : fedprox_auto_mu
	             seed : 0
Using Federated prox to Train

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/3.64k flops)
  dense/kernel/Initializer/stateless_random_uniform (600/1.20k flops)
    dense/kernel/Initializer/stateless_random_uniform/mul (600/600 flops)
    dense/kernel/Initializer/stateless_random_uniform/sub (1/1 flops)
  PGD/update_dense/kernel/add (600/600 flops)
  PGD/update_dense/kernel/mul (600/600 flops)
  PGD/update_dense/kernel/mul_1 (600/600 flops)
  PGD/update_dense/kernel/sub (600/600 flops)
  PGD/update_dense/bias/add (10/10 flops)
  PGD/update_dense/bias/mul (10/10 flops)
  PGD/update_dense/bias/mul_1 (10/10 flops)
  PGD/update_dense/bias/sub (10/10 flops)
  gradients/sparse_softmax_cross_entropy_loss/value_grad/Neg (1/1 flops)
  gradients/sparse_softmax_cross_entropy_loss/value_grad/mul (1/1 flops)
  sparse_softmax_cross_entropy_loss/num_present/Equal (1/1 flops)

======================End of Report==========================
30 Clients in Total
Training with 10 workers ---
At round 0 accuracy: 0.04627766599597585
At round 0 training accuracy: 0.06398706398706398
At round 0 training loss: 2.3676777861910843
learning rate of μ: 0.001
Updated μ: 1.0009999997590442, Gradient difference: 0.04150136419749851
At round 1 accuracy: 0.15694164989939638
At round 1 training accuracy: 0.16285516285516285
At round 1 training loss: 2.2066500494788
learning rate of μ: 0.001
Updated μ: 1.00169417773458, Gradient difference: 0.040023904476801424
At round 2 accuracy: 0.30784708249496984
At round 2 training accuracy: 0.319011319011319
At round 2 training loss: 2.075232623658298
learning rate of μ: 0.001
Updated μ: 1.002250118328543, Gradient difference: 0.038562038355646595
At round 3 accuracy: 0.43661971830985913
At round 3 training accuracy: 0.42065142065142064
At round 3 training loss: 1.9598826827284754
learning rate of μ: 0.001
Updated μ: 1.002722139262989, Gradient difference: 0.03713877028242618
At round 4 accuracy: 0.48490945674044267
At round 4 training accuracy: 0.4670824670824671
At round 4 training loss: 1.8597431263270459
learning rate of μ: 0.001
Updated μ: 1.0031360542285277, Gradient difference: 0.035775476928977826
At round 5 accuracy: 0.5110663983903421
At round 5 training accuracy: 0.49064449064449067
At round 5 training loss: 1.7779942429018891
learning rate of μ: 0.001
Updated μ: 1.0035078251169838, Gradient difference: 0.03461384675015132
At round 6 accuracy: 0.5251509054325956
At round 6 training accuracy: 0.5118965118965119
At round 6 training loss: 1.7083335177235859
learning rate of μ: 0.001
Updated μ: 1.0038470768781578, Gradient difference: 0.03357743220391366
At round 7 accuracy: 0.5392354124748491
At round 7 training accuracy: 0.5252945252945252
At round 7 training loss: 1.6484187507387065
learning rate of μ: 0.001
Updated μ: 1.0041602776701515, Gradient difference: 0.032641323755222676
At round 8 accuracy: 0.5492957746478874
At round 8 training accuracy: 0.5343035343035343
At round 8 training loss: 1.60046825636945
learning rate of μ: 0.001
Updated μ: 1.00445272219716, Gradient difference: 0.03187147832851797
At round 9 accuracy: 0.5533199195171026
At round 9 training accuracy: 0.5444675444675444
At round 9 training loss: 1.5580619631421624
learning rate of μ: 0.001
Updated μ: 1.0047279280275399, Gradient difference: 0.03119743235897348
At round 10 accuracy: 0.5593561368209256
At round 10 training accuracy: 0.5495495495495496
At round 10 training loss: 1.5224449941593716
learning rate of μ: 0.001
Updated μ: 1.0049887166602955, Gradient difference: 0.03062276626905618
At round 11 accuracy: 0.5633802816901409
At round 11 training accuracy: 0.5562485562485563
At round 11 training loss: 1.492490276918039
learning rate of μ: 0.001
Updated μ: 1.0052371060189507, Gradient difference: 0.03011045098513513
At round 12 accuracy: 0.5633802816901409
At round 12 training accuracy: 0.5631785631785632
At round 12 training loss: 1.4656284200481522
learning rate of μ: 0.001
Updated μ: 1.00547511732491, Gradient difference: 0.02970607637719764
At round 13 accuracy: 0.5694164989939637
At round 13 training accuracy: 0.568029568029568
At round 13 training loss: 1.4413385750873449
learning rate of μ: 0.001
Updated μ: 1.0057037140121392, Gradient difference: 0.02930705771577496
At round 14 accuracy: 0.5734406438631791
At round 14 training accuracy: 0.5717255717255717
At round 14 training loss: 1.419647658000672
learning rate of μ: 0.001
Updated μ: 1.005924153022934, Gradient difference: 0.028973946369711897
At round 15 accuracy: 0.5734406438631791
At round 15 training accuracy: 0.574959574959575
At round 15 training loss: 1.4000493694216418
learning rate of μ: 0.001
Updated μ: 1.0061372331301235, Gradient difference: 0.028665010370938916
At round 16 accuracy: 0.5754527162977867
At round 16 training accuracy: 0.5798105798105798
At round 16 training loss: 1.3829358219136594
learning rate of μ: 0.001
Updated μ: 1.0063436178359106, Gradient difference: 0.02837519026078404
At round 17 accuracy: 0.5814889336016097
At round 17 training accuracy: 0.5837375837375838
At round 17 training loss: 1.3665288970938012
learning rate of μ: 0.001
Updated μ: 1.0065439745587816, Gradient difference: 0.02811654041411017
At round 18 accuracy: 0.5855130784708249
At round 18 training accuracy: 0.5862785862785863
At round 18 training loss: 1.352752331801776
learning rate of μ: 0.001
Updated μ: 1.0067388917957394, Gradient difference: 0.027888106763348918
At round 19 accuracy: 0.5895372233400402
At round 19 training accuracy: 0.5872025872025872
At round 19 training loss: 1.3394698202541173
learning rate of μ: 0.001
Updated μ: 1.0069287561447786, Gradient difference: 0.027668436138300136
At round 20 accuracy: 0.5895372233400402
At round 20 training accuracy: 0.5892815892815892
At round 20 training loss: 1.3272790955902385
learning rate of μ: 0.001
Updated μ: 1.0071140194723711, Gradient difference: 0.02747353813459168
At round 21 accuracy: 0.5915492957746479
At round 21 training accuracy: 0.5908985908985909
At round 21 training loss: 1.315812431546055
learning rate of μ: 0.001
Updated μ: 1.0072949565689737, Gradient difference: 0.027282284871594068
At round 22 accuracy: 0.5915492957746479
At round 22 training accuracy: 0.592977592977593
At round 22 training loss: 1.3051157890717684
learning rate of μ: 0.001
Updated μ: 1.0074717126020194, Gradient difference: 0.027078205112182212
At round 23 accuracy: 0.6016096579476862
At round 23 training accuracy: 0.5959805959805959
At round 23 training loss: 1.2951556970682672
learning rate of μ: 0.001
Updated μ: 1.0076446080797645, Gradient difference: 0.026891771252429735
At round 24 accuracy: 0.6036217303822937
At round 24 training accuracy: 0.5971355971355972
At round 24 training loss: 1.2861590123281217
learning rate of μ: 0.001
Updated μ: 1.007814034731532, Gradient difference: 0.02673880621886043
At round 25 accuracy: 0.607645875251509
At round 25 training accuracy: 0.6006006006006006
At round 25 training loss: 1.2763667696203582
learning rate of μ: 0.001
Updated μ: 1.0079802441757384, Gradient difference: 0.02660107699765286
At round 26 accuracy: 0.6016096579476862
At round 26 training accuracy: 0.6015246015246015
At round 26 training loss: 1.2679535829324209
learning rate of μ: 0.001
Updated μ: 1.0081433607770995, Gradient difference: 0.02646047181433148
At round 27 accuracy: 0.6116700201207244
At round 27 training accuracy: 0.601986601986602
At round 27 training loss: 1.260280769563597
learning rate of μ: 0.001
Updated μ: 1.0083035160684424, Gradient difference: 0.026319834503777288
At round 28 accuracy: 0.613682092555332
At round 28 training accuracy: 0.6033726033726033
At round 28 training loss: 1.2526185099757496
learning rate of μ: 0.001
Updated μ: 1.0084608657831107, Gradient difference: 0.026184954997606275
At round 29 accuracy: 0.607645875251509
At round 29 training accuracy: 0.6045276045276046
At round 29 training loss: 1.2449603362181467
learning rate of μ: 0.001
Updated μ: 1.0086157282721266, Gradient difference: 0.02608574718493384
At round 30 accuracy: 0.6056338028169014
At round 30 training accuracy: 0.6047586047586048
At round 30 training loss: 1.237590634429061
learning rate of μ: 0.001
Updated μ: 1.0087681674590128, Gradient difference: 0.02598120122178327
At round 31 accuracy: 0.607645875251509
At round 31 training accuracy: 0.6089166089166089
At round 31 training loss: 1.2303654734216516
learning rate of μ: 0.001
Updated μ: 1.0089183295263098, Gradient difference: 0.025886615058338294
At round 32 accuracy: 0.6177062374245473
At round 32 training accuracy: 0.6114576114576115
At round 32 training loss: 1.2232376215886949
learning rate of μ: 0.001
Updated μ: 1.0090662234498844, Gradient difference: 0.025779093902936402
At round 33 accuracy: 0.6217303822937625
At round 33 training accuracy: 0.6133056133056133
At round 33 training loss: 1.2169453266394499
learning rate of μ: 0.001
Updated μ: 1.0092119622243567, Gradient difference: 0.025677591088942676
At round 34 accuracy: 0.6257545271629779
At round 34 training accuracy: 0.6153846153846154
At round 34 training loss: 1.2104774920872656
learning rate of μ: 0.001
Updated μ: 1.0093556454789383, Gradient difference: 0.025580864817486074
At round 35 accuracy: 0.6277665995975855
At round 35 training accuracy: 0.617001617001617
At round 35 training loss: 1.2040621949883414
learning rate of μ: 0.001
Updated μ: 1.0094972773191797, Gradient difference: 0.025472415375105403
At round 36 accuracy: 0.6277665995975855
At round 36 training accuracy: 0.6172326172326172
At round 36 training loss: 1.198215237911812
learning rate of μ: 0.001
Updated μ: 1.0096369032503363, Gradient difference: 0.025360072753077216
At round 37 accuracy: 0.6297786720321932
At round 37 training accuracy: 0.6176946176946176
At round 37 training loss: 1.1923887509621163
learning rate of μ: 0.001
Updated μ: 1.0097745736161892, Gradient difference: 0.025245268910795128
At round 38 accuracy: 0.6317907444668008
At round 38 training accuracy: 0.6188496188496189
At round 38 training loss: 1.1869450598562747
learning rate of μ: 0.001
Updated μ: 1.0099104858672945, Gradient difference: 0.02515630345613582
At round 39 accuracy: 0.6317907444668008
At round 39 training accuracy: 0.6202356202356203
At round 39 training loss: 1.1816119030513839
learning rate of μ: 0.001
Updated μ: 1.010044776580936, Gradient difference: 0.02508337522020042
At round 40 accuracy: 0.6317907444668008
At round 40 training accuracy: 0.6204666204666205
At round 40 training loss: 1.176525132489221
learning rate of μ: 0.001
Updated μ: 1.010177391579764, Gradient difference: 0.024991109116467423
At round 41 accuracy: 0.6317907444668008
At round 41 training accuracy: 0.6239316239316239
At round 41 training loss: 1.171394201232644
learning rate of μ: 0.001
Updated μ: 1.0103082668691634, Gradient difference: 0.02487723645228257
At round 42 accuracy: 0.6398390342052314
At round 42 training accuracy: 0.6273966273966274
At round 42 training loss: 1.1667455164106812
learning rate of μ: 0.001
Updated μ: 1.0104374117132442, Gradient difference: 0.02475561727866398
At round 43 accuracy: 0.641851106639839
At round 43 training accuracy: 0.6273966273966274
At round 43 training loss: 1.1615328307679411
learning rate of μ: 0.001
Updated μ: 1.0105651511532248, Gradient difference: 0.0246884709412586
At round 44 accuracy: 0.6438631790744467
At round 44 training accuracy: 0.629013629013629
At round 44 training loss: 1.1565905955350544
learning rate of μ: 0.001
Updated μ: 1.0106915081793013, Gradient difference: 0.02461861126773873
At round 45 accuracy: 0.6458752515090543
At round 45 training accuracy: 0.6301686301686301
At round 45 training loss: 1.151790188872861
learning rate of μ: 0.001
Updated μ: 1.010816481902003, Gradient difference: 0.024541501210376966
At round 46 accuracy: 0.647887323943662
At round 46 training accuracy: 0.6310926310926311
At round 46 training loss: 1.1471517699837106
learning rate of μ: 0.001
Updated μ: 1.0109400734644611, Gradient difference: 0.024457594013179488
At round 47 accuracy: 0.647887323943662
At round 47 training accuracy: 0.6315546315546315
At round 47 training loss: 1.1428143131895412
learning rate of μ: 0.001
Updated μ: 1.0110623785657604, Gradient difference: 0.024386092970963297
At round 48 accuracy: 0.6539235412474849
At round 48 training accuracy: 0.632016632016632
At round 48 training loss: 1.1388624431520225
learning rate of μ: 0.001
Updated μ: 1.0111834082141382, Gradient difference: 0.02431049281201918
At round 49 accuracy: 0.6539235412474849
At round 49 training accuracy: 0.6345576345576346
At round 49 training loss: 1.1344543344072944
learning rate of μ: 0.001
Updated μ: 1.011303159223035, Gradient difference: 0.02422800603590353
At round 50 accuracy: 0.6498993963782697
At round 50 training accuracy: 0.6368676368676369
At round 50 training loss: 1.130148491977296
learning rate of μ: 0.001
Updated μ: 1.0114215152666424, Gradient difference: 0.024115277821276027
At round 51 accuracy: 0.6498993963782697
At round 51 training accuracy: 0.6357126357126357
At round 51 training loss: 1.1257793891234387
learning rate of μ: 0.001
Updated μ: 1.01153880126397, Gradient difference: 0.02406333459240093
At round 52 accuracy: 0.6498993963782697
At round 52 training accuracy: 0.6377916377916378
At round 52 training loss: 1.121486824251097
learning rate of μ: 0.001
Updated μ: 1.0116549442782643, Gradient difference: 0.02399119142037711
At round 53 accuracy: 0.6519114688128773
At round 53 training accuracy: 0.6387156387156387
At round 53 training loss: 1.1174429660582548
learning rate of μ: 0.001
Updated μ: 1.0117700201582607, Gradient difference: 0.02392973009886978
At round 54 accuracy: 0.6539235412474849
At round 54 training accuracy: 0.6394086394086395
At round 54 training loss: 1.1134854503635117
learning rate of μ: 0.001
Updated μ: 1.0118840562628473, Gradient difference: 0.023869219836127064
At round 55 accuracy: 0.6539235412474849
At round 55 training accuracy: 0.6407946407946408
At round 55 training loss: 1.1098592681734962
learning rate of μ: 0.001
Updated μ: 1.0119970280583706, Gradient difference: 0.02379880158404033
At round 56 accuracy: 0.6559356136820925
At round 56 training accuracy: 0.6396396396396397
At round 56 training loss: 1.1062007644044856
learning rate of μ: 0.001
Updated μ: 1.0121090757840203, Gradient difference: 0.023753716976843878
At round 57 accuracy: 0.6599597585513078
At round 57 training accuracy: 0.6405636405636406
At round 57 training loss: 1.1023871080653445
learning rate of μ: 0.001
Updated μ: 1.0122201482875892, Gradient difference: 0.02369358203420931
At round 58 accuracy: 0.6579476861167002
At round 58 training accuracy: 0.6421806421806422
At round 58 training loss: 1.0986921010708033
learning rate of μ: 0.001
Updated μ: 1.0123302066795257, Gradient difference: 0.023620748579457777
At round 59 accuracy: 0.6519114688128773
At round 59 training accuracy: 0.6435666435666436
At round 59 training loss: 1.095126544115697
learning rate of μ: 0.001
Updated μ: 1.0124392101810311, Gradient difference: 0.023534581810674478
At round 60 accuracy: 0.6539235412474849
At round 60 training accuracy: 0.6451836451836452
At round 60 training loss: 1.0915327415534601
learning rate of μ: 0.001
Updated μ: 1.012547300109805, Gradient difference: 0.023474871828329583
At round 61 accuracy: 0.6539235412474849
At round 61 training accuracy: 0.6456456456456456
At round 61 training loss: 1.0880378205083925
learning rate of μ: 0.001
Updated μ: 1.0126544300574143, Gradient difference: 0.02340105644437659
At round 62 accuracy: 0.6559356136820925
At round 62 training accuracy: 0.6458766458766458
At round 62 training loss: 1.0845167298161644
learning rate of μ: 0.001
Updated μ: 1.0127606938693878, Gradient difference: 0.023344035678539186
At round 63 accuracy: 0.6559356136820925
At round 63 training accuracy: 0.6468006468006468
At round 63 training loss: 1.08146920418128
learning rate of μ: 0.001
Updated μ: 1.0128660245694268, Gradient difference: 0.023268486421145148
At round 64 accuracy: 0.6539235412474849
At round 64 training accuracy: 0.6472626472626473
At round 64 training loss: 1.078291375908573
learning rate of μ: 0.001
Updated μ: 1.0129704685432115, Gradient difference: 0.023199483996677962
At round 65 accuracy: 0.6559356136820925
At round 65 training accuracy: 0.6474936474936475
At round 65 training loss: 1.0750342567155917
learning rate of μ: 0.001
Updated μ: 1.0130741055635784, Gradient difference: 0.023144871336891987
At round 66 accuracy: 0.6599597585513078
At round 66 training accuracy: 0.65003465003465
At round 66 training loss: 1.071920419818784
learning rate of μ: 0.001
Updated μ: 1.0131769294066677, Gradient difference: 0.023085630984502017
At round 67 accuracy: 0.6599597585513078
At round 67 training accuracy: 0.6516516516516516
At round 67 training loss: 1.0686388778532314
learning rate of μ: 0.001
Updated μ: 1.0132790037760808, Gradient difference: 0.023037693075521542
At round 68 accuracy: 0.6579476861167002
At round 68 training accuracy: 0.6534996534996536
At round 68 training loss: 1.0655943916972446
learning rate of μ: 0.001
Updated μ: 1.0133802676212076, Gradient difference: 0.02297285118610727
At round 69 accuracy: 0.6619718309859155
At round 69 training accuracy: 0.6548856548856549
At round 69 training loss: 1.0626982547829844
learning rate of μ: 0.001
Updated μ: 1.013480760902402, Gradient difference: 0.022914036910150922
At round 70 accuracy: 0.6639839034205232
At round 70 training accuracy: 0.6562716562716563
At round 70 training loss: 1.0595728871755716
learning rate of μ: 0.001
Updated μ: 1.0135804978102536, Gradient difference: 0.022855533245726357
At round 71 accuracy: 0.6680080482897385
At round 71 training accuracy: 0.6578886578886579
At round 71 training loss: 1.0567319102363295
learning rate of μ: 0.001
Updated μ: 1.0136794657479813, Gradient difference: 0.022791208058697034
At round 72 accuracy: 0.6659959758551308
At round 72 training accuracy: 0.6574266574266574
At round 72 training loss: 1.0539842386200566
learning rate of μ: 0.001
Updated μ: 1.0137777073321192, Gradient difference: 0.022733910086476815
At round 73 accuracy: 0.6619718309859155
At round 73 training accuracy: 0.6581196581196581
At round 73 training loss: 1.0511637665322757
learning rate of μ: 0.001
Updated μ: 1.0138752682475387, Gradient difference: 0.022684613448360685
At round 74 accuracy: 0.6619718309859155
At round 74 training accuracy: 0.6585816585816586
At round 74 training loss: 1.0484628008623051
learning rate of μ: 0.001
Updated μ: 1.0139722015269408, Gradient difference: 0.022645316611536452
At round 75 accuracy: 0.6639839034205232
At round 75 training accuracy: 0.6588126588126588
At round 75 training loss: 1.0456297547416837
learning rate of μ: 0.001
Updated μ: 1.0140684123105, Gradient difference: 0.02258128387224329
At round 76 accuracy: 0.6639839034205232
At round 76 training accuracy: 0.6599676599676599
At round 76 training loss: 1.0427840730559608
learning rate of μ: 0.001
Updated μ: 1.0141638907524464, Gradient difference: 0.022512245875609054
At round 77 accuracy: 0.6639839034205232
At round 77 training accuracy: 0.6618156618156619
At round 77 training loss: 1.0399497402377975
learning rate of μ: 0.001
Updated μ: 1.0142586786895584, Gradient difference: 0.022450519953491517
At round 78 accuracy: 0.6659959758551308
At round 78 training accuracy: 0.6632016632016632
At round 78 training loss: 1.0370706099923033
learning rate of μ: 0.001
Updated μ: 1.0143529217088978, Gradient difference: 0.022421248199792753
At round 79 accuracy: 0.670020120724346
At round 79 training accuracy: 0.6634326634326634
At round 79 training loss: 1.0343981538889562
learning rate of μ: 0.001
Updated μ: 1.0144465403506007, Gradient difference: 0.02237095354345071
At round 80 accuracy: 0.6680080482897385
At round 80 training accuracy: 0.6645876645876646
At round 80 training loss: 1.0317706369753383
learning rate of μ: 0.001
Updated μ: 1.0145394472147036, Gradient difference: 0.022297308604479482
At round 81 accuracy: 0.6740442655935613
At round 81 training accuracy: 0.6652806652806653
At round 81 training loss: 1.0290156083780009
learning rate of μ: 0.001
Updated μ: 1.014631697482664, Gradient difference: 0.02223453928231094
At round 82 accuracy: 0.6680080482897385
At round 82 training accuracy: 0.6662046662046662
At round 82 training loss: 1.0265621197474253
learning rate of μ: 0.001
Updated μ: 1.0147233372834599, Gradient difference: 0.022180733608149993
At round 83 accuracy: 0.670020120724346
At round 83 training accuracy: 0.6666666666666666
At round 83 training loss: 1.0238660121933365
learning rate of μ: 0.001
Updated μ: 1.0148144321273003, Gradient difference: 0.022140887740427807
At round 84 accuracy: 0.6720321931589537
At round 84 training accuracy: 0.6666666666666666
At round 84 training loss: 1.0213578129701402
learning rate of μ: 0.001
Updated μ: 1.014904944749391, Gradient difference: 0.022090049762617598
At round 85 accuracy: 0.6720321931589537
At round 85 training accuracy: 0.668052668052668
At round 85 training loss: 1.0189131620740417
learning rate of μ: 0.001
Updated μ: 1.0149948503757742, Gradient difference: 0.022031129188186446
At round 86 accuracy: 0.6720321931589537
At round 86 training accuracy: 0.6678216678216679
At round 86 training loss: 1.016324849113853
learning rate of μ: 0.001
Updated μ: 1.0150842470948969, Gradient difference: 0.02199448662770164
At round 87 accuracy: 0.670020120724346
At round 87 training accuracy: 0.668976668976669
At round 87 training loss: 1.0138427043902898
learning rate of μ: 0.001
Updated μ: 1.0151730848084293, Gradient difference: 0.02194371611945275
At round 88 accuracy: 0.670020120724346
At round 88 training accuracy: 0.6694386694386695
At round 88 training loss: 1.0114895403867066
learning rate of μ: 0.001
Updated μ: 1.0152614092725478, Gradient difference: 0.021902539329335183
At round 89 accuracy: 0.6680080482897385
At round 89 training accuracy: 0.6699006699006699
At round 89 training loss: 1.0089364295799976
learning rate of μ: 0.001
Updated μ: 1.0153492579294998, Gradient difference: 0.021869099127289453
At round 90 accuracy: 0.6680080482897385
At round 90 training accuracy: 0.6708246708246708
At round 90 training loss: 1.006667230800603
learning rate of μ: 0.001
Updated μ: 1.0154366247150766, Gradient difference: 0.021832625107946767
At round 91 accuracy: 0.670020120724346
At round 91 training accuracy: 0.6701316701316701
At round 91 training loss: 1.0044942651314888
learning rate of μ: 0.001
Updated μ: 1.0155234251791667, Gradient difference: 0.021773282066651706
At round 92 accuracy: 0.670020120724346
At round 92 training accuracy: 0.6715176715176715
At round 92 training loss: 1.0021992544842582
learning rate of μ: 0.001
Updated μ: 1.0156096924461342, Gradient difference: 0.021720506733284795
At round 93 accuracy: 0.6720321931589537
At round 93 training accuracy: 0.6712866712866713
At round 93 training loss: 0.9998085996134779
learning rate of μ: 0.001
Updated μ: 1.015695494310154, Gradient difference: 0.021683290056949862
At round 94 accuracy: 0.670020120724346
At round 94 training accuracy: 0.6717486717486717
At round 94 training loss: 0.997739875955487
learning rate of μ: 0.001
Updated μ: 1.0157808536626232, Gradient difference: 0.021650480687269456
At round 95 accuracy: 0.6740442655935613
At round 95 training accuracy: 0.6735966735966736
At round 95 training loss: 0.9954751769068281
learning rate of μ: 0.001
Updated μ: 1.0158656701094515, Gradient difference: 0.02159057816264126
At round 96 accuracy: 0.676056338028169
At round 96 training accuracy: 0.6752136752136753
At round 96 training loss: 0.9932383719920341
learning rate of μ: 0.001
Updated μ: 1.0159499898151803, Gradient difference: 0.021540841759712402
At round 97 accuracy: 0.6740442655935613
At round 97 training accuracy: 0.6754446754446755
At round 97 training loss: 0.9908562048281773
learning rate of μ: 0.001
Updated μ: 1.0160338903246628, Gradient difference: 0.021509591140593625
At round 98 accuracy: 0.6780684104627767
At round 98 training accuracy: 0.6770616770616771
At round 98 training loss: 0.9887280703334451
learning rate of μ: 0.001
Updated μ: 1.0161172438847919, Gradient difference: 0.021443993930308276
At round 99 accuracy: 0.682092555331992
At round 99 training accuracy: 0.677985677985678
At round 99 training loss: 0.9864602876910163
learning rate of μ: 0.001
Updated μ: 1.0162001704396022, Gradient difference: 0.021407876175194806
At round 100 accuracy: 0.6841046277665996
At round 100 training accuracy: 0.6782166782166782
At round 100 training loss: 0.9845273529968297
learning rate of μ: 0.001
Updated μ: 1.0162825902462622, Gradient difference: 0.021349694852575218
At round 101 accuracy: 0.6841046277665996
At round 101 training accuracy: 0.6796026796026796
At round 101 training loss: 0.9823718568584224
learning rate of μ: 0.001
Updated μ: 1.016364594145167, Gradient difference: 0.021313744292637205
At round 102 accuracy: 0.6861167002012073
At round 102 training accuracy: 0.6786786786786787
At round 102 training loss: 0.9802767895446085
learning rate of μ: 0.001
Updated μ: 1.0164461684588568, Gradient difference: 0.021272987512656664
At round 103 accuracy: 0.6861167002012073
At round 103 training accuracy: 0.68006468006468
At round 103 training loss: 0.9782302001933078
learning rate of μ: 0.001
Updated μ: 1.01652729726958, Gradient difference: 0.02122678047294398
At round 104 accuracy: 0.682092555331992
At round 104 training accuracy: 0.6796026796026796
At round 104 training loss: 0.9762051094484319
learning rate of μ: 0.001
Updated μ: 1.0166081015151653, Gradient difference: 0.02121122119456365
At round 105 accuracy: 0.682092555331992
At round 105 training accuracy: 0.6786786786786787
At round 105 training loss: 0.9741431578226193
learning rate of μ: 0.001
Updated μ: 1.0166885427713888, Gradient difference: 0.02118458781831072
At round 106 accuracy: 0.682092555331992
At round 106 training accuracy: 0.6793716793716794
At round 106 training loss: 0.9720356544106682
learning rate of μ: 0.001
Updated μ: 1.0167686207082947, Gradient difference: 0.021156848937489984
At round 107 accuracy: 0.6841046277665996
At round 107 training accuracy: 0.6796026796026796
At round 107 training loss: 0.9699565850303256
learning rate of μ: 0.001
Updated μ: 1.0168483238346075, Gradient difference: 0.021125028983590033
At round 108 accuracy: 0.6861167002012073
At round 108 training accuracy: 0.6816816816816816
At round 108 training loss: 0.9679739903594088
learning rate of μ: 0.001
Updated μ: 1.0169275419871784, Gradient difference: 0.021062682036827016
At round 109 accuracy: 0.6861167002012073
At round 109 training accuracy: 0.6821436821436822
At round 109 training loss: 0.9658855426997174
learning rate of μ: 0.001
Updated μ: 1.0170064149009057, Gradient difference: 0.021036424452062694
At round 110 accuracy: 0.6881287726358148
At round 110 training accuracy: 0.6814506814506814
At round 110 training loss: 0.9638732133382021
learning rate of μ: 0.001
Updated μ: 1.0170849280984084, Gradient difference: 0.021005325370526635
At round 111 accuracy: 0.6881287726358148
At round 111 training accuracy: 0.6821436821436822
At round 111 training loss: 0.9619977514830272
learning rate of μ: 0.001
Updated μ: 1.0171630779258836, Gradient difference: 0.02097225079673186
At round 112 accuracy: 0.6881287726358148
At round 112 training accuracy: 0.6828366828366829
At round 112 training loss: 0.9601997522179869
learning rate of μ: 0.001
Updated μ: 1.0172407853489809, Gradient difference: 0.020916775377848463
At round 113 accuracy: 0.6921529175050302
At round 113 training accuracy: 0.681912681912682
At round 113 training loss: 0.9583115600829147
learning rate of μ: 0.001
Updated μ: 1.0173181626121677, Gradient difference: 0.02089053732576502
At round 114 accuracy: 0.6881287726358148
At round 114 training accuracy: 0.683991683991684
At round 114 training loss: 0.9563060350273795
learning rate of μ: 0.001
Updated μ: 1.017395170708804, Gradient difference: 0.020852791718395883
At round 115 accuracy: 0.6901408450704225
At round 115 training accuracy: 0.6863016863016863
At round 115 training loss: 0.9545329526242331
learning rate of μ: 0.001
Updated μ: 1.0174716689789087, Gradient difference: 0.02077561581710785
At round 116 accuracy: 0.6941649899396378
At round 116 training accuracy: 0.6863016863016863
At round 116 training loss: 0.9526085877671862
learning rate of μ: 0.001
Updated μ: 1.017547826918294, Gradient difference: 0.02074343160505258
At round 117 accuracy: 0.6901408450704225
At round 117 training accuracy: 0.6865326865326865
At round 117 training loss: 0.95081075347855
learning rate of μ: 0.001
Updated μ: 1.0176236676332626, Gradient difference: 0.020716693172575715
At round 118 accuracy: 0.6921529175050302
At round 118 training accuracy: 0.6879186879186879
At round 118 training loss: 0.9489348930887384
learning rate of μ: 0.001
Updated μ: 1.0176991880139639, Gradient difference: 0.0206882708379445
At round 119 accuracy: 0.6901408450704225
At round 119 training accuracy: 0.6888426888426888
At round 119 training loss: 0.947092765747303
learning rate of μ: 0.001
Updated μ: 1.017774363090291, Gradient difference: 0.020652115412387463
At round 120 accuracy: 0.6941649899396378
At round 120 training accuracy: 0.689073689073689
At round 120 training loss: 0.9453216941093953
learning rate of μ: 0.001
Updated μ: 1.0178491557136606, Gradient difference: 0.020604759524587603
At round 121 accuracy: 0.6941649899396378
At round 121 training accuracy: 0.6895356895356896
At round 121 training loss: 0.9435613908787527
learning rate of μ: 0.001
Updated μ: 1.0179236699479206, Gradient difference: 0.020585293734387184
At round 122 accuracy: 0.6941649899396378
At round 122 training accuracy: 0.6895356895356896
At round 122 training loss: 0.9418790436529018
learning rate of μ: 0.001
Updated μ: 1.017997851967832, Gradient difference: 0.02055013765987351
At round 123 accuracy: 0.6961770623742455
At round 123 training accuracy: 0.68999768999769
At round 123 training loss: 0.9399851801269296
learning rate of μ: 0.001
Updated μ: 1.0180716906977367, Gradient difference: 0.02051102948848186
At round 124 accuracy: 0.6981891348088531
At round 124 training accuracy: 0.6916146916146916
At round 124 training loss: 0.9381821882121694
learning rate of μ: 0.001
Updated μ: 1.0181451813595586, Gradient difference: 0.020469694627409874
At round 125 accuracy: 0.6981891348088531
At round 125 training accuracy: 0.6913836913836914
At round 125 training loss: 0.936468428842849
learning rate of μ: 0.001
Updated μ: 1.018218362078007, Gradient difference: 0.020438165589493452
At round 126 accuracy: 0.6981891348088531
At round 126 training accuracy: 0.6920766920766921
At round 126 training loss: 0.9346518736701828
learning rate of μ: 0.001
Updated μ: 1.0182912561986315, Gradient difference: 0.020412427041130016
At round 127 accuracy: 0.7002012072434608
At round 127 training accuracy: 0.693000693000693
At round 127 training loss: 0.9328620833236975
learning rate of μ: 0.001
Updated μ: 1.0183638167819233, Gradient difference: 0.020372729482883947
At round 128 accuracy: 0.7002012072434608
At round 128 training accuracy: 0.6943866943866944
At round 128 training loss: 0.931068796143848
learning rate of μ: 0.001
Updated μ: 1.018436073246829, Gradient difference: 0.020340510966922947
At round 129 accuracy: 0.7002012072434608
At round 129 training accuracy: 0.6946176946176946
At round 129 training loss: 0.929327523179626
learning rate of μ: 0.001
Updated μ: 1.0185079996371647, Gradient difference: 0.020300172136559203
At round 130 accuracy: 0.7002012072434608
At round 130 training accuracy: 0.6962346962346962
At round 130 training loss: 0.927688210452228
learning rate of μ: 0.001
Updated μ: 1.01857960429635, Gradient difference: 0.020261377318719947
At round 131 accuracy: 0.6981891348088531
At round 131 training accuracy: 0.695079695079695
At round 131 training loss: 0.9261176569215639
learning rate of μ: 0.001
Updated μ: 1.0186509152404628, Gradient difference: 0.020229769483440317
At round 132 accuracy: 0.7002012072434608
At round 132 training accuracy: 0.6964656964656964
At round 132 training loss: 0.9244347158384862
learning rate of μ: 0.001
Updated μ: 1.018721933022523, Gradient difference: 0.020197602062349367
At round 133 accuracy: 0.7002012072434608
At round 133 training accuracy: 0.6973896973896974
At round 133 training loss: 0.9227365464502186
learning rate of μ: 0.001
Updated μ: 1.0187926656497857, Gradient difference: 0.020167015560461864
At round 134 accuracy: 0.7002012072434608
At round 134 training accuracy: 0.6983136983136983
At round 134 training loss: 0.9210350669457472
learning rate of μ: 0.001
Updated μ: 1.0188631880724603, Gradient difference: 0.020157270521141767
At round 135 accuracy: 0.7022132796780685
At round 135 training accuracy: 0.6971586971586972
At round 135 training loss: 0.9194657248805208
learning rate of μ: 0.001
Updated μ: 1.0189334635763714, Gradient difference: 0.020136479290058174
At round 136 accuracy: 0.7002012072434608
At round 136 training accuracy: 0.6971586971586972
At round 136 training loss: 0.9178385627713097
learning rate of μ: 0.001
Updated μ: 1.019003502125705, Gradient difference: 0.0201179872481957
At round 137 accuracy: 0.7022132796780685
At round 137 training accuracy: 0.6980826980826981
At round 137 training loss: 0.9162672698484003
learning rate of μ: 0.001
Updated μ: 1.0190732036181067, Gradient difference: 0.020069982804058004
At round 138 accuracy: 0.6981891348088531
At round 138 training accuracy: 0.6983136983136983
At round 138 training loss: 0.9147758568977202
learning rate of μ: 0.001
Updated μ: 1.0191427102463135, Gradient difference: 0.020062394338722856
At round 139 accuracy: 0.6961770623742455
At round 139 training accuracy: 0.6983136983136983
At round 139 training loss: 0.9132360494227684
learning rate of μ: 0.001
Updated μ: 1.0192118308474674, Gradient difference: 0.019998802397803456
At round 140 accuracy: 0.6981891348088531
At round 140 training accuracy: 0.6980826980826981
At round 140 training loss: 0.9116847692171512
learning rate of μ: 0.001
Updated μ: 1.0192806691772316, Gradient difference: 0.01996449144723909
At round 141 accuracy: 0.6981891348088531
At round 141 training accuracy: 0.6983136983136983
At round 141 training loss: 0.9101566203639039
learning rate of μ: 0.001
Updated μ: 1.0193492373156179, Gradient difference: 0.01993304425317305
At round 142 accuracy: 0.6981891348088531
At round 142 training accuracy: 0.699006699006699
At round 142 training loss: 0.9085914731356144
learning rate of μ: 0.001
Updated μ: 1.0194176180013699, Gradient difference: 0.01992518985342872
At round 143 accuracy: 0.6961770623742455
At round 143 training accuracy: 0.6992376992376992
At round 143 training loss: 0.9071162564012629
learning rate of μ: 0.001
Updated μ: 1.019485762893314, Gradient difference: 0.0199027480180876
At round 144 accuracy: 0.7002012072434608
At round 144 training accuracy: 0.7001617001617002
At round 144 training loss: 0.9055810692018987
learning rate of μ: 0.001
Updated μ: 1.019553676461247, Gradient difference: 0.01988108762620239
At round 145 accuracy: 0.6981891348088531
At round 145 training accuracy: 0.6992376992376992
At round 145 training loss: 0.9041502580105171
learning rate of μ: 0.001
Updated μ: 1.0196213760154356, Gradient difference: 0.01986400996185397
At round 146 accuracy: 0.6981891348088531
At round 146 training accuracy: 0.7001617001617002
At round 146 training loss: 0.9025897571632454
learning rate of μ: 0.001
Updated μ: 1.0196886598544967, Gradient difference: 0.019786872775575247
At round 147 accuracy: 0.7022132796780685
At round 147 training accuracy: 0.7006237006237006
At round 147 training loss: 0.900999073881452
learning rate of μ: 0.001
Updated μ: 1.0197557253046892, Gradient difference: 0.019767153042635818
At round 148 accuracy: 0.6981891348088531
At round 148 training accuracy: 0.702009702009702
At round 148 training loss: 0.8995402691187498
learning rate of μ: 0.001
Updated μ: 1.0198224206315223, Gradient difference: 0.019701929885177202
At round 149 accuracy: 0.6981891348088531
At round 149 training accuracy: 0.7036267036267037
At round 149 training loss: 0.8979697748072072
learning rate of μ: 0.001
Updated μ: 1.0198888461283275, Gradient difference: 0.019665655377370563
At round 150 accuracy: 0.6981891348088531
At round 150 training accuracy: 0.704088704088704
At round 150 training loss: 0.8965569459337317
learning rate of μ: 0.001
Updated μ: 1.0199550279112068, Gradient difference: 0.01963655397638664
At round 151 accuracy: 0.7002012072434608
At round 151 training accuracy: 0.7054747054747055
At round 151 training loss: 0.8952030800429247
learning rate of μ: 0.001
Updated μ: 1.0200209929106776, Gradient difference: 0.019614955568267818
At round 152 accuracy: 0.7022132796780685
At round 152 training accuracy: 0.7052437052437053
At round 152 training loss: 0.8937000890862008
learning rate of μ: 0.001
Updated μ: 1.0200866832287871, Gradient difference: 0.019575560038779662
At round 153 accuracy: 0.7022132796780685
At round 153 training accuracy: 0.705012705012705
At round 153 training loss: 0.8921573414183511
learning rate of μ: 0.001
Updated μ: 1.020152184166861, Gradient difference: 0.019561132617535112
At round 154 accuracy: 0.7002012072434608
At round 154 training accuracy: 0.705936705936706
At round 154 training loss: 0.8907756983173667
learning rate of μ: 0.001
Updated μ: 1.0202174405598023, Gradient difference: 0.019529728943842584
At round 155 accuracy: 0.7022132796780685
At round 155 training accuracy: 0.7073227073227073
At round 155 training loss: 0.8893347865688688
learning rate of μ: 0.001
Updated μ: 1.0202823592035375, Gradient difference: 0.019469718624023632
At round 156 accuracy: 0.7022132796780685
At round 156 training accuracy: 0.7084777084777085
At round 156 training loss: 0.8880606198398971
learning rate of μ: 0.001
Updated μ: 1.020346952732035, Gradient difference: 0.019412753953896727
At round 157 accuracy: 0.7022132796780685
At round 157 training accuracy: 0.7082467082467082
At round 157 training loss: 0.8865676398612137
learning rate of μ: 0.001
Updated μ: 1.0204113348227073, Gradient difference: 0.01938943589155382
At round 158 accuracy: 0.7022132796780685
At round 158 training accuracy: 0.7094017094017094
At round 158 training loss: 0.885233593705786
learning rate of μ: 0.001
Updated μ: 1.0204755547626005, Gradient difference: 0.019380608298069476
At round 159 accuracy: 0.7022132796780685
At round 159 training accuracy: 0.7089397089397089
At round 159 training loss: 0.883910581252679
learning rate of μ: 0.001
Updated μ: 1.0205395199655467, Gradient difference: 0.019343345200307154
At round 160 accuracy: 0.7022132796780685
At round 160 training accuracy: 0.7098637098637098
At round 160 training loss: 0.8824940333330045
learning rate of μ: 0.001
Updated μ: 1.0206032879232956, Gradient difference: 0.019323024562298435
At round 161 accuracy: 0.7022132796780685
At round 161 training accuracy: 0.7103257103257103
At round 161 training loss: 0.881052368189507
learning rate of μ: 0.001
Updated μ: 1.0206668659088058, Gradient difference: 0.019304514480532897
At round 162 accuracy: 0.7022132796780685
At round 162 training accuracy: 0.7114807114807115
At round 162 training loss: 0.8798082620146424
learning rate of μ: 0.001
Updated μ: 1.020730205760004, Gradient difference: 0.019270904196419993
At round 163 accuracy: 0.7022132796780685
At round 163 training accuracy: 0.7117117117117117
At round 163 training loss: 0.8783527447461916
learning rate of μ: 0.001
Updated μ: 1.0207933713989834, Gradient difference: 0.019256354645379177
At round 164 accuracy: 0.7022132796780685
At round 164 training accuracy: 0.7112497112497113
At round 164 training loss: 0.8768598457686921
learning rate of μ: 0.001
Updated μ: 1.0208563596408677, Gradient difference: 0.01924048068916836
At round 165 accuracy: 0.7062374245472837
At round 165 training accuracy: 0.7112497112497113
At round 165 training loss: 0.8754085036204191
learning rate of μ: 0.001
Updated μ: 1.0209191399731428, Gradient difference: 0.01921487617100873
At round 166 accuracy: 0.7062374245472837
At round 166 training accuracy: 0.7126357126357127
At round 166 training loss: 0.8741197407479704
learning rate of μ: 0.001
Updated μ: 1.0209816455543617, Gradient difference: 0.019168265780971656
At round 167 accuracy: 0.704225352112676
At round 167 training accuracy: 0.7121737121737122
At round 167 training loss: 0.8726977746435445
learning rate of μ: 0.001
Updated μ: 1.021043966251045, Gradient difference: 0.019148790065336745
At round 168 accuracy: 0.704225352112676
At round 168 training accuracy: 0.711942711942712
At round 168 training loss: 0.8714020745972889
learning rate of μ: 0.001
Updated μ: 1.021106090099936, Gradient difference: 0.019125247599756992
At round 169 accuracy: 0.7082494969818913
At round 169 training accuracy: 0.7124047124047124
At round 169 training loss: 0.8700427820952943
learning rate of μ: 0.001
Updated μ: 1.0211680609455984, Gradient difference: 0.019114884092217416
At round 170 accuracy: 0.7082494969818913
At round 170 training accuracy: 0.7130977130977131
At round 170 training loss: 0.868704508471142
learning rate of μ: 0.001
Updated μ: 1.021229860492794, Gradient difference: 0.019098552477426464
At round 171 accuracy: 0.7062374245472837
At round 171 training accuracy: 0.7147147147147147
At round 171 training loss: 0.8674026594808565
learning rate of μ: 0.001
Updated μ: 1.0212914258282264, Gradient difference: 0.019062331782655564
At round 172 accuracy: 0.7082494969818913
At round 172 training accuracy: 0.7142527142527143
At round 172 training loss: 0.8662250996820093
learning rate of μ: 0.001
Updated μ: 1.0213528234518379, Gradient difference: 0.019046336642849958
At round 173 accuracy: 0.7082494969818913
At round 173 training accuracy: 0.7144837144837145
At round 173 training loss: 0.8648385258215519
learning rate of μ: 0.001
Updated μ: 1.021414040481181, Gradient difference: 0.019025997627657745
At round 174 accuracy: 0.7082494969818913
At round 174 training accuracy: 0.7154077154077154
At round 174 training loss: 0.8634732807583059
learning rate of μ: 0.001
Updated μ: 1.02147505779587, Gradient difference: 0.018999328448491164
At round 175 accuracy: 0.7122736418511066
At round 175 training accuracy: 0.7161007161007161
At round 175 training loss: 0.8621928468260541
learning rate of μ: 0.001
Updated μ: 1.0215357903527038, Gradient difference: 0.018945633910465724
At round 176 accuracy: 0.710261569416499
At round 176 training accuracy: 0.7167937167937168
At round 176 training loss: 0.8609362749090699
learning rate of μ: 0.001
Updated μ: 1.0215963787396318, Gradient difference: 0.018935447362054457
At round 177 accuracy: 0.710261569416499
At round 177 training accuracy: 0.717024717024717
At round 177 training loss: 0.8596543565058603
learning rate of μ: 0.001
Updated μ: 1.02165679301175, Gradient difference: 0.018915583355881945
At round 178 accuracy: 0.710261569416499
At round 178 training accuracy: 0.7167937167937168
At round 178 training loss: 0.8585688984953083
learning rate of μ: 0.001
Updated μ: 1.02171703798341, Gradient difference: 0.0188968996458005
At round 179 accuracy: 0.7082494969818913
At round 179 training accuracy: 0.7167937167937168
At round 179 training loss: 0.85729022491719
learning rate of μ: 0.001
Updated μ: 1.0217770092571854, Gradient difference: 0.018844968465909644
At round 180 accuracy: 0.710261569416499
At round 180 training accuracy: 0.717024717024717
At round 180 training loss: 0.8559668511186689
learning rate of μ: 0.001
Updated μ: 1.0218367864064928, Gradient difference: 0.018817618761630937
At round 181 accuracy: 0.710261569416499
At round 181 training accuracy: 0.7172557172557172
At round 181 training loss: 0.854813906675014
learning rate of μ: 0.001
Updated μ: 1.0218964668976431, Gradient difference: 0.01882073852830562
At round 182 accuracy: 0.7142857142857143
At round 182 training accuracy: 0.7172557172557172
At round 182 training loss: 0.8535520600843
learning rate of μ: 0.001
Updated μ: 1.0219559995720344, Gradient difference: 0.018807481035674232
At round 183 accuracy: 0.710261569416499
At round 183 training accuracy: 0.717948717948718
At round 183 training loss: 0.8524066200292697
learning rate of μ: 0.001
Updated μ: 1.0220153355621922, Gradient difference: 0.018778431142766273
At round 184 accuracy: 0.7142857142857143
At round 184 training accuracy: 0.7184107184107185
At round 184 training loss: 0.8512073229575824
learning rate of μ: 0.001
Updated μ: 1.022074540248333, Gradient difference: 0.018769801324978044
At round 185 accuracy: 0.7142857142857143
At round 185 training accuracy: 0.7188727188727189
At round 185 training loss: 0.8499418503143793
learning rate of μ: 0.001
Updated μ: 1.0221335516082486, Gradient difference: 0.018741170684516115
At round 186 accuracy: 0.7142857142857143
At round 186 training accuracy: 0.72002772002772
At round 186 training loss: 0.8488495634207414
learning rate of μ: 0.001
Updated μ: 1.0221923122192635, Gradient difference: 0.01869383742752288
At round 187 accuracy: 0.7122736418511066
At round 187 training accuracy: 0.72002772002772
At round 187 training loss: 0.8477803151060621
learning rate of μ: 0.001
Updated μ: 1.0222509252497367, Gradient difference: 0.018679000155735828
At round 188 accuracy: 0.7122736418511066
At round 188 training accuracy: 0.720951720951721
At round 188 training loss: 0.8465427100093901
learning rate of μ: 0.001
Updated μ: 1.0223093417964597, Gradient difference: 0.01864822971359996
At round 189 accuracy: 0.7122736418511066
At round 189 training accuracy: 0.7216447216447216
At round 189 training loss: 0.8452519359311046
learning rate of μ: 0.001
Updated μ: 1.0223675790402753, Gradient difference: 0.018622597912147078
At round 190 accuracy: 0.710261569416499
At round 190 training accuracy: 0.7221067221067221
At round 190 training loss: 0.8440461466344903
learning rate of μ: 0.001
Updated μ: 1.0224255948455272, Gradient difference: 0.018583088372674895
At round 191 accuracy: 0.710261569416499
At round 191 training accuracy: 0.7225687225687226
At round 191 training loss: 0.8427858155218165
learning rate of μ: 0.001
Updated μ: 1.0224834385040191, Gradient difference: 0.018559022135979035
At round 192 accuracy: 0.7122736418511066
At round 192 training accuracy: 0.7227997227997228
At round 192 training loss: 0.8416363716015339
learning rate of μ: 0.001
Updated μ: 1.0225410919618716, Gradient difference: 0.018528816437040456
At round 193 accuracy: 0.7122736418511066
At round 193 training accuracy: 0.7234927234927235
At round 193 training loss: 0.8404861576674468
learning rate of μ: 0.001
Updated μ: 1.0225985849659722, Gradient difference: 0.018507863084398543
At round 194 accuracy: 0.7122736418511066
At round 194 training accuracy: 0.723030723030723
At round 194 training loss: 0.839383906664378
learning rate of μ: 0.001
Updated μ: 1.0226559278335767, Gradient difference: 0.01848995629763809
At round 195 accuracy: 0.7122736418511066
At round 195 training accuracy: 0.7239547239547239
At round 195 training loss: 0.8382604960537199
learning rate of μ: 0.001
Updated μ: 1.022713058341248, Gradient difference: 0.018451618451598463
At round 196 accuracy: 0.7142857142857143
At round 196 training accuracy: 0.7244167244167244
At round 196 training loss: 0.8372254239325987
learning rate of μ: 0.001
Updated μ: 1.0227699963654115, Gradient difference: 0.01841933286410908
At round 197 accuracy: 0.7142857142857143
At round 197 training accuracy: 0.7262647262647263
At round 197 training loss: 0.8360948302118518
learning rate of μ: 0.001
Updated μ: 1.0228267006072242, Gradient difference: 0.01837326697664694
At round 198 accuracy: 0.7142857142857143
At round 198 training accuracy: 0.7246477246477246
At round 198 training loss: 0.8350162380120032
learning rate of μ: 0.001
Updated μ: 1.022883237861885, Gradient difference: 0.018348508541885872
At round 199 accuracy: 0.7122736418511066
At round 199 training accuracy: 0.7248787248787248
At round 199 training loss: 0.8340290579407009
learning rate of μ: 0.001
Updated μ: 1.0229395845540559, Gradient difference: 0.01831576263060294
At round 200 accuracy: 0.7122736418511066
At round 200 training accuracy: 0.7267267267267268
