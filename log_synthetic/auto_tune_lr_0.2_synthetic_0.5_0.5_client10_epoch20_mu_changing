Arguments:
	        auto_tune : 0.2
	       batch_size : 10
	clients_per_round : 10
	          dataset : synthetic_0.5_0.5
	     drop_percent : 0.0
	       eval_every : 1
	    learning_rate : 0.01
	            model : mclr
	     model_params : (10,)
	               mu : 1.0
	       num_epochs : 20
	        num_iters : 1
	       num_rounds : 200
	        optimizer : fedprox_auto_mu
	             seed : 0
Using Federated prox to Train

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/3.64k flops)
  dense/kernel/Initializer/stateless_random_uniform (600/1.20k flops)
    dense/kernel/Initializer/stateless_random_uniform/mul (600/600 flops)
    dense/kernel/Initializer/stateless_random_uniform/sub (1/1 flops)
  PGD/update_dense/kernel/add (600/600 flops)
  PGD/update_dense/kernel/mul (600/600 flops)
  PGD/update_dense/kernel/mul_1 (600/600 flops)
  PGD/update_dense/kernel/sub (600/600 flops)
  PGD/update_dense/bias/add (10/10 flops)
  PGD/update_dense/bias/mul (10/10 flops)
  PGD/update_dense/bias/mul_1 (10/10 flops)
  PGD/update_dense/bias/sub (10/10 flops)
  gradients/sparse_softmax_cross_entropy_loss/value_grad/Neg (1/1 flops)
  gradients/sparse_softmax_cross_entropy_loss/value_grad/mul (1/1 flops)
  sparse_softmax_cross_entropy_loss/num_present/Equal (1/1 flops)

======================End of Report==========================
30 Clients in Total
Training with 10 workers ---
At round 0 accuracy: 0.04693733865289838
At round 0 training accuracy: 0.04931162644610794
At round 0 training loss: 1.7335920322068055
learning rate of μ: 0.2
Gradient difference: 143.68560131796275
At round 1 accuracy: 0.04482515841351795
At round 1 training accuracy: 0.04541171543736586
At round 1 training loss: 1.8675588540050714
learning rate of μ: 0.2
Gradient difference: 150.00422051120663
At round 2 accuracy: 0.0636000938746773
At round 2 training accuracy: 0.06197979374967283
At round 2 training loss: 1.960784244600849
learning rate of μ: 0.2
Gradient difference: 153.61144431327133
At round 3 accuracy: 0.9239615113823046
At round 3 training accuracy: 0.9257446474375752
At round 3 training loss: 0.26072611397222273
learning rate of μ: 0.2
Gradient difference: 58.56602547212261
At round 4 accuracy: 0.9260736916216851
At round 4 training accuracy: 0.9286237763702037
At round 4 training loss: 0.24272449044991912
learning rate of μ: 0.2
Gradient difference: 56.578427043984235
At round 5 accuracy: 0.9272471250880076
At round 5 training accuracy: 0.9294089933518296
At round 5 training loss: 0.23943565819075685
learning rate of μ: 0.2
Gradient difference: 53.750554847351935
At round 6 accuracy: 0.9260736916216851
At round 6 training accuracy: 0.9288593414646914
At round 6 training loss: 0.20749005297726297
learning rate of μ: 0.2
Gradient difference: 54.48791179054842
At round 7 accuracy: 0.9345224125792068
At round 7 training accuracy: 0.9362927288907501
At round 7 training loss: 0.1946239731377072
learning rate of μ: 0.2
Gradient difference: 51.944009451591484
At round 8 accuracy: 0.9387467730579676
At round 8 training accuracy: 0.9411610741768308
At round 8 training loss: 0.18758018180300495
learning rate of μ: 0.2
Gradient difference: 50.1796561792065
At round 9 accuracy: 0.9403895799108191
At round 9 training accuracy: 0.9428100298382454
At round 9 training loss: 0.17997919963344652
learning rate of μ: 0.2
Gradient difference: 48.91341601695766
At round 10 accuracy: 0.9392161464444966
At round 10 training accuracy: 0.9421556823535571
At round 10 training loss: 0.17597765047202135
learning rate of μ: 0.2
Gradient difference: 49.48259611273889
At round 11 accuracy: 0.9401548932175545
At round 11 training accuracy: 0.9428885515364079
At round 11 training loss: 0.16817304141365005
learning rate of μ: 0.2
Gradient difference: 48.1257750346605
At round 12 accuracy: 0.9410936399906126
At round 12 training accuracy: 0.9438046380149715
At round 12 training loss: 0.1637578779660748
learning rate of μ: 0.2
Gradient difference: 47.69936092484809
At round 13 accuracy: 0.9410936399906126
At round 13 training accuracy: 0.9437261163168089
At round 13 training loss: 0.162794054028216
learning rate of μ: 0.2
Gradient difference: 48.085113825273694
At round 14 accuracy: 0.9446139403895799
At round 14 training accuracy: 0.9473119405329006
At round 14 training loss: 0.1571668703334751
learning rate of μ: 0.2
Gradient difference: 45.37797886062127
At round 15 accuracy: 0.9448486270828444
At round 15 training accuracy: 0.9473119405329006
At round 15 training loss: 0.1551600594969072
learning rate of μ: 0.2
Gradient difference: 44.70935578528877
At round 16 accuracy: 0.9460220605491668
At round 16 training accuracy: 0.9480971575145265
At round 16 training loss: 0.15416653043277154
learning rate of μ: 0.2
Gradient difference: 44.745660337711314
At round 17 accuracy: 0.9464914339356958
At round 17 training accuracy: 0.9486991572004397
At round 17 training loss: 0.14528572206845514
learning rate of μ: 0.2
Gradient difference: 42.873494014744104
At round 18 accuracy: 0.9467261206289603
At round 18 training accuracy: 0.9483588965084018
At round 18 training loss: 0.14430665664038875
learning rate of μ: 0.2
Gradient difference: 43.15861835602297
At round 19 accuracy: 0.9476648674020183
At round 19 training accuracy: 0.9492488090875779
At round 19 training loss: 0.138203883309683
learning rate of μ: 0.2
Gradient difference: 41.728097331338176
At round 20 accuracy: 0.9467261206289603
At round 20 training accuracy: 0.9490917656912526
At round 20 training loss: 0.13786058405930077
learning rate of μ: 0.2
Gradient difference: 41.91564633596467
At round 21 accuracy: 0.9462567472424314
At round 21 training accuracy: 0.9489347222949275
At round 21 training loss: 0.13748932625832058
learning rate of μ: 0.2
Gradient difference: 42.039325780971815
At round 22 accuracy: 0.9457873738559024
At round 22 training accuracy: 0.9487253310998273
At round 22 training loss: 0.1375497025547956
learning rate of μ: 0.2
Gradient difference: 42.181390740771555
At round 23 accuracy: 0.9455526871626379
At round 23 training accuracy: 0.9485944616028896
At round 23 training loss: 0.1375348479400527
learning rate of μ: 0.2
Gradient difference: 42.2589150068659
At round 24 accuracy: 0.9540014081201595
At round 24 training accuracy: 0.9557922839344606
At round 24 training loss: 0.12747280692818008
learning rate of μ: 0.2
Gradient difference: 40.506382371062976
At round 25 accuracy: 0.9568176484393335
At round 25 training accuracy: 0.9585405433701513
At round 25 training loss: 0.12361861667380804
learning rate of μ: 0.2
Gradient difference: 39.67997475669968
At round 26 accuracy: 0.957521708519127
At round 26 training accuracy: 0.9588808040621892
At round 26 training loss: 0.120882529483967
learning rate of μ: 0.2
Gradient difference: 39.271815842772284
At round 27 accuracy: 0.9577563952123914
At round 27 training accuracy: 0.9597968905407528
At round 27 training loss: 0.11953899464369125
learning rate of μ: 0.2
Gradient difference: 39.129250940391216
At round 28 accuracy: 0.9582257685989204
At round 28 training accuracy: 0.9596136732450401
At round 28 training loss: 0.11966847728068897
learning rate of μ: 0.2
Gradient difference: 38.53968830564205
At round 29 accuracy: 0.9584604552921849
At round 29 training accuracy: 0.9594828037481024
At round 29 training loss: 0.11940636055214865
learning rate of μ: 0.2
Gradient difference: 38.563309747510196
At round 30 accuracy: 0.9584604552921849
At round 30 training accuracy: 0.9594828037481024
At round 30 training loss: 0.11928635042181153
learning rate of μ: 0.2
Gradient difference: 38.624351858770495
At round 31 accuracy: 0.957991081905656
At round 31 training accuracy: 0.9595089776474899
At round 31 training loss: 0.1139517480422712
learning rate of μ: 0.2
Gradient difference: 37.64535275429336
At round 32 accuracy: 0.9596338887585074
At round 32 training accuracy: 0.9621787153850181
At round 32 training loss: 0.11147119646214484
learning rate of μ: 0.2
Gradient difference: 37.08995335687623
At round 33 accuracy: 0.9622154423844168
At round 33 training accuracy: 0.9641155839396953
At round 33 training loss: 0.1059696497422083
learning rate of μ: 0.2
Gradient difference: 34.80079168617305
At round 34 accuracy: 0.9640929359305327
At round 34 training accuracy: 0.9654504528084594
At round 34 training loss: 0.10403572507926115
learning rate of μ: 0.2
Gradient difference: 34.118486688756654
At round 35 accuracy: 0.9664398028631777
At round 35 training accuracy: 0.9675967125582369
At round 35 training loss: 0.10060876388715007
learning rate of μ: 0.2
Gradient difference: 32.39840915913731
At round 36 accuracy: 0.9669091762497066
At round 36 training accuracy: 0.9690101031251636
At round 36 training loss: 0.0980507453576986
learning rate of μ: 0.2
Gradient difference: 31.980985361819535
At round 37 accuracy: 0.9666744895564422
At round 37 training accuracy: 0.9689577553263885
At round 37 training loss: 0.09774728921443993
learning rate of μ: 0.2
Gradient difference: 32.008303299109905
At round 38 accuracy: 0.9671438629429712
At round 38 training accuracy: 0.968983929225776
At round 38 training loss: 0.0962774359145964
learning rate of μ: 0.2
Gradient difference: 31.872612086656165
At round 39 accuracy: 0.9683172964092936
At round 39 training accuracy: 0.9700570591006649
At round 39 training loss: 0.0948629508927428
learning rate of μ: 0.2
Gradient difference: 31.285588170166378
At round 40 accuracy: 0.9685519831025581
At round 40 training accuracy: 0.9700570591006649
At round 40 training loss: 0.09466050258343994
learning rate of μ: 0.2
Gradient difference: 31.329852829694254
At round 41 accuracy: 0.9685519831025581
At round 41 training accuracy: 0.9702926241951526
At round 41 training loss: 0.0937915248566414
learning rate of μ: 0.2
Gradient difference: 31.145265170881057
At round 42 accuracy: 0.9685519831025581
At round 42 training accuracy: 0.9703449719939277
At round 42 training loss: 0.09307597288858457
learning rate of μ: 0.2
Gradient difference: 31.324324164450136
At round 43 accuracy: 0.9680826097160291
At round 43 training accuracy: 0.9703187980945401
At round 43 training loss: 0.09287341828609022
learning rate of μ: 0.2
Gradient difference: 31.33360599538143
At round 44 accuracy: 0.9678479230227646
At round 44 training accuracy: 0.9700832330000524
At round 44 training loss: 0.09196003831088288
learning rate of μ: 0.2
Gradient difference: 30.98881028239442
At round 45 accuracy: 0.9676132363295001
At round 45 training accuracy: 0.969821494006177
At round 45 training loss: 0.09181527370936159
learning rate of μ: 0.2
Gradient difference: 31.02439472650693
At round 46 accuracy: 0.9666744895564422
At round 46 training accuracy: 0.9687221902319008
At round 46 training loss: 0.091918430214087
learning rate of μ: 0.2
Gradient difference: 31.094289654765042
At round 47 accuracy: 0.9671438629429712
At round 47 training accuracy: 0.9687745380306758
At round 47 training loss: 0.0900999933939382
learning rate of μ: 0.2
Gradient difference: 30.91845523118703
At round 48 accuracy: 0.9685519831025581
At round 48 training accuracy: 0.9697953201067895
At round 48 training loss: 0.08879894569444223
learning rate of μ: 0.2
Gradient difference: 30.44903782475543
At round 49 accuracy: 0.9671438629429712
At round 49 training accuracy: 0.9688792336282259
At round 49 training loss: 0.08866264485777084
learning rate of μ: 0.2
Gradient difference: 30.65048904753258
At round 50 accuracy: 0.9676132363295001
At round 50 training accuracy: 0.968983929225776
At round 50 training loss: 0.08840928971833242
learning rate of μ: 0.2
Gradient difference: 30.67969573941507
At round 51 accuracy: 0.9711335367284675
At round 51 training accuracy: 0.9724912317437052
At round 51 training loss: 0.08404594832377621
learning rate of μ: 0.2
Gradient difference: 28.366191156824275
At round 52 accuracy: 0.9711335367284675
At round 52 training accuracy: 0.972386536146155
At round 52 training loss: 0.08396650265701006
learning rate of μ: 0.2
Gradient difference: 28.373848021454254
At round 53 accuracy: 0.9711335367284675
At round 53 training accuracy: 0.9719939276553421
At round 53 training loss: 0.08286971863353021
learning rate of μ: 0.2
Gradient difference: 28.241132820054222
At round 54 accuracy: 0.9711335367284675
At round 54 training accuracy: 0.971941579856567
At round 54 training loss: 0.08276199635355867
learning rate of μ: 0.2
Gradient difference: 28.276577325562336
At round 55 accuracy: 0.97230697019479
At round 55 training accuracy: 0.9725174056430927
At round 55 training loss: 0.08160914593773529
learning rate of μ: 0.2
Gradient difference: 27.818891684041628
At round 56 accuracy: 0.9725416568880545
At round 56 training accuracy: 0.9740878396063446
At round 56 training loss: 0.07941094495972731
learning rate of μ: 0.2
Gradient difference: 26.861571029814854
At round 57 accuracy: 0.9732457169678479
At round 57 training accuracy: 0.9750039260849082
At round 57 training loss: 0.0784162383706205
learning rate of μ: 0.2
Gradient difference: 26.15780120724191
At round 58 accuracy: 0.9737150903543769
At round 58 training accuracy: 0.9749777521855206
At round 58 training loss: 0.0783802821700001
learning rate of μ: 0.2
Gradient difference: 26.076433986816514
At round 59 accuracy: 0.9753578972072283
At round 59 training accuracy: 0.977019316337748
At round 59 training loss: 0.07651750029595558
learning rate of μ: 0.2
Gradient difference: 25.20925680644463
At round 60 accuracy: 0.9758272705937573
At round 60 training accuracy: 0.9769146207401979
At round 60 training loss: 0.07628676769009095
learning rate of μ: 0.2
Gradient difference: 25.21740104613468
At round 61 accuracy: 0.9758272705937573
At round 61 training accuracy: 0.976522012249385
At round 61 training loss: 0.07590507199567863
learning rate of μ: 0.2
Gradient difference: 25.114470048853722
At round 62 accuracy: 0.9762966439802864
At round 62 training accuracy: 0.9767052295450976
At round 62 training loss: 0.0752231385162728
learning rate of μ: 0.2
Gradient difference: 24.55859382222281
At round 63 accuracy: 0.9755925839004929
At round 63 training accuracy: 0.9766267078469351
At round 63 training loss: 0.075114461750472
learning rate of μ: 0.2
Gradient difference: 24.576497298548528
At round 64 accuracy: 0.9758272705937573
At round 64 training accuracy: 0.9767314034444852
At round 64 training loss: 0.07446578652596257
learning rate of μ: 0.2
Gradient difference: 24.479540734912145
At round 65 accuracy: 0.9762966439802864
At round 65 training accuracy: 0.9767052295450976
At round 65 training loss: 0.07365852696579524
learning rate of μ: 0.2
Gradient difference: 23.836421403364913
At round 66 accuracy: 0.9765313306735508
At round 66 training accuracy: 0.9776736638224363
At round 66 training loss: 0.07233164410847098
learning rate of μ: 0.2
Gradient difference: 23.33383461475302
At round 67 accuracy: 0.9758272705937573
At round 67 training accuracy: 0.9779877506150866
At round 67 training loss: 0.07122424673252861
learning rate of μ: 0.2
Gradient difference: 23.0821498567622
At round 68 accuracy: 0.9755925839004929
At round 68 training accuracy: 0.977856881118149
At round 68 training loss: 0.07023713400239394
learning rate of μ: 0.2
Gradient difference: 22.689858417000973
At round 69 accuracy: 0.9762966439802864
At round 69 training accuracy: 0.9780924462126368
At round 69 training loss: 0.06957226918223365
learning rate of μ: 0.2
Gradient difference: 22.491948860052396
At round 70 accuracy: 0.9765313306735508
At round 70 training accuracy: 0.9781709679107994
At round 70 training loss: 0.06928382501748166
learning rate of μ: 0.2
Gradient difference: 22.352322676906343
At round 71 accuracy: 0.9767660173668153
At round 71 training accuracy: 0.9777260116212113
At round 71 training loss: 0.06903363103230689
learning rate of μ: 0.2
Gradient difference: 22.2157861743329
At round 72 accuracy: 0.9760619572870218
At round 72 training accuracy: 0.9779877506150866
At round 72 training loss: 0.06877000183733752
learning rate of μ: 0.2
Gradient difference: 22.206982303807816
At round 73 accuracy: 0.9760619572870218
At round 73 training accuracy: 0.9783803591058996
At round 73 training loss: 0.06808133128540404
learning rate of μ: 0.2
Gradient difference: 21.97130601968066
At round 74 accuracy: 0.9760619572870218
At round 74 training accuracy: 0.9783803591058996
At round 74 training loss: 0.06791920130438374
learning rate of μ: 0.2
Gradient difference: 21.98571585970266
At round 75 accuracy: 0.9767660173668153
At round 75 training accuracy: 0.9783280113071245
At round 75 training loss: 0.06709024773577588
learning rate of μ: 0.2
Gradient difference: 21.523183271602434
At round 76 accuracy: 0.9767660173668153
At round 76 training accuracy: 0.9784850547034497
At round 76 training loss: 0.0667626482359382
learning rate of μ: 0.2
Gradient difference: 21.348041506074207
At round 77 accuracy: 0.9770007040600798
At round 77 training accuracy: 0.9787729675967125
At round 77 training loss: 0.06627862128639946
learning rate of μ: 0.2
Gradient difference: 21.055002785626247
At round 78 accuracy: 0.9772353907533443
At round 78 training accuracy: 0.9786944458985499
At round 78 training loss: 0.06607257009588498
learning rate of μ: 0.2
Gradient difference: 20.954115354857855
At round 79 accuracy: 0.9777047641398733
At round 79 training accuracy: 0.9784327069046747
At round 79 training loss: 0.06584612716732635
learning rate of μ: 0.2
Gradient difference: 20.661482705561617
At round 80 accuracy: 0.9774700774466087
At round 80 training accuracy: 0.9788776631942627
At round 80 training loss: 0.06549770798332709
learning rate of μ: 0.2
Gradient difference: 20.51738846339731
At round 81 accuracy: 0.9772353907533443
At round 81 training accuracy: 0.9788514892948752
At round 81 training loss: 0.06543773354210994
learning rate of μ: 0.2
Gradient difference: 20.615558834471074
At round 82 accuracy: 0.9774700774466087
At round 82 training accuracy: 0.9787991414961
At round 82 training loss: 0.06519177289545966
learning rate of μ: 0.2
Gradient difference: 20.453152200404723
At round 83 accuracy: 0.9777047641398733
At round 83 training accuracy: 0.9789561848924253
At round 83 training loss: 0.06433079302363987
learning rate of μ: 0.2
Gradient difference: 19.882529815397856
At round 84 accuracy: 0.9772353907533443
At round 84 training accuracy: 0.9791655760875255
At round 84 training loss: 0.06367295771993096
learning rate of μ: 0.2
Gradient difference: 19.447586780371505
At round 85 accuracy: 0.9779394508331377
At round 85 training accuracy: 0.9797152279746637
At round 85 training loss: 0.06290542229793619
learning rate of μ: 0.2
Gradient difference: 18.762231778526804
At round 86 accuracy: 0.9786435109129312
At round 86 training accuracy: 0.9801601842642517
At round 86 training loss: 0.062342025080166
learning rate of μ: 0.2
Gradient difference: 18.418283818697066
At round 87 accuracy: 0.9788781976061958
At round 87 training accuracy: 0.9807098361513898
At round 87 training loss: 0.06229293060539994
learning rate of μ: 0.2
Gradient difference: 18.01325294031157
At round 88 accuracy: 0.9791128842994602
At round 88 training accuracy: 0.9805789666544522
At round 88 training loss: 0.062071030611429344
learning rate of μ: 0.2
Gradient difference: 18.047465681475437
At round 89 accuracy: 0.9793475709927247
At round 89 training accuracy: 0.9808407056483275
At round 89 training loss: 0.06190315506261901
learning rate of μ: 0.2
Gradient difference: 17.876008088501692
At round 90 accuracy: 0.9793475709927247
At round 90 training accuracy: 0.9807883578495524
At round 90 training loss: 0.06159267270909066
learning rate of μ: 0.2
Gradient difference: 17.90795741832728
At round 91 accuracy: 0.9793475709927247
At round 91 training accuracy: 0.9807883578495524
At round 91 training loss: 0.0613787300955025
learning rate of μ: 0.2
Gradient difference: 17.92970489360505
At round 92 accuracy: 0.9798169443792537
At round 92 training accuracy: 0.9809454012458776
At round 92 training loss: 0.06082464610652551
learning rate of μ: 0.2
Gradient difference: 17.562488163156697
At round 93 accuracy: 0.9798169443792537
At round 93 training accuracy: 0.9806313144532273
At round 93 training loss: 0.060543471843396
learning rate of μ: 0.2
Gradient difference: 17.633605411110942
At round 94 accuracy: 0.9809903778455762
At round 94 training accuracy: 0.9824373135109669
At round 94 training loss: 0.05962555841531567
learning rate of μ: 0.2
Gradient difference: 16.400750819403974
At round 95 accuracy: 0.9812250645388406
At round 95 training accuracy: 0.9825681830079045
At round 95 training loss: 0.05932717309408829
learning rate of μ: 0.2
Gradient difference: 16.32391652559861
At round 96 accuracy: 0.9814597512321052
At round 96 training accuracy: 0.9829084436999425
At round 96 training loss: 0.05885289618385042
learning rate of μ: 0.2
Gradient difference: 16.165465855359084
At round 97 accuracy: 0.9812250645388406
At round 97 training accuracy: 0.9828037481023923
At round 97 training loss: 0.057600179131635974
learning rate of μ: 0.2
Gradient difference: 15.76708168447218
At round 98 accuracy: 0.9814597512321052
At round 98 training accuracy: 0.9826467047060671
At round 98 training loss: 0.05723107182617751
learning rate of μ: 0.2
Gradient difference: 15.7401677543219
At round 99 accuracy: 0.9816944379253696
At round 99 training accuracy: 0.9830654870962676
At round 99 training loss: 0.05707407104854914
learning rate of μ: 0.2
Gradient difference: 15.578446833265653
At round 100 accuracy: 0.9819291246186341
At round 100 training accuracy: 0.9830393131968801
At round 100 training loss: 0.05690068689197547
learning rate of μ: 0.2
Gradient difference: 15.607952524874815
At round 101 accuracy: 0.9821638113118986
At round 101 training accuracy: 0.9831178348950427
At round 101 training loss: 0.05670287512028623
learning rate of μ: 0.2
Gradient difference: 15.536547473316746
At round 102 accuracy: 0.9821638113118986
At round 102 training accuracy: 0.9833533999895304
At round 102 training loss: 0.056460823056583065
learning rate of μ: 0.2
Gradient difference: 15.343790047051227
At round 103 accuracy: 0.9826331846984276
At round 103 training accuracy: 0.9834057477883055
At round 103 training loss: 0.056147753547884434
learning rate of μ: 0.2
Gradient difference: 15.19412342026391
At round 104 accuracy: 0.9828678713916921
At round 104 training accuracy: 0.983824530178506
At round 104 training loss: 0.05560306053482898
learning rate of μ: 0.2
Gradient difference: 14.7078901885322
At round 105 accuracy: 0.9828678713916921
At round 105 training accuracy: 0.9837460084803434
At round 105 training loss: 0.05517901513321091
learning rate of μ: 0.2
Gradient difference: 14.298874757329166
At round 106 accuracy: 0.9828678713916921
At round 106 training accuracy: 0.9836936606815684
At round 106 training loss: 0.055004983563462916
learning rate of μ: 0.2
Gradient difference: 14.3190824990335
At round 107 accuracy: 0.9828678713916921
At round 107 training accuracy: 0.9836936606815684
At round 107 training loss: 0.054861972922537396
learning rate of μ: 0.2
Gradient difference: 14.342988138692105
At round 108 accuracy: 0.9828678713916921
At round 108 training accuracy: 0.9836151389834058
At round 108 training loss: 0.054409654320706706
learning rate of μ: 0.2
Gradient difference: 14.085561069385813
At round 109 accuracy: 0.9831025580849566
At round 109 training accuracy: 0.9837983562791185
At round 109 training loss: 0.054331056102386824
learning rate of μ: 0.2
Gradient difference: 13.923504996625278
At round 110 accuracy: 0.9831025580849566
At round 110 training accuracy: 0.983772182379731
At round 110 training loss: 0.05420898819321145
learning rate of μ: 0.2
Gradient difference: 13.938591032762549
At round 111 accuracy: 0.9828678713916921
At round 111 training accuracy: 0.9836413128827933
At round 111 training loss: 0.053917695403915715
learning rate of μ: 0.2
Gradient difference: 13.868840852060796
At round 112 accuracy: 0.9828678713916921
At round 112 training accuracy: 0.9836151389834058
At round 112 training loss: 0.053833259031962365
learning rate of μ: 0.2
Gradient difference: 13.881455733540008
At round 113 accuracy: 0.9828678713916921
At round 113 training accuracy: 0.9836151389834058
At round 113 training loss: 0.05376149380712393
learning rate of μ: 0.2
Gradient difference: 13.88539488474155
At round 114 accuracy: 0.9828678713916921
At round 114 training accuracy: 0.9837198345809559
At round 114 training loss: 0.05366835588669046
learning rate of μ: 0.2
Gradient difference: 13.849212155331156
At round 115 accuracy: 0.9828678713916921
At round 115 training accuracy: 0.9836936606815684
At round 115 training loss: 0.053601052794456164
learning rate of μ: 0.2
Gradient difference: 13.853117382351167
At round 116 accuracy: 0.9828678713916921
At round 116 training accuracy: 0.9837983562791185
At round 116 training loss: 0.05348960463622266
learning rate of μ: 0.2
Gradient difference: 13.775815848064687
At round 117 accuracy: 0.9821638113118986
At round 117 training accuracy: 0.9840339213736062
At round 117 training loss: 0.052927264665712806
learning rate of μ: 0.2
Gradient difference: 13.460655715122902
At round 118 accuracy: 0.9823984980051631
At round 118 training accuracy: 0.983824530178506
At round 118 training loss: 0.0526344604577259
learning rate of μ: 0.2
Gradient difference: 13.281672754076233
At round 119 accuracy: 0.9821638113118986
At round 119 training accuracy: 0.983824530178506
At round 119 training loss: 0.05260682230723478
learning rate of μ: 0.2
Gradient difference: 13.288340280030656
At round 120 accuracy: 0.9821638113118986
At round 120 training accuracy: 0.9839553996754437
At round 120 training loss: 0.05232101928600968
learning rate of μ: 0.2
Gradient difference: 13.27900427068438
At round 121 accuracy: 0.9819291246186341
At round 121 training accuracy: 0.9838768779772811
At round 121 training loss: 0.05222394336205243
learning rate of μ: 0.2
Gradient difference: 13.329445793615438
At round 122 accuracy: 0.9819291246186341
At round 122 training accuracy: 0.9838507040778935
At round 122 training loss: 0.052196935246118306
learning rate of μ: 0.2
Gradient difference: 13.34201742810666
At round 123 accuracy: 0.9819291246186341
At round 123 training accuracy: 0.9838507040778935
At round 123 training loss: 0.05216399956495204
learning rate of μ: 0.2
Gradient difference: 13.34642738022111
At round 124 accuracy: 0.9819291246186341
At round 124 training accuracy: 0.983824530178506
At round 124 training loss: 0.05214766634280131
learning rate of μ: 0.2
Gradient difference: 13.355132514738667
At round 125 accuracy: 0.9821638113118986
At round 125 training accuracy: 0.983772182379731
At round 125 training loss: 0.05218379027914896
learning rate of μ: 0.2
Gradient difference: 13.423343053243624
At round 126 accuracy: 0.9821638113118986
At round 126 training accuracy: 0.9840077474742187
At round 126 training loss: 0.05199891232979099
learning rate of μ: 0.2
Gradient difference: 13.273486125799476
At round 127 accuracy: 0.9821638113118986
At round 127 training accuracy: 0.9839815735748312
At round 127 training loss: 0.05198846796056383
learning rate of μ: 0.2
Gradient difference: 13.284849625132512
At round 128 accuracy: 0.9823984980051631
At round 128 training accuracy: 0.9839292257760561
At round 128 training loss: 0.051761867532498715
learning rate of μ: 0.2
Gradient difference: 13.20540248424395
At round 129 accuracy: 0.9823984980051631
At round 129 training accuracy: 0.9838768779772811
At round 129 training loss: 0.051652200870939115
learning rate of μ: 0.2
Gradient difference: 13.157540244569658
At round 130 accuracy: 0.9823984980051631
At round 130 training accuracy: 0.9839553996754437
At round 130 training loss: 0.05138986586428588
learning rate of μ: 0.2
Gradient difference: 12.826368199649213
At round 131 accuracy: 0.9823984980051631
At round 131 training accuracy: 0.9838768779772811
At round 131 training loss: 0.05138688935674323
learning rate of μ: 0.2
Gradient difference: 12.838702153760515
At round 132 accuracy: 0.9826331846984276
At round 132 training accuracy: 0.9840339213736062
At round 132 training loss: 0.051213893855354124
learning rate of μ: 0.2
Gradient difference: 12.8075745553873
At round 133 accuracy: 0.9823984980051631
At round 133 training accuracy: 0.9840077474742187
At round 133 training loss: 0.05127525714655627
learning rate of μ: 0.2
Gradient difference: 12.94160554570098
At round 134 accuracy: 0.9823984980051631
At round 134 training accuracy: 0.9840077474742187
At round 134 training loss: 0.05081758710346352
learning rate of μ: 0.2
Gradient difference: 12.79498877462485
At round 135 accuracy: 0.9821638113118986
At round 135 training accuracy: 0.9839553996754437
At round 135 training loss: 0.0505245800610722
learning rate of μ: 0.2
Gradient difference: 12.880083660228372
At round 136 accuracy: 0.9821638113118986
At round 136 training accuracy: 0.9839292257760561
At round 136 training loss: 0.05049367944175335
learning rate of μ: 0.2
Gradient difference: 12.877779934681557
At round 137 accuracy: 0.9821638113118986
At round 137 training accuracy: 0.9839553996754437
At round 137 training loss: 0.05058094386860524
learning rate of μ: 0.2
Gradient difference: 12.887754145723026
At round 138 accuracy: 0.9821638113118986
At round 138 training accuracy: 0.9839030518766686
At round 138 training loss: 0.050561407470586496
learning rate of μ: 0.2
Gradient difference: 12.893429847383722
At round 139 accuracy: 0.9821638113118986
At round 139 training accuracy: 0.9839292257760561
At round 139 training loss: 0.050551201984091444
learning rate of μ: 0.2
Gradient difference: 12.900952618038163
At round 140 accuracy: 0.9821638113118986
At round 140 training accuracy: 0.9839292257760561
At round 140 training loss: 0.05040070444040965
learning rate of μ: 0.2
Gradient difference: 12.835258550724793
At round 141 accuracy: 0.9821638113118986
At round 141 training accuracy: 0.9839553996754437
At round 141 training loss: 0.049853605440062496
learning rate of μ: 0.2
Gradient difference: 12.57043681682474
At round 142 accuracy: 0.9821638113118986
At round 142 training accuracy: 0.9839292257760561
At round 142 training loss: 0.04983617303703042
learning rate of μ: 0.2
Gradient difference: 12.572396807463411
At round 143 accuracy: 0.9821638113118986
At round 143 training accuracy: 0.9839553996754437
At round 143 training loss: 0.04981131516471591
learning rate of μ: 0.2
Gradient difference: 12.571282460941092
At round 144 accuracy: 0.9821638113118986
At round 144 training accuracy: 0.9839292257760561
At round 144 training loss: 0.04979073398579743
learning rate of μ: 0.2
Gradient difference: 12.572417611189051
At round 145 accuracy: 0.9819291246186341
At round 145 training accuracy: 0.9839030518766686
At round 145 training loss: 0.04977820639689204
learning rate of μ: 0.2
Gradient difference: 12.57740283991894
At round 146 accuracy: 0.9821638113118986
At round 146 training accuracy: 0.9840600952729938
At round 146 training loss: 0.049738258759078016
learning rate of μ: 0.2
Gradient difference: 12.543848980548146
At round 147 accuracy: 0.9821638113118986
At round 147 training accuracy: 0.9841386169711563
At round 147 training loss: 0.04963767651597296
learning rate of μ: 0.2
Gradient difference: 12.390785385428376
At round 148 accuracy: 0.9823984980051631
At round 148 training accuracy: 0.9842171386693189
At round 148 training loss: 0.049350079768428964
learning rate of μ: 0.2
Gradient difference: 12.256805960814491
At round 149 accuracy: 0.9826331846984276
At round 149 training accuracy: 0.984321834266869
At round 149 training loss: 0.049225815744212746
learning rate of μ: 0.2
Gradient difference: 12.213759338732437
At round 150 accuracy: 0.9826331846984276
At round 150 training accuracy: 0.9844527037638068
At round 150 training loss: 0.04907092436678143
learning rate of μ: 0.2
Gradient difference: 12.162538307162597
At round 151 accuracy: 0.9828678713916921
At round 151 training accuracy: 0.9845573993613569
At round 151 training loss: 0.048712389007881836
learning rate of μ: 0.2
Gradient difference: 12.037532727270305
At round 152 accuracy: 0.9831025580849566
At round 152 training accuracy: 0.9845835732607444
At round 152 training loss: 0.04869324254848651
learning rate of μ: 0.2
Gradient difference: 12.035736742873603
At round 153 accuracy: 0.9823984980051631
At round 153 training accuracy: 0.9845050515625818
At round 153 training loss: 0.04860886505812683
learning rate of μ: 0.2
Gradient difference: 12.04361919968386
At round 154 accuracy: 0.9823984980051631
At round 154 training accuracy: 0.9845573993613569
At round 154 training loss: 0.04860939177582861
learning rate of μ: 0.2
Gradient difference: 12.052113692481061
At round 155 accuracy: 0.9823984980051631
At round 155 training accuracy: 0.9845573993613569
At round 155 training loss: 0.048595276840343725
learning rate of μ: 0.2
Gradient difference: 12.055172737931004
At round 156 accuracy: 0.9828678713916921
At round 156 training accuracy: 0.9846359210595195
At round 156 training loss: 0.04857352010673911
learning rate of μ: 0.2
Gradient difference: 12.133967858986704
At round 157 accuracy: 0.9828678713916921
At round 157 training accuracy: 0.9846359210595195
At round 157 training loss: 0.048288462344833936
learning rate of μ: 0.2
Gradient difference: 11.923651065900783
At round 158 accuracy: 0.9828678713916921
At round 158 training accuracy: 0.9846882688582945
At round 158 training loss: 0.04827093510610724
learning rate of μ: 0.2
Gradient difference: 11.922758458820969
At round 159 accuracy: 0.9828678713916921
At round 159 training accuracy: 0.984662094958907
At round 159 training loss: 0.04825875243857549
learning rate of μ: 0.2
Gradient difference: 11.928937352970715
At round 160 accuracy: 0.9823984980051631
At round 160 training accuracy: 0.9847406166570696
At round 160 training loss: 0.04795518611938211
learning rate of μ: 0.2
Gradient difference: 11.785909061578913
At round 161 accuracy: 0.9826331846984276
At round 161 training accuracy: 0.984714442757682
At round 161 training loss: 0.04788130972457535
learning rate of μ: 0.2
Gradient difference: 11.733848560573572
At round 162 accuracy: 0.9828678713916921
At round 162 training accuracy: 0.9849761817515573
At round 162 training loss: 0.04779045127711458
learning rate of μ: 0.2
Gradient difference: 11.579162876514092
At round 163 accuracy: 0.9828678713916921
At round 163 training accuracy: 0.9849500078521698
At round 163 training loss: 0.04778272501524237
learning rate of μ: 0.2
Gradient difference: 11.58703215082556
At round 164 accuracy: 0.983337244778221
At round 164 training accuracy: 0.9850285295503324
At round 164 training loss: 0.047501163467576574
learning rate of μ: 0.2
Gradient difference: 11.207461138084314
At round 165 accuracy: 0.9828678713916921
At round 165 training accuracy: 0.9850023556509448
At round 165 training loss: 0.047442675245685705
learning rate of μ: 0.2
Gradient difference: 11.195408462241604
At round 166 accuracy: 0.9831025580849566
At round 166 training accuracy: 0.9849761817515573
At round 166 training loss: 0.04743556452567149
learning rate of μ: 0.2
Gradient difference: 11.209351551616269
At round 167 accuracy: 0.9828678713916921
At round 167 training accuracy: 0.9850023556509448
At round 167 training loss: 0.047426522663818076
learning rate of μ: 0.2
Gradient difference: 11.207886311916734
At round 168 accuracy: 0.9831025580849566
At round 168 training accuracy: 0.9851332251478825
At round 168 training loss: 0.04728308818250396
learning rate of μ: 0.2
Gradient difference: 11.034681709634269
At round 169 accuracy: 0.9835719314714856
At round 169 training accuracy: 0.9850808773491074
At round 169 training loss: 0.04707520652784278
learning rate of μ: 0.2
Gradient difference: 10.979600494097749
At round 170 accuracy: 0.9835719314714856
At round 170 training accuracy: 0.9850808773491074
At round 170 training loss: 0.04706812406636361
learning rate of μ: 0.2
Gradient difference: 10.981172394376484
At round 171 accuracy: 0.9835719314714856
At round 171 training accuracy: 0.9850808773491074
At round 171 training loss: 0.04705436089527088
learning rate of μ: 0.2
Gradient difference: 10.983609397431326
At round 172 accuracy: 0.9835719314714856
At round 172 training accuracy: 0.9850808773491074
At round 172 training loss: 0.04704781344493187
learning rate of μ: 0.2
Gradient difference: 10.988068714967445
At round 173 accuracy: 0.98380661816475
At round 173 training accuracy: 0.9850808773491074
At round 173 training loss: 0.046874977363432824
learning rate of μ: 0.2
Gradient difference: 11.014226409995981
At round 174 accuracy: 0.98380661816475
At round 174 training accuracy: 0.9850285295503324
At round 174 training loss: 0.04686537277250143
learning rate of μ: 0.2
Gradient difference: 11.015441989258882
At round 175 accuracy: 0.98380661816475
At round 175 training accuracy: 0.9851332251478825
At round 175 training loss: 0.04665573124415602
learning rate of μ: 0.2
Gradient difference: 10.865355753122303
At round 176 accuracy: 0.9840413048580146
At round 176 training accuracy: 0.9850023556509448
At round 176 training loss: 0.04654908562289988
learning rate of μ: 0.2
Gradient difference: 10.756353804978016
At round 177 accuracy: 0.9835719314714856
At round 177 training accuracy: 0.9850808773491074
At round 177 training loss: 0.04613337785105074
learning rate of μ: 0.2
Gradient difference: 10.579969248703796
At round 178 accuracy: 0.98380661816475
At round 178 training accuracy: 0.98515939904727
At round 178 training loss: 0.04620627512106634
learning rate of μ: 0.2
Gradient difference: 10.675021818301522
At round 179 accuracy: 0.9831025580849566
At round 179 training accuracy: 0.9853164424435953
At round 179 training loss: 0.04599836381595044
learning rate of μ: 0.2
Gradient difference: 10.485789468173845
At round 180 accuracy: 0.9835719314714856
At round 180 training accuracy: 0.9852640946448202
At round 180 training loss: 0.04576563572932288
learning rate of μ: 0.2
Gradient difference: 10.306651497336626
At round 181 accuracy: 0.983337244778221
At round 181 training accuracy: 0.9852117468460452
At round 181 training loss: 0.04580774031678609
learning rate of μ: 0.2
Gradient difference: 10.407445437752408
At round 182 accuracy: 0.9831025580849566
At round 182 training accuracy: 0.9852379207454327
At round 182 training loss: 0.045710994308034096
learning rate of μ: 0.2
Gradient difference: 10.396715648275705
At round 183 accuracy: 0.9831025580849566
At round 183 training accuracy: 0.9852902685442078
At round 183 training loss: 0.045596792080596076
learning rate of μ: 0.2
Gradient difference: 10.39206356456145
At round 184 accuracy: 0.983337244778221
At round 184 training accuracy: 0.9852640946448202
At round 184 training loss: 0.04556462246368761
learning rate of μ: 0.2
Gradient difference: 10.402226314757211
At round 185 accuracy: 0.9831025580849566
At round 185 training accuracy: 0.9853164424435953
At round 185 training loss: 0.04527649233521713
learning rate of μ: 0.2
Gradient difference: 10.217008220301887
At round 186 accuracy: 0.9831025580849566
At round 186 training accuracy: 0.9853164424435953
At round 186 training loss: 0.045266373512396894
learning rate of μ: 0.2
Gradient difference: 10.226514351868756
At round 187 accuracy: 0.9831025580849566
At round 187 training accuracy: 0.9852902685442078
At round 187 training loss: 0.045247920003370024
learning rate of μ: 0.2
Gradient difference: 10.234207260999565
At round 188 accuracy: 0.983337244778221
At round 188 training accuracy: 0.9853164424435953
At round 188 training loss: 0.04511781333204614
learning rate of μ: 0.2
Gradient difference: 10.110370900077134
At round 189 accuracy: 0.983337244778221
At round 189 training accuracy: 0.9853164424435953
At round 189 training loss: 0.044927562297448397
learning rate of μ: 0.2
Gradient difference: 10.135261017331933
At round 190 accuracy: 0.983337244778221
At round 190 training accuracy: 0.9853164424435953
At round 190 training loss: 0.04483655634920925
learning rate of μ: 0.2
Gradient difference: 10.142423782974706
At round 191 accuracy: 0.9835719314714856
At round 191 training accuracy: 0.9853687902423703
At round 191 training loss: 0.04470884753771113
learning rate of μ: 0.2
Gradient difference: 9.977312689329066
At round 192 accuracy: 0.9835719314714856
At round 192 training accuracy: 0.9853687902423703
At round 192 training loss: 0.04469409348660685
learning rate of μ: 0.2
Gradient difference: 9.98674755291437
At round 193 accuracy: 0.98380661816475
At round 193 training accuracy: 0.9852117468460452
At round 193 training loss: 0.04461626639227228
learning rate of μ: 0.2
Gradient difference: 9.98043847045224
At round 194 accuracy: 0.98380661816475
At round 194 training accuracy: 0.9852117468460452
At round 194 training loss: 0.044608837397189446
learning rate of μ: 0.2
Gradient difference: 9.984317299614132
At round 195 accuracy: 0.9831025580849566
At round 195 training accuracy: 0.9852902685442078
At round 195 training loss: 0.04463071388730205
learning rate of μ: 0.2
Gradient difference: 10.041175768563313
At round 196 accuracy: 0.9831025580849566
At round 196 training accuracy: 0.9852902685442078
At round 196 training loss: 0.04461682569233918
learning rate of μ: 0.2
Gradient difference: 10.032078881004226
At round 197 accuracy: 0.9831025580849566
At round 197 training accuracy: 0.9852379207454327
At round 197 training loss: 0.04454726493423334
learning rate of μ: 0.2
Gradient difference: 10.078975787099242
At round 198 accuracy: 0.9828678713916921
At round 198 training accuracy: 0.9852379207454327
At round 198 training loss: 0.04448550948038029
learning rate of μ: 0.2
Gradient difference: 10.042266576534125
At round 199 accuracy: 0.9831025580849566
At round 199 training accuracy: 0.9854734858399204
At round 199 training loss: 0.04447937674854928
learning rate of μ: 0.2
Gradient difference: 10.002802941901622
At round 200 accuracy: 0.9831025580849566
At round 200 training accuracy: 0.9854473119405329
